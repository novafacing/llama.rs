#[cfg(target_arch = "x86")]
pub use core::arch::x86::{
    __m128, __m128 as __v4sf, __m128 as __m128_u, __m128i, __m128i as __v8hi, __m128i as __m128i_u,
    __m128i as __v4si, __m128i as __v4su, __m128i as __v2di, __m256, __m256 as __v8sf,
    __m256 as __m256_u, __m256i, __m256i as __m256i_u, __m256i as __v8si, __m256i as __v16hi,
    __m256i as __v8su, __m256i as __v4di, __m256i as __v32qi, __m256i as __v4du,
    __m256i as __v32qu, _mm256_add_epi32, _mm256_add_ps, _mm256_and_si256, _mm256_andnot_ps,
    _mm256_andnot_si256, _mm256_castps256_ps128, _mm256_castsi256_ps, _mm256_castsi256_si128,
    _mm256_cmpeq_epi8, _mm256_cvtepi32_ps, _mm256_cvtph_ps, _mm256_cvtps_epi32, _mm256_cvtps_ph,
    _mm256_extractf128_ps, _mm256_fmadd_ps, _mm256_loadu_ps, _mm256_loadu_si256, _mm256_madd_epi16,
    _mm256_maddubs_epi16, _mm256_max_ps, _mm256_mul_ps, _mm256_or_si256, _mm256_packs_epi16,
    _mm256_packs_epi32, _mm256_permutevar8x32_epi32, _mm256_round_ps, _mm256_set1_epi16,
    _mm256_set1_epi32, _mm256_set1_epi64x, _mm256_set1_epi8, _mm256_set1_ps, _mm256_set_epi16,
    _mm256_set_epi32, _mm256_set_epi64x, _mm256_set_epi8, _mm256_set_m128i, _mm256_set_ps,
    _mm256_setr_epi32, _mm256_setzero_ps, _mm256_setzero_si256, _mm256_shuffle_epi8,
    _mm256_sign_epi8, _mm256_storeu_ps, _mm256_storeu_si256, _mm256_sub_epi8, _mm_add_epi32,
    _mm_add_ps, _mm_add_ss, _mm_castps_si128, _mm_castsi128_ps, _mm_cvtph_ps, _mm_cvtps_ph,
    _mm_cvtsi128_si32, _mm_cvtss_f32, _mm_hadd_ps, _mm_loadu_ps, _mm_loadu_si128, _mm_max_ps,
    _mm_max_ss, _mm_movehdup_ps, _mm_movehl_ps, _mm_setr_epi16, _mm_setr_ps, _mm_setzero_ps,
    _mm_setzero_si128, _mm_shuffle_epi32, _mm_srli_epi16, _mm_storel_epi64, _mm_storeu_si128,
    _mm_unpackhi_epi64,
};
#[cfg(target_arch = "x86_64")]
pub use core::arch::x86_64::{
    __m128, __m128 as __v4sf, __m128 as __m128_u, __m128i, __m128i as __v8hi, __m128i as __m128i_u,
    __m128i as __v4si, __m128i as __v4su, __m128i as __v2di, __m256, __m256 as __v8sf,
    __m256 as __m256_u, __m256i, __m256i as __m256i_u, __m256i as __v8si, __m256i as __v16hi,
    __m256i as __v8su, __m256i as __v4di, __m256i as __v32qi, __m256i as __v4du,
    __m256i as __v32qu, _mm256_add_epi32, _mm256_add_ps, _mm256_and_si256, _mm256_andnot_ps,
    _mm256_andnot_si256, _mm256_castps256_ps128, _mm256_castsi256_ps, _mm256_castsi256_si128,
    _mm256_cmpeq_epi8, _mm256_cvtepi32_ps, _mm256_cvtph_ps, _mm256_cvtps_epi32, _mm256_cvtps_ph,
    _mm256_extractf128_ps, _mm256_fmadd_ps, _mm256_loadu_ps, _mm256_loadu_si256, _mm256_madd_epi16,
    _mm256_maddubs_epi16, _mm256_max_ps, _mm256_mul_ps, _mm256_or_si256, _mm256_packs_epi16,
    _mm256_packs_epi32, _mm256_permutevar8x32_epi32, _mm256_round_ps, _mm256_set1_epi16,
    _mm256_set1_epi32, _mm256_set1_epi64x, _mm256_set1_epi8, _mm256_set1_ps, _mm256_set_epi16,
    _mm256_set_epi32, _mm256_set_epi64x, _mm256_set_epi8, _mm256_set_m128i, _mm256_set_ps,
    _mm256_setr_epi32, _mm256_setzero_ps, _mm256_setzero_si256, _mm256_shuffle_epi8,
    _mm256_sign_epi8, _mm256_storeu_ps, _mm256_storeu_si256, _mm256_sub_epi8, _mm_add_epi32,
    _mm_add_ps, _mm_add_ss, _mm_castps_si128, _mm_castsi128_ps, _mm_cvtph_ps, _mm_cvtps_ph,
    _mm_cvtsi128_si32, _mm_cvtss_f32, _mm_hadd_ps, _mm_loadu_ps, _mm_loadu_si128, _mm_max_ps,
    _mm_max_ss, _mm_movehdup_ps, _mm_movehl_ps, _mm_setr_epi16, _mm_setr_ps, _mm_setzero_ps,
    _mm_setzero_si128, _mm_shuffle_epi32, _mm_srli_epi16, _mm_storel_epi64, _mm_storeu_si128,
    _mm_unpackhi_epi64,
};
use libc;
extern "C" {
    pub type _IO_wide_data;
    pub type _IO_codecvt;
    pub type _IO_marker;
    fn clock_gettime(__clock_id: clockid_t, __tp: *mut timespec) -> libc::c_int;
    fn clock() -> clock_t;
    fn cosf(_: libc::c_float) -> libc::c_float;
    fn sinf(_: libc::c_float) -> libc::c_float;
    fn tanhf(_: libc::c_float) -> libc::c_float;
    fn expf(_: libc::c_float) -> libc::c_float;
    fn logf(_: libc::c_float) -> libc::c_float;
    fn log2(_: libc::c_double) -> libc::c_double;
    fn powf(_: libc::c_float, _: libc::c_float) -> libc::c_float;
    fn sqrtf(_: libc::c_float) -> libc::c_float;
    fn fabsf(_: libc::c_float) -> libc::c_float;
    fn floor(_: libc::c_double) -> libc::c_double;
    fn roundf(_: libc::c_float) -> libc::c_float;
    fn free(_: *mut libc::c_void);
    fn posix_memalign(
        __memptr: *mut *mut libc::c_void,
        __alignment: size_t,
        __size: size_t,
    ) -> libc::c_int;
    fn abort() -> !;
    fn memcpy(_: *mut libc::c_void, _: *const libc::c_void, _: libc::c_ulong) -> *mut libc::c_void;
    fn memset(_: *mut libc::c_void, _: libc::c_int, _: libc::c_ulong) -> *mut libc::c_void;
    fn strncpy(_: *mut libc::c_char, _: *const libc::c_char, _: libc::c_ulong)
        -> *mut libc::c_char;
    fn strlen(_: *const libc::c_char) -> libc::c_ulong;
    static mut stderr: *mut FILE;
    fn fclose(__stream: *mut FILE) -> libc::c_int;
    fn fopen(_: *const libc::c_char, _: *const libc::c_char) -> *mut FILE;
    fn fprintf(_: *mut FILE, _: *const libc::c_char, _: ...) -> libc::c_int;
    fn printf(_: *const libc::c_char, _: ...) -> libc::c_int;
    fn snprintf(
        _: *mut libc::c_char,
        _: libc::c_ulong,
        _: *const libc::c_char,
        _: ...
    ) -> libc::c_int;
    fn pthread_create(
        __newthread: *mut pthread_t,
        __attr: *const pthread_attr_t,
        __start_routine: Option<unsafe extern "C" fn(*mut libc::c_void) -> *mut libc::c_void>,
        __arg: *mut libc::c_void,
    ) -> libc::c_int;
    fn pthread_join(__th: pthread_t, __thread_return: *mut *mut libc::c_void) -> libc::c_int;
    fn sched_yield() -> libc::c_int;
}
pub type __int8_t = libc::c_schar;
pub type __uint8_t = libc::c_uchar;
pub type __int16_t = libc::c_short;
pub type __uint16_t = libc::c_ushort;
pub type __int32_t = libc::c_int;
pub type __uint32_t = libc::c_uint;
pub type __int64_t = libc::c_long;
pub type __uint64_t = libc::c_ulong;
pub type __off_t = libc::c_long;
pub type __off64_t = libc::c_long;
pub type __clock_t = libc::c_long;
pub type __time_t = libc::c_long;
pub type __clockid_t = libc::c_int;
pub type __syscall_slong_t = libc::c_long;
pub type int8_t = __int8_t;
pub type int16_t = __int16_t;
pub type int32_t = __int32_t;
pub type int64_t = __int64_t;
pub type uint8_t = __uint8_t;
pub type uint16_t = __uint16_t;
pub type uint32_t = __uint32_t;
pub type uint64_t = __uint64_t;
pub type uintptr_t = libc::c_ulong;
pub type size_t = libc::c_ulong;
pub type ggml_fp16_t = uint16_t;
#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct __loadu_ps {
    pub __v: __m128_u,
}
#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct __mm_storel_epi64_struct {
    pub __u: libc::c_longlong,
}
#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct __loadu_ps_0 {
    pub __v: __m256_u,
}
#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct __storeu_si128 {
    pub __v: __m128i_u,
}
#[derive(Copy, Clone)]
#[repr(C)]
pub struct ggml_object {
    pub offs: size_t,
    pub size: size_t,
    pub next: *mut ggml_object,
    pub padding: [libc::c_char; 8],
}
#[derive(Copy, Clone)]
#[repr(C)]
pub struct ggml_context {
    pub mem_size: size_t,
    pub mem_buffer: *mut libc::c_void,
    pub mem_buffer_owned: bool,
    pub no_alloc: bool,
    pub n_objects: libc::c_int,
    pub objects_begin: *mut ggml_object,
    pub objects_end: *mut ggml_object,
    pub scratch: ggml_scratch,
    pub scratch_save: ggml_scratch,
}
#[derive(Copy, Clone)]
#[repr(C)]
pub struct ggml_scratch {
    pub offs: size_t,
    pub size: size_t,
    pub data: *mut libc::c_void,
}
pub type ggml_type = libc::c_uint;
pub const GGML_TYPE_COUNT: ggml_type = 13;
pub const GGML_TYPE_I32: ggml_type = 12;
pub const GGML_TYPE_I16: ggml_type = 11;
pub const GGML_TYPE_I8: ggml_type = 10;
pub const GGML_TYPE_Q8_1: ggml_type = 9;
pub const GGML_TYPE_Q8_0: ggml_type = 8;
pub const GGML_TYPE_Q5_1: ggml_type = 7;
pub const GGML_TYPE_Q5_0: ggml_type = 6;
pub const GGML_TYPE_Q4_1: ggml_type = 3;
pub const GGML_TYPE_Q4_0: ggml_type = 2;
pub const GGML_TYPE_F16: ggml_type = 1;
pub const GGML_TYPE_F32: ggml_type = 0;
pub type ggml_backend = libc::c_uint;
pub const GGML_BACKEND_CL: ggml_backend = 2;
pub const GGML_BACKEND_CUDA: ggml_backend = 1;
pub const GGML_BACKEND_CPU: ggml_backend = 0;
pub type ggml_ftype = libc::c_int;
pub const GGML_FTYPE_MOSTLY_Q5_1: ggml_ftype = 9;
pub const GGML_FTYPE_MOSTLY_Q5_0: ggml_ftype = 8;
pub const GGML_FTYPE_MOSTLY_Q8_0: ggml_ftype = 7;
pub const GGML_FTYPE_MOSTLY_Q4_1_SOME_F16: ggml_ftype = 4;
pub const GGML_FTYPE_MOSTLY_Q4_1: ggml_ftype = 3;
pub const GGML_FTYPE_MOSTLY_Q4_0: ggml_ftype = 2;
pub const GGML_FTYPE_MOSTLY_F16: ggml_ftype = 1;
pub const GGML_FTYPE_ALL_F32: ggml_ftype = 0;
pub const GGML_FTYPE_UNKNOWN: ggml_ftype = -1;
pub type ggml_op = libc::c_uint;
pub const GGML_OP_COUNT: ggml_op = 51;
pub const GGML_OP_MAP_BINARY: ggml_op = 50;
pub const GGML_OP_MAP_UNARY: ggml_op = 49;
pub const GGML_OP_FLASH_FF: ggml_op = 48;
pub const GGML_OP_FLASH_ATTN: ggml_op = 47;
pub const GGML_OP_CONV_1D_2S: ggml_op = 46;
pub const GGML_OP_CONV_1D_1S: ggml_op = 45;
pub const GGML_OP_CLAMP: ggml_op = 44;
pub const GGML_OP_ALIBI: ggml_op = 43;
pub const GGML_OP_ROPE_BACK: ggml_op = 42;
pub const GGML_OP_ROPE: ggml_op = 41;
pub const GGML_OP_SOFT_MAX: ggml_op = 40;
pub const GGML_OP_DIAG_MASK_ZERO: ggml_op = 39;
pub const GGML_OP_DIAG_MASK_INF: ggml_op = 38;
pub const GGML_OP_DIAG: ggml_op = 37;
pub const GGML_OP_GET_ROWS_BACK: ggml_op = 36;
pub const GGML_OP_GET_ROWS: ggml_op = 35;
pub const GGML_OP_TRANSPOSE: ggml_op = 34;
pub const GGML_OP_PERMUTE: ggml_op = 33;
pub const GGML_OP_VIEW: ggml_op = 32;
pub const GGML_OP_RESHAPE: ggml_op = 31;
pub const GGML_OP_CONT: ggml_op = 30;
pub const GGML_OP_CPY: ggml_op = 29;
pub const GGML_OP_SET: ggml_op = 28;
pub const GGML_OP_SCALE: ggml_op = 27;
pub const GGML_OP_MUL_MAT: ggml_op = 26;
pub const GGML_OP_RMS_NORM_BACK: ggml_op = 25;
pub const GGML_OP_RMS_NORM: ggml_op = 24;
pub const GGML_OP_NORM: ggml_op = 23;
pub const GGML_OP_SILU_BACK: ggml_op = 22;
pub const GGML_OP_SILU: ggml_op = 21;
pub const GGML_OP_GELU: ggml_op = 20;
pub const GGML_OP_RELU: ggml_op = 19;
pub const GGML_OP_STEP: ggml_op = 18;
pub const GGML_OP_NEG: ggml_op = 17;
pub const GGML_OP_SGN: ggml_op = 16;
pub const GGML_OP_ABS: ggml_op = 15;
pub const GGML_OP_REPEAT: ggml_op = 14;
pub const GGML_OP_MEAN: ggml_op = 13;
pub const GGML_OP_SUM_ROWS: ggml_op = 12;
pub const GGML_OP_SUM: ggml_op = 11;
pub const GGML_OP_LOG: ggml_op = 10;
pub const GGML_OP_SQRT: ggml_op = 9;
pub const GGML_OP_SQR: ggml_op = 8;
pub const GGML_OP_DIV: ggml_op = 7;
pub const GGML_OP_MUL: ggml_op = 6;
pub const GGML_OP_SUB: ggml_op = 5;
pub const GGML_OP_ACC: ggml_op = 4;
pub const GGML_OP_ADD1: ggml_op = 3;
pub const GGML_OP_ADD: ggml_op = 2;
pub const GGML_OP_DUP: ggml_op = 1;
pub const GGML_OP_NONE: ggml_op = 0;
#[derive(Copy, Clone)]
#[repr(C)]
pub struct ggml_tensor {
    pub type_0: ggml_type,
    pub backend: ggml_backend,
    pub n_dims: libc::c_int,
    pub ne: [int64_t; 4],
    pub nb: [size_t; 4],
    pub op: ggml_op,
    pub is_param: bool,
    pub grad: *mut ggml_tensor,
    pub src0: *mut ggml_tensor,
    pub src1: *mut ggml_tensor,
    pub opt: [*mut ggml_tensor; 4],
    pub n_tasks: libc::c_int,
    pub perf_runs: libc::c_int,
    pub perf_cycles: int64_t,
    pub perf_time_us: int64_t,
    pub data: *mut libc::c_void,
    pub name: [libc::c_char; 32],
    pub padding: [libc::c_char; 16],
}
#[derive(Copy, Clone)]
#[repr(C)]
pub struct ggml_cgraph {
    pub n_nodes: libc::c_int,
    pub n_leafs: libc::c_int,
    pub n_threads: libc::c_int,
    pub work_size: size_t,
    pub work: *mut ggml_tensor,
    pub nodes: [*mut ggml_tensor; 4096],
    pub grads: [*mut ggml_tensor; 4096],
    pub leafs: [*mut ggml_tensor; 4096],
    pub perf_runs: libc::c_int,
    pub perf_cycles: int64_t,
    pub perf_time_us: int64_t,
}
#[derive(Copy, Clone)]
#[repr(C)]
pub struct ggml_init_params {
    pub mem_size: size_t,
    pub mem_buffer: *mut libc::c_void,
    pub no_alloc: bool,
}
#[derive(Copy, Clone)]
#[repr(C)]
pub struct timespec {
    pub tv_sec: __time_t,
    pub tv_nsec: __syscall_slong_t,
}
pub type clockid_t = __clockid_t;
pub type clock_t = __clock_t;
#[derive(Copy, Clone)]
#[repr(C)]
pub struct block_q8_1 {
    pub d: libc::c_float,
    pub s: libc::c_float,
    pub qs: [int8_t; 32],
}
#[derive(Copy, Clone)]
#[repr(C)]
pub struct block_q8_0 {
    pub d: ggml_fp16_t,
    pub qs: [int8_t; 32],
}
#[derive(Copy, Clone)]
#[repr(C)]
pub struct block_q5_1 {
    pub d: ggml_fp16_t,
    pub m: ggml_fp16_t,
    pub qh: [uint8_t; 4],
    pub qs: [uint8_t; 16],
}
#[derive(Copy, Clone)]
#[repr(C)]
pub struct block_q5_0 {
    pub d: ggml_fp16_t,
    pub qh: [uint8_t; 4],
    pub qs: [uint8_t; 16],
}
#[derive(Copy, Clone)]
#[repr(C)]
pub struct block_q4_1 {
    pub d: ggml_fp16_t,
    pub m: ggml_fp16_t,
    pub qs: [uint8_t; 16],
}
#[derive(Copy, Clone)]
#[repr(C)]
pub struct block_q4_0 {
    pub d: ggml_fp16_t,
    pub qs: [uint8_t; 16],
}
pub type FILE = _IO_FILE;
#[derive(Copy, Clone)]
#[repr(C)]
pub struct _IO_FILE {
    pub _flags: libc::c_int,
    pub _IO_read_ptr: *mut libc::c_char,
    pub _IO_read_end: *mut libc::c_char,
    pub _IO_read_base: *mut libc::c_char,
    pub _IO_write_base: *mut libc::c_char,
    pub _IO_write_ptr: *mut libc::c_char,
    pub _IO_write_end: *mut libc::c_char,
    pub _IO_buf_base: *mut libc::c_char,
    pub _IO_buf_end: *mut libc::c_char,
    pub _IO_save_base: *mut libc::c_char,
    pub _IO_backup_base: *mut libc::c_char,
    pub _IO_save_end: *mut libc::c_char,
    pub _markers: *mut _IO_marker,
    pub _chain: *mut _IO_FILE,
    pub _fileno: libc::c_int,
    pub _flags2: libc::c_int,
    pub _old_offset: __off_t,
    pub _cur_column: libc::c_ushort,
    pub _vtable_offset: libc::c_schar,
    pub _shortbuf: [libc::c_char; 1],
    pub _lock: *mut libc::c_void,
    pub _offset: __off64_t,
    pub _codecvt: *mut _IO_codecvt,
    pub _wide_data: *mut _IO_wide_data,
    pub _freeres_list: *mut _IO_FILE,
    pub _freeres_buf: *mut libc::c_void,
    pub __pad5: size_t,
    pub _mode: libc::c_int,
    pub _unused2: [libc::c_char; 20],
}
pub type _IO_lock_t = ();
#[derive(Copy, Clone)]
#[repr(C)]
pub struct ggml_context_container {
    pub used: bool,
    pub context: ggml_context,
}
#[derive(Copy, Clone)]
#[repr(C)]
pub struct ggml_state {
    pub contexts: [ggml_context_container; 64],
}
pub type ggml_unary_op_f32_t =
    Option<unsafe extern "C" fn(libc::c_int, *mut libc::c_float, *const libc::c_float) -> ()>;
pub type ggml_binary_op_f32_t = Option<
    unsafe extern "C" fn(
        libc::c_int,
        *mut libc::c_float,
        *const libc::c_float,
        *const libc::c_float,
    ) -> (),
>;
pub type ggml_lock_t = libc::c_int;
#[derive(Copy, Clone)]
#[repr(C)]
pub struct ggml_compute_state_shared {
    pub spin: ggml_lock_t,
    pub n_threads: libc::c_int,
    pub n_ready: libc::c_int,
    pub has_work: bool,
    pub stop: bool,
}
pub type ggml_thread_t = pthread_t;
pub type pthread_t = libc::c_ulong;
#[derive(Copy, Clone)]
#[repr(C)]
pub struct ggml_compute_state {
    pub thrd: ggml_thread_t,
    pub params: ggml_compute_params,
    pub node: *mut ggml_tensor,
    pub shared: *mut ggml_compute_state_shared,
}
#[derive(Copy, Clone)]
#[repr(C)]
pub struct ggml_compute_params {
    pub type_0: ggml_task_type,
    pub ith: libc::c_int,
    pub nth: libc::c_int,
    pub wsize: size_t,
    pub wdata: *mut libc::c_void,
}
pub type ggml_task_type = libc::c_uint;
pub const GGML_TASK_FINALIZE: ggml_task_type = 2;
pub const GGML_TASK_COMPUTE: ggml_task_type = 1;
pub const GGML_TASK_INIT: ggml_task_type = 0;
pub type ggml_float = libc::c_double;
#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct __loadu_si128 {
    pub __v: __m128i_u,
}
#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct __storeu_ps {
    pub __v: __m256_u,
}
pub type dequantize_row_q_t =
    Option<unsafe extern "C" fn(*const libc::c_void, *mut libc::c_float, libc::c_int) -> ()>;
#[derive(Copy, Clone)]
#[repr(C)]
pub struct quantize_fns_t {
    pub dequantize_row_q: dequantize_row_q_t,
    pub quantize_row_q: quantize_row_q_t,
    pub quantize_row_q_reference: quantize_row_q_t,
    pub quantize_row_q_dot: quantize_row_q_t,
    pub vec_dot_q: vec_dot_q_t,
    pub vec_dot_type: ggml_type,
}
pub type vec_dot_q_t = Option<
    unsafe extern "C" fn(
        libc::c_int,
        *mut libc::c_float,
        *const libc::c_void,
        *const libc::c_void,
    ) -> (),
>;
pub type quantize_row_q_t =
    Option<unsafe extern "C" fn(*const libc::c_float, *mut libc::c_void, libc::c_int) -> ()>;
#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct __storeu_si256 {
    pub __v: __m256i_u,
}
#[derive(Copy, Clone)]
#[repr(C, packed)]
pub struct __loadu_si256 {
    pub __v: __m256i_u,
}
pub type thread_ret_t = *mut libc::c_void;
#[derive(Copy, Clone)]
#[repr(C)]
pub union pthread_attr_t {
    pub __size: [libc::c_char; 56],
    pub __align: libc::c_long,
}
pub type ggml_opt_type = libc::c_uint;
pub const GGML_OPT_LBFGS: ggml_opt_type = 1;
pub const GGML_OPT_ADAM: ggml_opt_type = 0;
pub type ggml_linesearch = libc::c_uint;
pub const GGML_LINESEARCH_BACKTRACKING_STRONG_WOLFE: ggml_linesearch = 2;
pub const GGML_LINESEARCH_BACKTRACKING_WOLFE: ggml_linesearch = 1;
pub const GGML_LINESEARCH_BACKTRACKING_ARMIJO: ggml_linesearch = 0;
pub const GGML_LINESEARCH_DEFAULT: ggml_linesearch = 1;
pub type ggml_opt_result = libc::c_int;
pub const GGML_LINESEARCH_INVALID_PARAMETERS: ggml_opt_result = -124;
pub const GGML_LINESEARCH_MAXIMUM_ITERATIONS: ggml_opt_result = -125;
pub const GGML_LINESEARCH_MAXIMUM_STEP: ggml_opt_result = -126;
pub const GGML_LINESEARCH_MINIMUM_STEP: ggml_opt_result = -127;
pub const GGML_LINESEARCH_FAIL: ggml_opt_result = -128;
pub const GGML_OPT_FAIL: ggml_opt_result = 4;
pub const GGML_OPT_INVALID_WOLFE: ggml_opt_result = 3;
pub const GGML_OPT_NO_CONTEXT: ggml_opt_result = 2;
pub const GGML_OPT_DID_NOT_CONVERGE: ggml_opt_result = 1;
pub const GGML_OPT_OK: ggml_opt_result = 0;
#[derive(Copy, Clone)]
#[repr(C)]
pub struct ggml_opt_params {
    pub type_0: ggml_opt_type,
    pub n_threads: libc::c_int,
    pub past: libc::c_int,
    pub delta: libc::c_float,
    pub max_no_improvement: libc::c_int,
    pub print_forward_graph: bool,
    pub print_backward_graph: bool,
    pub adam: C2RustUnnamed_0,
    pub lbfgs: C2RustUnnamed,
}
#[derive(Copy, Clone)]
#[repr(C)]
pub struct C2RustUnnamed {
    pub m: libc::c_int,
    pub n_iter: libc::c_int,
    pub max_linesearch: libc::c_int,
    pub eps: libc::c_float,
    pub ftol: libc::c_float,
    pub wolfe: libc::c_float,
    pub min_step: libc::c_float,
    pub max_step: libc::c_float,
    pub linesearch: ggml_linesearch,
}
#[derive(Copy, Clone)]
#[repr(C)]
pub struct C2RustUnnamed_0 {
    pub n_iter: libc::c_int,
    pub alpha: libc::c_float,
    pub beta1: libc::c_float,
    pub beta2: libc::c_float,
    pub eps: libc::c_float,
    pub eps_f: libc::c_float,
    pub eps_g: libc::c_float,
}
#[derive(Copy, Clone)]
#[repr(C)]
pub struct ggml_lbfgs_iteration_data {
    pub alpha: libc::c_float,
    pub ys: libc::c_float,
    pub s: *mut libc::c_float,
    pub y: *mut libc::c_float,
}
static mut GGML_OBJECT_SIZE: size_t = ::core::mem::size_of::<ggml_object>() as libc::c_ulong;
#[inline]
unsafe extern "C" fn ggml_aligned_malloc(mut size: size_t) -> *mut libc::c_void {
    let mut aligned_memory: *mut libc::c_void = 0 as *mut libc::c_void;
    let mut result: libc::c_int =
        posix_memalign(&mut aligned_memory, 16 as libc::c_int as size_t, size);
    if result != 0 as libc::c_int {
        return 0 as *mut libc::c_void;
    }
    return aligned_memory;
}
#[no_mangle]
pub unsafe extern "C" fn atomic_fetch_add(
    mut ptr: *mut libc::c_int,
    mut val: libc::c_int,
) -> libc::c_int {
    return ::core::intrinsics::atomic_xadd_seqcst(ptr, val);
}
#[no_mangle]
pub unsafe extern "C" fn atomic_fetch_sub(
    mut ptr: *mut libc::c_int,
    mut val: libc::c_int,
) -> libc::c_int {
    return ::core::intrinsics::atomic_xsub_seqcst(ptr, val);
}
#[no_mangle]
pub unsafe extern "C" fn atomic_load(mut ptr: *mut libc::c_int) -> libc::c_int {
    return ::core::intrinsics::atomic_xadd_seqcst(ptr, 0 as libc::c_int);
}
#[no_mangle]
pub unsafe extern "C" fn atomic_store(
    mut ptr: *mut libc::c_int,
    mut val: libc::c_int,
) -> libc::c_int {
    return ::core::intrinsics::atomic_xchg_acquire(ptr, val);
}
#[no_mangle]
pub unsafe extern "C" fn atomic_exchange(
    mut ptr: *mut libc::c_int,
    mut val: libc::c_int,
) -> libc::c_int {
    return ::core::intrinsics::atomic_xchg_acquire(ptr, val);
}
#[inline(always)]
unsafe extern "C" fn _cvtsh_ss(mut __a: libc::c_ushort) -> libc::c_float {
    let mut __v: __v8hi = _mm_setr_epi16(
        __a as libc::c_short,
        0 as libc::c_int as libc::c_short,
        0 as libc::c_int as libc::c_short,
        0 as libc::c_int as libc::c_short,
        0 as libc::c_int as libc::c_short,
        0 as libc::c_int as libc::c_short,
        0 as libc::c_int as libc::c_short,
        0 as libc::c_int as libc::c_short,
    );
    let mut __r: __v4sf = _mm_cvtph_ps(__v);
    return ::core::mem::transmute::<
        _,
        [libc::c_float; ::core::mem::size_of::<__v4sf>() / ::core::mem::size_of::<libc::c_float>()],
    >(__r)[0 as libc::c_int as usize];
}
static mut table_gelu_f16: [ggml_fp16_t; 65536] = [0; 65536];
static mut table_silu_f16: [ggml_fp16_t; 65536] = [0; 65536];
static mut table_exp_f16: [ggml_fp16_t; 65536] = [0; 65536];
static mut table_f32_f16: [libc::c_float; 65536] = [0.; 65536];
#[inline]
unsafe extern "C" fn ggml_lookup_fp16_to_fp32(mut f: ggml_fp16_t) -> libc::c_float {
    let mut s: uint16_t = 0;
    memcpy(
        &mut s as *mut uint16_t as *mut libc::c_void,
        &mut f as *mut ggml_fp16_t as *const libc::c_void,
        ::core::mem::size_of::<uint16_t>() as libc::c_ulong,
    );
    return table_f32_f16[s as usize];
}
#[no_mangle]
pub unsafe extern "C" fn ggml_fp16_to_fp32(mut x: ggml_fp16_t) -> libc::c_float {
    return ggml_lookup_fp16_to_fp32(x);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_fp32_to_fp16(mut x: libc::c_float) -> ggml_fp16_t {
    return ({
        ::core::mem::transmute::<
            _,
            [libc::c_short;
                ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
        >(_mm_cvtps_ph(
            _mm_setr_ps(
                x,
                0 as libc::c_int as libc::c_float,
                0 as libc::c_int as libc::c_float,
                0 as libc::c_int as libc::c_float,
            ),
            0 as libc::c_int,
        ))[0 as libc::c_int as usize] as libc::c_ushort
    });
}
#[no_mangle]
pub unsafe extern "C" fn ggml_fp16_to_fp32_row(
    mut x: *const ggml_fp16_t,
    mut y: *mut libc::c_float,
    mut n: size_t,
) {
    let mut i: size_t = 0 as libc::c_int as size_t;
    while i < n {
        *y.offset(i as isize) = ggml_lookup_fp16_to_fp32(*x.offset(i as isize));
        i = i.wrapping_add(1);
        i;
    }
}
#[no_mangle]
pub unsafe extern "C" fn ggml_fp32_to_fp16_row(
    mut x: *const libc::c_float,
    mut y: *mut ggml_fp16_t,
    mut n: size_t,
) {
    let mut i: size_t = 0 as libc::c_int as size_t;
    while i.wrapping_add(7 as libc::c_int as libc::c_ulong) < n {
        let mut x_vec: __m256 = _mm256_loadu_ps(x.offset(i as isize));
        let mut y_vec: __m128i = _mm256_cvtps_ph(x_vec, 0 as libc::c_int);
        _mm_storeu_si128(y.offset(i as isize) as *mut __m128i, y_vec);
        i = (i as libc::c_ulong).wrapping_add(8 as libc::c_int as libc::c_ulong) as size_t
            as size_t;
    }
    while i.wrapping_add(3 as libc::c_int as libc::c_ulong) < n {
        let mut x_vec_0: __m128 = _mm_loadu_ps(x.offset(i as isize));
        let mut y_vec_0: __m128i = _mm_cvtps_ph(x_vec_0, 0 as libc::c_int);
        _mm_storel_epi64(y.offset(i as isize) as *mut __m128i, y_vec_0);
        i = (i as libc::c_ulong).wrapping_add(4 as libc::c_int as libc::c_ulong) as size_t
            as size_t;
    }
    while i < n {
        *y.offset(i as isize) = ({
            ::core::mem::transmute::<
                _,
                [libc::c_short;
                    ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
            >(_mm_cvtps_ph(
                _mm_setr_ps(
                    *x.offset(i as isize),
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                ),
                0 as libc::c_int,
            ))[0 as libc::c_int as usize] as libc::c_ushort
        });
        i = i.wrapping_add(1);
        i;
    }
}
#[no_mangle]
pub unsafe extern "C" fn ggml_time_init() {}
#[no_mangle]
pub unsafe extern "C" fn ggml_time_ms() -> int64_t {
    let mut ts: timespec = timespec {
        tv_sec: 0,
        tv_nsec: 0,
    };
    clock_gettime(1 as libc::c_int, &mut ts);
    return ts.tv_sec * 1000 as libc::c_int as libc::c_long
        + ts.tv_nsec / 1000000 as libc::c_int as libc::c_long;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_time_us() -> int64_t {
    let mut ts: timespec = timespec {
        tv_sec: 0,
        tv_nsec: 0,
    };
    clock_gettime(1 as libc::c_int, &mut ts);
    return ts.tv_sec * 1000000 as libc::c_int as libc::c_long
        + ts.tv_nsec / 1000 as libc::c_int as libc::c_long;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cycles() -> int64_t {
    return clock();
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cycles_per_ms() -> int64_t {
    return 1000000 as libc::c_int as __clock_t / 1000 as libc::c_int as libc::c_long;
}
static mut CACHE_LINE_SIZE_F32: size_t = 0;
#[inline]
unsafe extern "C" fn hsum_float_8(x: __m256) -> libc::c_float {
    let mut res: __m128 = _mm256_extractf128_ps(x, 1 as libc::c_int);
    res = _mm_add_ps(res, _mm256_castps256_ps128(x));
    res = _mm_add_ps(res, _mm_movehl_ps(res, res));
    res = _mm_add_ss(res, _mm_movehdup_ps(res));
    return _mm_cvtss_f32(res);
}
#[inline]
unsafe extern "C" fn hsum_i32_8(a: __m256i) -> libc::c_int {
    let sum128: __m128i = _mm_add_epi32(
        _mm256_castsi256_si128(a),
        _mm_castps_si128(_mm256_extractf128_ps(
            _mm256_castsi256_ps(a),
            1 as libc::c_int,
        )),
    );
    let hi64: __m128i = _mm_unpackhi_epi64(sum128, sum128);
    let sum64: __m128i = _mm_add_epi32(hi64, sum128);
    let hi32: __m128i = _mm_shuffle_epi32(
        sum64,
        (2 as libc::c_int) << 6 as libc::c_int
            | (3 as libc::c_int) << 4 as libc::c_int
            | (0 as libc::c_int) << 2 as libc::c_int
            | 1 as libc::c_int,
    );
    return _mm_cvtsi128_si32(_mm_add_epi32(sum64, hi32));
}
#[inline]
unsafe extern "C" fn bytes_from_bits_32(mut x: *const uint8_t) -> __m256i {
    let mut x32: uint32_t = 0;
    memcpy(
        &mut x32 as *mut uint32_t as *mut libc::c_void,
        x as *const libc::c_void,
        ::core::mem::size_of::<uint32_t>() as libc::c_ulong,
    );
    let shuf_mask: __m256i = _mm256_set_epi64x(
        0x303030303030303 as libc::c_long as libc::c_longlong,
        0x202020202020202 as libc::c_long as libc::c_longlong,
        0x101010101010101 as libc::c_long as libc::c_longlong,
        0 as libc::c_int as libc::c_longlong,
    );
    let mut bytes: __m256i = _mm256_shuffle_epi8(_mm256_set1_epi32(x32 as libc::c_int), shuf_mask);
    let bit_mask: __m256i =
        _mm256_set1_epi64x(0x7fbfdfeff7fbfdfe as libc::c_long as libc::c_longlong);
    bytes = _mm256_or_si256(bytes, bit_mask);
    return _mm256_cmpeq_epi8(
        bytes,
        _mm256_set1_epi64x(-(1 as libc::c_int) as libc::c_longlong),
    );
}
#[inline]
unsafe extern "C" fn bytes_from_nibbles_32(mut rsi: *const uint8_t) -> __m256i {
    let tmp: __m128i = _mm_loadu_si128(rsi as *const __m128i);
    let bytes: __m256i = _mm256_set_m128i(_mm_srli_epi16(tmp, 4 as libc::c_int), tmp);
    let lowMask: __m256i = _mm256_set1_epi8(0xf as libc::c_int as libc::c_char);
    return _mm256_and_si256(lowMask, bytes);
}
#[inline]
unsafe extern "C" fn sum_i16_pairs_float(x: __m256i) -> __m256 {
    let ones: __m256i = _mm256_set1_epi16(1 as libc::c_int as libc::c_short);
    let summed_pairs: __m256i = _mm256_madd_epi16(ones, x);
    return _mm256_cvtepi32_ps(summed_pairs);
}
#[inline]
unsafe extern "C" fn mul_sum_us8_pairs_float(ax: __m256i, sy: __m256i) -> __m256 {
    let dot: __m256i = _mm256_maddubs_epi16(ax, sy);
    return sum_i16_pairs_float(dot);
}
#[inline]
unsafe extern "C" fn mul_sum_i8_pairs_float(x: __m256i, y: __m256i) -> __m256 {
    let ax: __m256i = _mm256_sign_epi8(x, x);
    let sy: __m256i = _mm256_sign_epi8(y, x);
    return mul_sum_us8_pairs_float(ax, sy);
}
unsafe extern "C" fn quantize_row_q4_0_reference(
    mut x: *const libc::c_float,
    mut y: *mut block_q4_0,
    mut k: libc::c_int,
) {
    static mut qk: libc::c_int = 32 as libc::c_int;
    let nb: libc::c_int = k / qk;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nb {
        let mut amax: libc::c_float = 0.0f32;
        let mut max: libc::c_float = 0.0f32;
        let mut j: libc::c_int = 0 as libc::c_int;
        while j < qk {
            let v: libc::c_float = *x.offset((i * qk + j) as isize);
            if amax < fabsf(v) {
                amax = fabsf(v);
                max = v;
            }
            j += 1;
            j;
        }
        let d: libc::c_float = max / -(8 as libc::c_int) as libc::c_float;
        let id: libc::c_float = if d != 0. { 1.0f32 / d } else { 0.0f32 };
        (*y.offset(i as isize)).d = ({
            ::core::mem::transmute::<
                _,
                [libc::c_short;
                    ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
            >(_mm_cvtps_ph(
                _mm_setr_ps(
                    d,
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                ),
                0 as libc::c_int,
            ))[0 as libc::c_int as usize] as libc::c_ushort
        });
        let mut j_0: libc::c_int = 0 as libc::c_int;
        while j_0 < qk / 2 as libc::c_int {
            let x0: libc::c_float = *x.offset((i * qk + 0 as libc::c_int + j_0) as isize) * id;
            let x1: libc::c_float = *x.offset((i * qk + qk / 2 as libc::c_int + j_0) as isize) * id;
            let xi0: uint8_t = (if (15 as libc::c_int) < (x0 + 8.5f32) as int8_t as libc::c_int {
                15 as libc::c_int
            } else {
                (x0 + 8.5f32) as int8_t as libc::c_int
            }) as uint8_t;
            let xi1: uint8_t = (if (15 as libc::c_int) < (x1 + 8.5f32) as int8_t as libc::c_int {
                15 as libc::c_int
            } else {
                (x1 + 8.5f32) as int8_t as libc::c_int
            }) as uint8_t;
            (*y.offset(i as isize)).qs[j_0 as usize] = xi0;
            let ref mut fresh0 = (*y.offset(i as isize)).qs[j_0 as usize];
            *fresh0 =
                (*fresh0 as libc::c_int | (xi1 as libc::c_int) << 4 as libc::c_int) as uint8_t;
            j_0 += 1;
            j_0;
        }
        i += 1;
        i;
    }
}
unsafe extern "C" fn quantize_row_q4_0(
    mut x: *const libc::c_float,
    mut y: *mut libc::c_void,
    mut k: libc::c_int,
) {
    quantize_row_q4_0_reference(x, y as *mut block_q4_0, k);
}
unsafe extern "C" fn quantize_row_q4_1_reference(
    mut x: *const libc::c_float,
    mut y: *mut block_q4_1,
    mut k: libc::c_int,
) {
    let qk: libc::c_int = 32 as libc::c_int;
    let nb: libc::c_int = k / qk;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nb {
        let mut min: libc::c_float = 3.40282347e+38f32;
        let mut max: libc::c_float = -3.40282347e+38f32;
        let mut j: libc::c_int = 0 as libc::c_int;
        while j < qk {
            let v: libc::c_float = *x.offset((i * qk + j) as isize);
            if v < min {
                min = v;
            }
            if v > max {
                max = v;
            }
            j += 1;
            j;
        }
        let d: libc::c_float = (max - min)
            / (((1 as libc::c_int) << 4 as libc::c_int) - 1 as libc::c_int) as libc::c_float;
        let id: libc::c_float = if d != 0. { 1.0f32 / d } else { 0.0f32 };
        (*y.offset(i as isize)).d = ({
            ::core::mem::transmute::<
                _,
                [libc::c_short;
                    ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
            >(_mm_cvtps_ph(
                _mm_setr_ps(
                    d,
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                ),
                0 as libc::c_int,
            ))[0 as libc::c_int as usize] as libc::c_ushort
        });
        (*y.offset(i as isize)).m = ({
            ::core::mem::transmute::<
                _,
                [libc::c_short;
                    ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
            >(_mm_cvtps_ph(
                _mm_setr_ps(
                    min,
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                ),
                0 as libc::c_int,
            ))[0 as libc::c_int as usize] as libc::c_ushort
        });
        let mut j_0: libc::c_int = 0 as libc::c_int;
        while j_0 < qk / 2 as libc::c_int {
            let x0: libc::c_float =
                (*x.offset((i * qk + 0 as libc::c_int + j_0) as isize) - min) * id;
            let x1: libc::c_float =
                (*x.offset((i * qk + qk / 2 as libc::c_int + j_0) as isize) - min) * id;
            let xi0: uint8_t = (if (15 as libc::c_int) < (x0 + 0.5f32) as int8_t as libc::c_int {
                15 as libc::c_int
            } else {
                (x0 + 0.5f32) as int8_t as libc::c_int
            }) as uint8_t;
            let xi1: uint8_t = (if (15 as libc::c_int) < (x1 + 0.5f32) as int8_t as libc::c_int {
                15 as libc::c_int
            } else {
                (x1 + 0.5f32) as int8_t as libc::c_int
            }) as uint8_t;
            (*y.offset(i as isize)).qs[j_0 as usize] = xi0;
            let ref mut fresh1 = (*y.offset(i as isize)).qs[j_0 as usize];
            *fresh1 =
                (*fresh1 as libc::c_int | (xi1 as libc::c_int) << 4 as libc::c_int) as uint8_t;
            j_0 += 1;
            j_0;
        }
        i += 1;
        i;
    }
}
unsafe extern "C" fn quantize_row_q4_1(
    mut x: *const libc::c_float,
    mut y: *mut libc::c_void,
    mut k: libc::c_int,
) {
    quantize_row_q4_1_reference(x, y as *mut block_q4_1, k);
}
unsafe extern "C" fn quantize_row_q5_0_reference(
    mut x: *const libc::c_float,
    mut y: *mut block_q5_0,
    mut k: libc::c_int,
) {
    static mut qk: libc::c_int = 32 as libc::c_int;
    let nb: libc::c_int = k / qk;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nb {
        let mut amax: libc::c_float = 0.0f32;
        let mut max: libc::c_float = 0.0f32;
        let mut j: libc::c_int = 0 as libc::c_int;
        while j < qk {
            let v: libc::c_float = *x.offset((i * qk + j) as isize);
            if amax < fabsf(v) {
                amax = fabsf(v);
                max = v;
            }
            j += 1;
            j;
        }
        let d: libc::c_float = max / -(16 as libc::c_int) as libc::c_float;
        let id: libc::c_float = if d != 0. { 1.0f32 / d } else { 0.0f32 };
        (*y.offset(i as isize)).d = ({
            ::core::mem::transmute::<
                _,
                [libc::c_short;
                    ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
            >(_mm_cvtps_ph(
                _mm_setr_ps(
                    d,
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                ),
                0 as libc::c_int,
            ))[0 as libc::c_int as usize] as libc::c_ushort
        });
        let mut qh: uint32_t = 0 as libc::c_int as uint32_t;
        let mut j_0: libc::c_int = 0 as libc::c_int;
        while j_0 < qk / 2 as libc::c_int {
            let x0: libc::c_float = *x.offset((i * qk + 0 as libc::c_int + j_0) as isize) * id;
            let x1: libc::c_float = *x.offset((i * qk + qk / 2 as libc::c_int + j_0) as isize) * id;
            let xi0: uint8_t = (if (31 as libc::c_int) < (x0 + 16.5f32) as int8_t as libc::c_int {
                31 as libc::c_int
            } else {
                (x0 + 16.5f32) as int8_t as libc::c_int
            }) as uint8_t;
            let xi1: uint8_t = (if (31 as libc::c_int) < (x1 + 16.5f32) as int8_t as libc::c_int {
                31 as libc::c_int
            } else {
                (x1 + 16.5f32) as int8_t as libc::c_int
            }) as uint8_t;
            (*y.offset(i as isize)).qs[j_0 as usize] = (xi0 as libc::c_int & 0xf as libc::c_int
                | (xi1 as libc::c_int & 0xf as libc::c_int) << 4 as libc::c_int)
                as uint8_t;
            qh |= (((xi0 as libc::c_int & 0x10 as libc::c_int) >> 4 as libc::c_int)
                << j_0 + 0 as libc::c_int) as libc::c_uint;
            qh |= (((xi1 as libc::c_int & 0x10 as libc::c_int) >> 4 as libc::c_int)
                << j_0 + qk / 2 as libc::c_int) as libc::c_uint;
            j_0 += 1;
            j_0;
        }
        memcpy(
            &mut (*y.offset(i as isize)).qh as *mut [uint8_t; 4] as *mut libc::c_void,
            &mut qh as *mut uint32_t as *const libc::c_void,
            ::core::mem::size_of::<uint32_t>() as libc::c_ulong,
        );
        i += 1;
        i;
    }
}
unsafe extern "C" fn quantize_row_q5_0(
    mut x: *const libc::c_float,
    mut y: *mut libc::c_void,
    mut k: libc::c_int,
) {
    quantize_row_q5_0_reference(x, y as *mut block_q5_0, k);
}
unsafe extern "C" fn quantize_row_q5_1_reference(
    mut x: *const libc::c_float,
    mut y: *mut block_q5_1,
    mut k: libc::c_int,
) {
    let qk: libc::c_int = 32 as libc::c_int;
    let nb: libc::c_int = k / qk;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nb {
        let mut min: libc::c_float = 3.40282347e+38f32;
        let mut max: libc::c_float = -3.40282347e+38f32;
        let mut j: libc::c_int = 0 as libc::c_int;
        while j < qk {
            let v: libc::c_float = *x.offset((i * qk + j) as isize);
            if v < min {
                min = v;
            }
            if v > max {
                max = v;
            }
            j += 1;
            j;
        }
        let d: libc::c_float = (max - min)
            / (((1 as libc::c_int) << 5 as libc::c_int) - 1 as libc::c_int) as libc::c_float;
        let id: libc::c_float = if d != 0. { 1.0f32 / d } else { 0.0f32 };
        (*y.offset(i as isize)).d = ({
            ::core::mem::transmute::<
                _,
                [libc::c_short;
                    ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
            >(_mm_cvtps_ph(
                _mm_setr_ps(
                    d,
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                ),
                0 as libc::c_int,
            ))[0 as libc::c_int as usize] as libc::c_ushort
        });
        (*y.offset(i as isize)).m = ({
            ::core::mem::transmute::<
                _,
                [libc::c_short;
                    ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
            >(_mm_cvtps_ph(
                _mm_setr_ps(
                    min,
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                ),
                0 as libc::c_int,
            ))[0 as libc::c_int as usize] as libc::c_ushort
        });
        let mut qh: uint32_t = 0 as libc::c_int as uint32_t;
        let mut j_0: libc::c_int = 0 as libc::c_int;
        while j_0 < qk / 2 as libc::c_int {
            let x0: libc::c_float =
                (*x.offset((i * qk + 0 as libc::c_int + j_0) as isize) - min) * id;
            let x1: libc::c_float =
                (*x.offset((i * qk + qk / 2 as libc::c_int + j_0) as isize) - min) * id;
            let xi0: uint8_t = (x0 + 0.5f32) as uint8_t;
            let xi1: uint8_t = (x1 + 0.5f32) as uint8_t;
            (*y.offset(i as isize)).qs[j_0 as usize] = (xi0 as libc::c_int & 0xf as libc::c_int
                | (xi1 as libc::c_int & 0xf as libc::c_int) << 4 as libc::c_int)
                as uint8_t;
            qh |= (((xi0 as libc::c_int & 0x10 as libc::c_int) >> 4 as libc::c_int)
                << j_0 + 0 as libc::c_int) as libc::c_uint;
            qh |= (((xi1 as libc::c_int & 0x10 as libc::c_int) >> 4 as libc::c_int)
                << j_0 + qk / 2 as libc::c_int) as libc::c_uint;
            j_0 += 1;
            j_0;
        }
        memcpy(
            &mut (*y.offset(i as isize)).qh as *mut [uint8_t; 4] as *mut libc::c_void,
            &mut qh as *mut uint32_t as *const libc::c_void,
            ::core::mem::size_of::<[uint8_t; 4]>() as libc::c_ulong,
        );
        i += 1;
        i;
    }
}
unsafe extern "C" fn quantize_row_q5_1(
    mut x: *const libc::c_float,
    mut y: *mut libc::c_void,
    mut k: libc::c_int,
) {
    quantize_row_q5_1_reference(x, y as *mut block_q5_1, k);
}
unsafe extern "C" fn quantize_row_q8_0_reference(
    mut x: *const libc::c_float,
    mut y: *mut block_q8_0,
    mut k: libc::c_int,
) {
    let nb: libc::c_int = k / 32 as libc::c_int;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nb {
        let mut amax: libc::c_float = 0.0f32;
        let mut j: libc::c_int = 0 as libc::c_int;
        while j < 32 as libc::c_int {
            let v: libc::c_float = *x.offset((i * 32 as libc::c_int + j) as isize);
            amax = if amax > fabsf(v) { amax } else { fabsf(v) };
            j += 1;
            j;
        }
        let d: libc::c_float =
            amax / (((1 as libc::c_int) << 7 as libc::c_int) - 1 as libc::c_int) as libc::c_float;
        let id: libc::c_float = if d != 0. { 1.0f32 / d } else { 0.0f32 };
        (*y.offset(i as isize)).d = ({
            ::core::mem::transmute::<
                _,
                [libc::c_short;
                    ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
            >(_mm_cvtps_ph(
                _mm_setr_ps(
                    d,
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                ),
                0 as libc::c_int,
            ))[0 as libc::c_int as usize] as libc::c_ushort
        });
        let mut j_0: libc::c_int = 0 as libc::c_int;
        while j_0 < 32 as libc::c_int {
            let x0: libc::c_float = *x.offset((i * 32 as libc::c_int + j_0) as isize) * id;
            (*y.offset(i as isize)).qs[j_0 as usize] = roundf(x0) as int8_t;
            j_0 += 1;
            j_0;
        }
        i += 1;
        i;
    }
}
unsafe extern "C" fn quantize_row_q8_0(
    mut x: *const libc::c_float,
    mut vy: *mut libc::c_void,
    mut k: libc::c_int,
) {
    let nb: libc::c_int = k / 32 as libc::c_int;
    let mut y: *mut block_q8_0 = vy as *mut block_q8_0;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nb {
        let mut v0: __m256 = _mm256_loadu_ps(x);
        let mut v1: __m256 = _mm256_loadu_ps(x.offset(8 as libc::c_int as isize));
        let mut v2: __m256 = _mm256_loadu_ps(x.offset(16 as libc::c_int as isize));
        let mut v3: __m256 = _mm256_loadu_ps(x.offset(24 as libc::c_int as isize));
        x = x.offset(32 as libc::c_int as isize);
        let signBit: __m256 = _mm256_set1_ps(-0.0f32);
        let mut maxAbs: __m256 = _mm256_andnot_ps(signBit, v0);
        maxAbs = _mm256_max_ps(maxAbs, _mm256_andnot_ps(signBit, v1));
        maxAbs = _mm256_max_ps(maxAbs, _mm256_andnot_ps(signBit, v2));
        maxAbs = _mm256_max_ps(maxAbs, _mm256_andnot_ps(signBit, v3));
        let mut max4: __m128 = _mm_max_ps(
            _mm256_extractf128_ps(maxAbs, 1 as libc::c_int),
            _mm256_castps256_ps128(maxAbs),
        );
        max4 = _mm_max_ps(max4, _mm_movehl_ps(max4, max4));
        max4 = _mm_max_ss(max4, _mm_movehdup_ps(max4));
        let maxScalar: libc::c_float = _mm_cvtss_f32(max4);
        let d: libc::c_float = maxScalar / 127.0f32;
        (*y.offset(i as isize)).d = ({
            ::core::mem::transmute::<
                _,
                [libc::c_short;
                    ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
            >(_mm_cvtps_ph(
                _mm_setr_ps(
                    d,
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                ),
                0 as libc::c_int,
            ))[0 as libc::c_int as usize] as libc::c_ushort
        });
        let id: libc::c_float = if maxScalar != 0.0f32 {
            127.0f32 / maxScalar
        } else {
            0.0f32
        };
        let mul: __m256 = _mm256_set1_ps(id);
        v0 = _mm256_mul_ps(v0, mul);
        v1 = _mm256_mul_ps(v1, mul);
        v2 = _mm256_mul_ps(v2, mul);
        v3 = _mm256_mul_ps(v3, mul);
        v0 = _mm256_round_ps(v0, 0 as libc::c_uint as libc::c_int);
        v1 = _mm256_round_ps(v1, 0 as libc::c_uint as libc::c_int);
        v2 = _mm256_round_ps(v2, 0 as libc::c_uint as libc::c_int);
        v3 = _mm256_round_ps(v3, 0 as libc::c_uint as libc::c_int);
        let mut i0: __m256i = _mm256_cvtps_epi32(v0);
        let mut i1: __m256i = _mm256_cvtps_epi32(v1);
        let mut i2: __m256i = _mm256_cvtps_epi32(v2);
        let mut i3: __m256i = _mm256_cvtps_epi32(v3);
        i0 = _mm256_packs_epi32(i0, i1);
        i2 = _mm256_packs_epi32(i2, i3);
        i0 = _mm256_packs_epi16(i0, i2);
        let perm: __m256i = _mm256_setr_epi32(
            0 as libc::c_int,
            4 as libc::c_int,
            1 as libc::c_int,
            5 as libc::c_int,
            2 as libc::c_int,
            6 as libc::c_int,
            3 as libc::c_int,
            7 as libc::c_int,
        );
        i0 = _mm256_permutevar8x32_epi32(i0, perm);
        _mm256_storeu_si256(
            ((*y.offset(i as isize)).qs).as_mut_ptr() as *mut __m256i,
            i0,
        );
        i += 1;
        i;
    }
}
unsafe extern "C" fn quantize_row_q8_1_reference(
    mut x: *const libc::c_float,
    mut y: *mut block_q8_1,
    mut k: libc::c_int,
) {
    let nb: libc::c_int = k / 32 as libc::c_int;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nb {
        let mut amax: libc::c_float = 0.0f32;
        let mut j: libc::c_int = 0 as libc::c_int;
        while j < 32 as libc::c_int {
            let v: libc::c_float = *x.offset((i * 32 as libc::c_int + j) as isize);
            amax = if amax > fabsf(v) { amax } else { fabsf(v) };
            j += 1;
            j;
        }
        let d: libc::c_float =
            amax / (((1 as libc::c_int) << 7 as libc::c_int) - 1 as libc::c_int) as libc::c_float;
        let id: libc::c_float = if d != 0. { 1.0f32 / d } else { 0.0f32 };
        (*y.offset(i as isize)).d = d;
        let mut sum: libc::c_int = 0 as libc::c_int;
        let mut j_0: libc::c_int = 0 as libc::c_int;
        while j_0 < 32 as libc::c_int / 2 as libc::c_int {
            let v0: libc::c_float = *x.offset((i * 32 as libc::c_int + j_0) as isize) * id;
            let v1: libc::c_float = *x.offset(
                (i * 32 as libc::c_int + 32 as libc::c_int / 2 as libc::c_int + j_0) as isize,
            ) * id;
            (*y.offset(i as isize)).qs[j_0 as usize] = roundf(v0) as int8_t;
            (*y.offset(i as isize)).qs[(32 as libc::c_int / 2 as libc::c_int + j_0) as usize] =
                roundf(v1) as int8_t;
            sum += (*y.offset(i as isize)).qs[j_0 as usize] as libc::c_int;
            sum += (*y.offset(i as isize)).qs[(32 as libc::c_int / 2 as libc::c_int + j_0) as usize]
                as libc::c_int;
            j_0 += 1;
            j_0;
        }
        (*y.offset(i as isize)).s = sum as libc::c_float * d;
        i += 1;
        i;
    }
}
unsafe extern "C" fn quantize_row_q8_1(
    mut x: *const libc::c_float,
    mut vy: *mut libc::c_void,
    mut k: libc::c_int,
) {
    let nb: libc::c_int = k / 32 as libc::c_int;
    let mut y: *mut block_q8_1 = vy as *mut block_q8_1;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nb {
        let mut v0: __m256 = _mm256_loadu_ps(x);
        let mut v1: __m256 = _mm256_loadu_ps(x.offset(8 as libc::c_int as isize));
        let mut v2: __m256 = _mm256_loadu_ps(x.offset(16 as libc::c_int as isize));
        let mut v3: __m256 = _mm256_loadu_ps(x.offset(24 as libc::c_int as isize));
        x = x.offset(32 as libc::c_int as isize);
        let signBit: __m256 = _mm256_set1_ps(-0.0f32);
        let mut maxAbs: __m256 = _mm256_andnot_ps(signBit, v0);
        maxAbs = _mm256_max_ps(maxAbs, _mm256_andnot_ps(signBit, v1));
        maxAbs = _mm256_max_ps(maxAbs, _mm256_andnot_ps(signBit, v2));
        maxAbs = _mm256_max_ps(maxAbs, _mm256_andnot_ps(signBit, v3));
        let mut max4: __m128 = _mm_max_ps(
            _mm256_extractf128_ps(maxAbs, 1 as libc::c_int),
            _mm256_castps256_ps128(maxAbs),
        );
        max4 = _mm_max_ps(max4, _mm_movehl_ps(max4, max4));
        max4 = _mm_max_ss(max4, _mm_movehdup_ps(max4));
        let maxScalar: libc::c_float = _mm_cvtss_f32(max4);
        let d: libc::c_float = maxScalar / 127.0f32;
        (*y.offset(i as isize)).d = d;
        let id: libc::c_float = if maxScalar != 0.0f32 {
            127.0f32 / maxScalar
        } else {
            0.0f32
        };
        let mul: __m256 = _mm256_set1_ps(id);
        v0 = _mm256_mul_ps(v0, mul);
        v1 = _mm256_mul_ps(v1, mul);
        v2 = _mm256_mul_ps(v2, mul);
        v3 = _mm256_mul_ps(v3, mul);
        v0 = _mm256_round_ps(v0, 0 as libc::c_uint as libc::c_int);
        v1 = _mm256_round_ps(v1, 0 as libc::c_uint as libc::c_int);
        v2 = _mm256_round_ps(v2, 0 as libc::c_uint as libc::c_int);
        v3 = _mm256_round_ps(v3, 0 as libc::c_uint as libc::c_int);
        let mut i0: __m256i = _mm256_cvtps_epi32(v0);
        let mut i1: __m256i = _mm256_cvtps_epi32(v1);
        let mut i2: __m256i = _mm256_cvtps_epi32(v2);
        let mut i3: __m256i = _mm256_cvtps_epi32(v3);
        (*y.offset(i as isize)).s = d * hsum_i32_8(_mm256_add_epi32(
            _mm256_add_epi32(i0, i1),
            _mm256_add_epi32(i2, i3),
        )) as libc::c_float;
        i0 = _mm256_packs_epi32(i0, i1);
        i2 = _mm256_packs_epi32(i2, i3);
        i0 = _mm256_packs_epi16(i0, i2);
        let perm: __m256i = _mm256_setr_epi32(
            0 as libc::c_int,
            4 as libc::c_int,
            1 as libc::c_int,
            5 as libc::c_int,
            2 as libc::c_int,
            6 as libc::c_int,
            3 as libc::c_int,
            7 as libc::c_int,
        );
        i0 = _mm256_permutevar8x32_epi32(i0, perm);
        _mm256_storeu_si256(
            ((*y.offset(i as isize)).qs).as_mut_ptr() as *mut __m256i,
            i0,
        );
        i += 1;
        i;
    }
}
unsafe extern "C" fn dequantize_row_q4_0(
    mut x: *const block_q4_0,
    mut y: *mut libc::c_float,
    mut k: libc::c_int,
) {
    static mut qk: libc::c_int = 32 as libc::c_int;
    let nb: libc::c_int = k / qk;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nb {
        let d: libc::c_float = ggml_lookup_fp16_to_fp32((*x.offset(i as isize)).d);
        let mut j: libc::c_int = 0 as libc::c_int;
        while j < qk / 2 as libc::c_int {
            let x0: libc::c_int = ((*x.offset(i as isize)).qs[j as usize] as libc::c_int
                & 0xf as libc::c_int)
                - 8 as libc::c_int;
            let x1: libc::c_int = ((*x.offset(i as isize)).qs[j as usize] as libc::c_int
                >> 4 as libc::c_int)
                - 8 as libc::c_int;
            *y.offset((i * qk + j + 0 as libc::c_int) as isize) = x0 as libc::c_float * d;
            *y.offset((i * qk + j + qk / 2 as libc::c_int) as isize) = x1 as libc::c_float * d;
            j += 1;
            j;
        }
        i += 1;
        i;
    }
}
unsafe extern "C" fn dequantize_row_q4_1(
    mut x: *const block_q4_1,
    mut y: *mut libc::c_float,
    mut k: libc::c_int,
) {
    static mut qk: libc::c_int = 32 as libc::c_int;
    let nb: libc::c_int = k / qk;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nb {
        let d: libc::c_float = ggml_lookup_fp16_to_fp32((*x.offset(i as isize)).d);
        let m: libc::c_float = ggml_lookup_fp16_to_fp32((*x.offset(i as isize)).m);
        let mut j: libc::c_int = 0 as libc::c_int;
        while j < qk / 2 as libc::c_int {
            let x0: libc::c_int =
                (*x.offset(i as isize)).qs[j as usize] as libc::c_int & 0xf as libc::c_int;
            let x1: libc::c_int =
                (*x.offset(i as isize)).qs[j as usize] as libc::c_int >> 4 as libc::c_int;
            *y.offset((i * qk + j + 0 as libc::c_int) as isize) = x0 as libc::c_float * d + m;
            *y.offset((i * qk + j + qk / 2 as libc::c_int) as isize) = x1 as libc::c_float * d + m;
            j += 1;
            j;
        }
        i += 1;
        i;
    }
}
unsafe extern "C" fn dequantize_row_q5_0(
    mut x: *const block_q5_0,
    mut y: *mut libc::c_float,
    mut k: libc::c_int,
) {
    static mut qk: libc::c_int = 32 as libc::c_int;
    let nb: libc::c_int = k / qk;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nb {
        let d: libc::c_float = ggml_lookup_fp16_to_fp32((*x.offset(i as isize)).d);
        let mut qh: uint32_t = 0;
        memcpy(
            &mut qh as *mut uint32_t as *mut libc::c_void,
            ((*x.offset(i as isize)).qh).as_ptr() as *const libc::c_void,
            ::core::mem::size_of::<uint32_t>() as libc::c_ulong,
        );
        let mut j: libc::c_int = 0 as libc::c_int;
        while j < qk / 2 as libc::c_int {
            let xh_0: uint8_t = ((qh >> j + 0 as libc::c_int) << 4 as libc::c_int
                & 0x10 as libc::c_int as libc::c_uint) as uint8_t;
            let xh_1: uint8_t =
                (qh >> j + 12 as libc::c_int & 0x10 as libc::c_int as libc::c_uint) as uint8_t;
            let x0: int32_t = ((*x.offset(i as isize)).qs[j as usize] as libc::c_int
                & 0xf as libc::c_int
                | xh_0 as libc::c_int)
                - 16 as libc::c_int;
            let x1: int32_t = ((*x.offset(i as isize)).qs[j as usize] as libc::c_int
                >> 4 as libc::c_int
                | xh_1 as libc::c_int)
                - 16 as libc::c_int;
            *y.offset((i * qk + j + 0 as libc::c_int) as isize) = x0 as libc::c_float * d;
            *y.offset((i * qk + j + qk / 2 as libc::c_int) as isize) = x1 as libc::c_float * d;
            j += 1;
            j;
        }
        i += 1;
        i;
    }
}
unsafe extern "C" fn dequantize_row_q5_1(
    mut x: *const block_q5_1,
    mut y: *mut libc::c_float,
    mut k: libc::c_int,
) {
    static mut qk: libc::c_int = 32 as libc::c_int;
    let nb: libc::c_int = k / qk;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nb {
        let d: libc::c_float = ggml_lookup_fp16_to_fp32((*x.offset(i as isize)).d);
        let m: libc::c_float = ggml_lookup_fp16_to_fp32((*x.offset(i as isize)).m);
        let mut qh: uint32_t = 0;
        memcpy(
            &mut qh as *mut uint32_t as *mut libc::c_void,
            ((*x.offset(i as isize)).qh).as_ptr() as *const libc::c_void,
            ::core::mem::size_of::<uint32_t>() as libc::c_ulong,
        );
        let mut j: libc::c_int = 0 as libc::c_int;
        while j < qk / 2 as libc::c_int {
            let xh_0: uint8_t = ((qh >> j + 0 as libc::c_int) << 4 as libc::c_int
                & 0x10 as libc::c_int as libc::c_uint) as uint8_t;
            let xh_1: uint8_t =
                (qh >> j + 12 as libc::c_int & 0x10 as libc::c_int as libc::c_uint) as uint8_t;
            let x0: libc::c_int = (*x.offset(i as isize)).qs[j as usize] as libc::c_int
                & 0xf as libc::c_int
                | xh_0 as libc::c_int;
            let x1: libc::c_int = (*x.offset(i as isize)).qs[j as usize] as libc::c_int
                >> 4 as libc::c_int
                | xh_1 as libc::c_int;
            *y.offset((i * qk + j + 0 as libc::c_int) as isize) = x0 as libc::c_float * d + m;
            *y.offset((i * qk + j + qk / 2 as libc::c_int) as isize) = x1 as libc::c_float * d + m;
            j += 1;
            j;
        }
        i += 1;
        i;
    }
}
unsafe extern "C" fn dequantize_row_q8_0(
    mut vx: *const libc::c_void,
    mut y: *mut libc::c_float,
    mut k: libc::c_int,
) {
    static mut qk: libc::c_int = 32 as libc::c_int;
    let nb: libc::c_int = k / qk;
    let mut x: *const block_q8_0 = vx as *const block_q8_0;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nb {
        let d: libc::c_float = ggml_lookup_fp16_to_fp32((*x.offset(i as isize)).d);
        let mut j: libc::c_int = 0 as libc::c_int;
        while j < qk {
            *y.offset((i * qk + j) as isize) =
                (*x.offset(i as isize)).qs[j as usize] as libc::c_int as libc::c_float * d;
            j += 1;
            j;
        }
        i += 1;
        i;
    }
}
static mut quantize_fns: [quantize_fns_t; 13] = unsafe {
    [
        quantize_fns_t {
            dequantize_row_q: None,
            quantize_row_q: None,
            quantize_row_q_reference: None,
            quantize_row_q_dot: None,
            vec_dot_q: None,
            vec_dot_type: GGML_TYPE_F32,
        },
        quantize_fns_t {
            dequantize_row_q: None,
            quantize_row_q: None,
            quantize_row_q_reference: None,
            quantize_row_q_dot: None,
            vec_dot_q: None,
            vec_dot_type: GGML_TYPE_F32,
        },
        {
            let mut init = quantize_fns_t {
                dequantize_row_q: ::core::mem::transmute::<
                    Option<
                        unsafe extern "C" fn(
                            *const block_q4_0,
                            *mut libc::c_float,
                            libc::c_int,
                        ) -> (),
                    >,
                    dequantize_row_q_t,
                >(Some(
                    dequantize_row_q4_0
                        as unsafe extern "C" fn(
                            *const block_q4_0,
                            *mut libc::c_float,
                            libc::c_int,
                        ) -> (),
                )),
                quantize_row_q: Some(
                    quantize_row_q4_0
                        as unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut libc::c_void,
                            libc::c_int,
                        ) -> (),
                ),
                quantize_row_q_reference: ::core::mem::transmute::<
                    Option<
                        unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut block_q4_0,
                            libc::c_int,
                        ) -> (),
                    >,
                    quantize_row_q_t,
                >(Some(
                    quantize_row_q4_0_reference
                        as unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut block_q4_0,
                            libc::c_int,
                        ) -> (),
                )),
                quantize_row_q_dot: Some(
                    quantize_row_q8_0
                        as unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut libc::c_void,
                            libc::c_int,
                        ) -> (),
                ),
                vec_dot_q: Some(
                    ggml_vec_dot_q4_0_q8_0
                        as unsafe extern "C" fn(
                            libc::c_int,
                            *mut libc::c_float,
                            *const libc::c_void,
                            *const libc::c_void,
                        ) -> (),
                ),
                vec_dot_type: GGML_TYPE_Q8_0,
            };
            init
        },
        {
            let mut init = quantize_fns_t {
                dequantize_row_q: ::core::mem::transmute::<
                    Option<
                        unsafe extern "C" fn(
                            *const block_q4_1,
                            *mut libc::c_float,
                            libc::c_int,
                        ) -> (),
                    >,
                    dequantize_row_q_t,
                >(Some(
                    dequantize_row_q4_1
                        as unsafe extern "C" fn(
                            *const block_q4_1,
                            *mut libc::c_float,
                            libc::c_int,
                        ) -> (),
                )),
                quantize_row_q: Some(
                    quantize_row_q4_1
                        as unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut libc::c_void,
                            libc::c_int,
                        ) -> (),
                ),
                quantize_row_q_reference: ::core::mem::transmute::<
                    Option<
                        unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut block_q4_1,
                            libc::c_int,
                        ) -> (),
                    >,
                    quantize_row_q_t,
                >(Some(
                    quantize_row_q4_1_reference
                        as unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut block_q4_1,
                            libc::c_int,
                        ) -> (),
                )),
                quantize_row_q_dot: Some(
                    quantize_row_q8_1
                        as unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut libc::c_void,
                            libc::c_int,
                        ) -> (),
                ),
                vec_dot_q: Some(
                    ggml_vec_dot_q4_1_q8_1
                        as unsafe extern "C" fn(
                            libc::c_int,
                            *mut libc::c_float,
                            *const libc::c_void,
                            *const libc::c_void,
                        ) -> (),
                ),
                vec_dot_type: GGML_TYPE_Q8_1,
            };
            init
        },
        quantize_fns_t {
            dequantize_row_q: None,
            quantize_row_q: None,
            quantize_row_q_reference: None,
            quantize_row_q_dot: None,
            vec_dot_q: None,
            vec_dot_type: GGML_TYPE_F32,
        },
        quantize_fns_t {
            dequantize_row_q: None,
            quantize_row_q: None,
            quantize_row_q_reference: None,
            quantize_row_q_dot: None,
            vec_dot_q: None,
            vec_dot_type: GGML_TYPE_F32,
        },
        {
            let mut init = quantize_fns_t {
                dequantize_row_q: ::core::mem::transmute::<
                    Option<
                        unsafe extern "C" fn(
                            *const block_q5_0,
                            *mut libc::c_float,
                            libc::c_int,
                        ) -> (),
                    >,
                    dequantize_row_q_t,
                >(Some(
                    dequantize_row_q5_0
                        as unsafe extern "C" fn(
                            *const block_q5_0,
                            *mut libc::c_float,
                            libc::c_int,
                        ) -> (),
                )),
                quantize_row_q: Some(
                    quantize_row_q5_0
                        as unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut libc::c_void,
                            libc::c_int,
                        ) -> (),
                ),
                quantize_row_q_reference: ::core::mem::transmute::<
                    Option<
                        unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut block_q5_0,
                            libc::c_int,
                        ) -> (),
                    >,
                    quantize_row_q_t,
                >(Some(
                    quantize_row_q5_0_reference
                        as unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut block_q5_0,
                            libc::c_int,
                        ) -> (),
                )),
                quantize_row_q_dot: Some(
                    quantize_row_q8_0
                        as unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut libc::c_void,
                            libc::c_int,
                        ) -> (),
                ),
                vec_dot_q: Some(
                    ggml_vec_dot_q5_0_q8_0
                        as unsafe extern "C" fn(
                            libc::c_int,
                            *mut libc::c_float,
                            *const libc::c_void,
                            *const libc::c_void,
                        ) -> (),
                ),
                vec_dot_type: GGML_TYPE_Q8_0,
            };
            init
        },
        {
            let mut init = quantize_fns_t {
                dequantize_row_q: ::core::mem::transmute::<
                    Option<
                        unsafe extern "C" fn(
                            *const block_q5_1,
                            *mut libc::c_float,
                            libc::c_int,
                        ) -> (),
                    >,
                    dequantize_row_q_t,
                >(Some(
                    dequantize_row_q5_1
                        as unsafe extern "C" fn(
                            *const block_q5_1,
                            *mut libc::c_float,
                            libc::c_int,
                        ) -> (),
                )),
                quantize_row_q: Some(
                    quantize_row_q5_1
                        as unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut libc::c_void,
                            libc::c_int,
                        ) -> (),
                ),
                quantize_row_q_reference: ::core::mem::transmute::<
                    Option<
                        unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut block_q5_1,
                            libc::c_int,
                        ) -> (),
                    >,
                    quantize_row_q_t,
                >(Some(
                    quantize_row_q5_1_reference
                        as unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut block_q5_1,
                            libc::c_int,
                        ) -> (),
                )),
                quantize_row_q_dot: Some(
                    quantize_row_q8_1
                        as unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut libc::c_void,
                            libc::c_int,
                        ) -> (),
                ),
                vec_dot_q: Some(
                    ggml_vec_dot_q5_1_q8_1
                        as unsafe extern "C" fn(
                            libc::c_int,
                            *mut libc::c_float,
                            *const libc::c_void,
                            *const libc::c_void,
                        ) -> (),
                ),
                vec_dot_type: GGML_TYPE_Q8_1,
            };
            init
        },
        {
            let mut init = quantize_fns_t {
                dequantize_row_q: Some(
                    dequantize_row_q8_0
                        as unsafe extern "C" fn(
                            *const libc::c_void,
                            *mut libc::c_float,
                            libc::c_int,
                        ) -> (),
                ),
                quantize_row_q: Some(
                    quantize_row_q8_0
                        as unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut libc::c_void,
                            libc::c_int,
                        ) -> (),
                ),
                quantize_row_q_reference: ::core::mem::transmute::<
                    Option<
                        unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut block_q8_0,
                            libc::c_int,
                        ) -> (),
                    >,
                    quantize_row_q_t,
                >(Some(
                    quantize_row_q8_0_reference
                        as unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut block_q8_0,
                            libc::c_int,
                        ) -> (),
                )),
                quantize_row_q_dot: Some(
                    quantize_row_q8_0
                        as unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut libc::c_void,
                            libc::c_int,
                        ) -> (),
                ),
                vec_dot_q: Some(
                    ggml_vec_dot_q8_0_q8_0
                        as unsafe extern "C" fn(
                            libc::c_int,
                            *mut libc::c_float,
                            *const libc::c_void,
                            *const libc::c_void,
                        ) -> (),
                ),
                vec_dot_type: GGML_TYPE_Q8_0,
            };
            init
        },
        {
            let mut init = quantize_fns_t {
                dequantize_row_q: None,
                quantize_row_q: Some(
                    quantize_row_q8_1
                        as unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut libc::c_void,
                            libc::c_int,
                        ) -> (),
                ),
                quantize_row_q_reference: ::core::mem::transmute::<
                    Option<
                        unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut block_q8_1,
                            libc::c_int,
                        ) -> (),
                    >,
                    quantize_row_q_t,
                >(Some(
                    quantize_row_q8_1_reference
                        as unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut block_q8_1,
                            libc::c_int,
                        ) -> (),
                )),
                quantize_row_q_dot: Some(
                    quantize_row_q8_1
                        as unsafe extern "C" fn(
                            *const libc::c_float,
                            *mut libc::c_void,
                            libc::c_int,
                        ) -> (),
                ),
                vec_dot_q: None,
                vec_dot_type: GGML_TYPE_Q8_1,
            };
            init
        },
        quantize_fns_t {
            dequantize_row_q: None,
            quantize_row_q: None,
            quantize_row_q_reference: None,
            quantize_row_q_dot: None,
            vec_dot_q: None,
            vec_dot_type: GGML_TYPE_F32,
        },
        quantize_fns_t {
            dequantize_row_q: None,
            quantize_row_q: None,
            quantize_row_q_reference: None,
            quantize_row_q_dot: None,
            vec_dot_q: None,
            vec_dot_type: GGML_TYPE_F32,
        },
        quantize_fns_t {
            dequantize_row_q: None,
            quantize_row_q: None,
            quantize_row_q_reference: None,
            quantize_row_q_dot: None,
            vec_dot_q: None,
            vec_dot_type: GGML_TYPE_F32,
        },
    ]
};
#[no_mangle]
pub unsafe extern "C" fn ggml_internal_get_quantize_fn(mut i: size_t) -> quantize_fns_t {
    if !(i < GGML_TYPE_COUNT as libc::c_int as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            1526 as libc::c_int,
            b"i < GGML_TYPE_COUNT\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    return quantize_fns[i as usize];
}
#[inline]
unsafe extern "C" fn ggml_vec_set_i8(n: libc::c_int, mut x: *mut int8_t, v: int8_t) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *x.offset(i as isize) = v;
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_set_i16(n: libc::c_int, mut x: *mut int16_t, v: int16_t) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *x.offset(i as isize) = v;
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_set_i32(n: libc::c_int, mut x: *mut int32_t, v: int32_t) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *x.offset(i as isize) = v;
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_set_f16(n: libc::c_int, mut x: *mut ggml_fp16_t, v: int32_t) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *x.offset(i as isize) = v as ggml_fp16_t;
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_add_f32(
    n: libc::c_int,
    mut z: *mut libc::c_float,
    mut x: *const libc::c_float,
    mut y: *const libc::c_float,
) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *z.offset(i as isize) = *x.offset(i as isize) + *y.offset(i as isize);
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_add1_f32(
    n: libc::c_int,
    mut z: *mut libc::c_float,
    mut x: *const libc::c_float,
    v: libc::c_float,
) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *z.offset(i as isize) = *x.offset(i as isize) + v;
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_acc_f32(
    n: libc::c_int,
    mut y: *mut libc::c_float,
    mut x: *const libc::c_float,
) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *y.offset(i as isize) += *x.offset(i as isize);
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_acc1_f32(
    n: libc::c_int,
    mut y: *mut libc::c_float,
    v: libc::c_float,
) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *y.offset(i as isize) += v;
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_sub_f32(
    n: libc::c_int,
    mut z: *mut libc::c_float,
    mut x: *const libc::c_float,
    mut y: *const libc::c_float,
) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *z.offset(i as isize) = *x.offset(i as isize) - *y.offset(i as isize);
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_set_f32(n: libc::c_int, mut x: *mut libc::c_float, v: libc::c_float) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *x.offset(i as isize) = v;
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_cpy_f32(
    n: libc::c_int,
    mut y: *mut libc::c_float,
    mut x: *const libc::c_float,
) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *y.offset(i as isize) = *x.offset(i as isize);
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_neg_f32(
    n: libc::c_int,
    mut y: *mut libc::c_float,
    mut x: *const libc::c_float,
) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *y.offset(i as isize) = -*x.offset(i as isize);
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_mul_f32(
    n: libc::c_int,
    mut z: *mut libc::c_float,
    mut x: *const libc::c_float,
    mut y: *const libc::c_float,
) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *z.offset(i as isize) = *x.offset(i as isize) * *y.offset(i as isize);
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_div_f32(
    n: libc::c_int,
    mut z: *mut libc::c_float,
    mut x: *const libc::c_float,
    mut y: *const libc::c_float,
) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *z.offset(i as isize) = *x.offset(i as isize) / *y.offset(i as isize);
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_dot_f32(
    n: libc::c_int,
    mut s: *mut libc::c_float,
    mut x: *const libc::c_float,
    mut y: *const libc::c_float,
) {
    let mut sumf: libc::c_float = 0.0f32;
    let np: libc::c_int = n & !(32 as libc::c_int - 1 as libc::c_int);
    let mut sum: [__m256; 4] = [
        _mm256_setzero_ps(),
        _mm256_setzero_ps(),
        _mm256_setzero_ps(),
        _mm256_setzero_ps(),
    ];
    let mut ax: [__m256; 4] = [_mm256_setzero_ps(); 4];
    let mut ay: [__m256; 4] = [_mm256_setzero_ps(); 4];
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < np {
        let mut j: libc::c_int = 0 as libc::c_int;
        while j < 32 as libc::c_int / 8 as libc::c_int {
            ax[j as usize] =
                _mm256_loadu_ps(x.offset(i as isize).offset((j * 8 as libc::c_int) as isize));
            ay[j as usize] =
                _mm256_loadu_ps(y.offset(i as isize).offset((j * 8 as libc::c_int) as isize));
            sum[j as usize] = _mm256_fmadd_ps(ax[j as usize], ay[j as usize], sum[j as usize]);
            j += 1;
            j;
        }
        i += 32 as libc::c_int;
    }
    let mut i_0: libc::c_int = 0 as libc::c_int;
    while i_0 < 32 as libc::c_int / 8 as libc::c_int / 2 as libc::c_int {
        sum[(2 as libc::c_int * i_0) as usize] = _mm256_add_ps(
            sum[(2 as libc::c_int * i_0) as usize],
            sum[(2 as libc::c_int * i_0 + 1 as libc::c_int) as usize],
        );
        i_0 += 1;
        i_0;
    }
    let mut i_1: libc::c_int = 0 as libc::c_int;
    while i_1 < 32 as libc::c_int / 8 as libc::c_int / 4 as libc::c_int {
        sum[(4 as libc::c_int * i_1) as usize] = _mm256_add_ps(
            sum[(4 as libc::c_int * i_1) as usize],
            sum[(4 as libc::c_int * i_1 + 2 as libc::c_int) as usize],
        );
        i_1 += 1;
        i_1;
    }
    let mut i_2: libc::c_int = 0 as libc::c_int;
    while i_2 < 32 as libc::c_int / 8 as libc::c_int / 8 as libc::c_int {
        sum[(8 as libc::c_int * i_2) as usize] = _mm256_add_ps(
            sum[(8 as libc::c_int * i_2) as usize],
            sum[(8 as libc::c_int * i_2 + 4 as libc::c_int) as usize],
        );
        i_2 += 1;
        i_2;
    }
    let t0: __m128 = _mm_add_ps(
        _mm256_castps256_ps128(sum[0 as libc::c_int as usize]),
        _mm256_extractf128_ps(sum[0 as libc::c_int as usize], 1 as libc::c_int),
    );
    let t1: __m128 = _mm_hadd_ps(t0, t0);
    sumf = _mm_cvtss_f32(_mm_hadd_ps(t1, t1));
    let mut i_3: libc::c_int = np;
    while i_3 < n {
        sumf += *x.offset(i_3 as isize) * *y.offset(i_3 as isize);
        i_3 += 1;
        i_3;
    }
    *s = sumf;
}
#[inline]
unsafe extern "C" fn ggml_vec_dot_f16(
    n: libc::c_int,
    mut s: *mut libc::c_float,
    mut x: *mut ggml_fp16_t,
    mut y: *mut ggml_fp16_t,
) {
    let mut sumf: ggml_float = 0.0f64;
    let np: libc::c_int = n & !(32 as libc::c_int - 1 as libc::c_int);
    let mut sum: [__m256; 4] = [
        _mm256_setzero_ps(),
        _mm256_setzero_ps(),
        _mm256_setzero_ps(),
        _mm256_setzero_ps(),
    ];
    let mut ax: [__m256; 4] = [_mm256_setzero_ps(); 4];
    let mut ay: [__m256; 4] = [_mm256_setzero_ps(); 4];
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < np {
        let mut j: libc::c_int = 0 as libc::c_int;
        while j < 32 as libc::c_int / 8 as libc::c_int {
            ax[j as usize] = _mm256_cvtph_ps(_mm_loadu_si128(
                x.offset(i as isize).offset((j * 8 as libc::c_int) as isize) as *mut __m128i,
            ));
            ay[j as usize] = _mm256_cvtph_ps(_mm_loadu_si128(
                y.offset(i as isize).offset((j * 8 as libc::c_int) as isize) as *mut __m128i,
            ));
            sum[j as usize] = _mm256_fmadd_ps(ax[j as usize], ay[j as usize], sum[j as usize]);
            j += 1;
            j;
        }
        i += 32 as libc::c_int;
    }
    let mut i_0: libc::c_int = 0 as libc::c_int;
    while i_0 < 32 as libc::c_int / 8 as libc::c_int / 2 as libc::c_int {
        sum[(2 as libc::c_int * i_0) as usize] = _mm256_add_ps(
            sum[(2 as libc::c_int * i_0) as usize],
            sum[(2 as libc::c_int * i_0 + 1 as libc::c_int) as usize],
        );
        i_0 += 1;
        i_0;
    }
    let mut i_1: libc::c_int = 0 as libc::c_int;
    while i_1 < 32 as libc::c_int / 8 as libc::c_int / 4 as libc::c_int {
        sum[(4 as libc::c_int * i_1) as usize] = _mm256_add_ps(
            sum[(4 as libc::c_int * i_1) as usize],
            sum[(4 as libc::c_int * i_1 + 2 as libc::c_int) as usize],
        );
        i_1 += 1;
        i_1;
    }
    let mut i_2: libc::c_int = 0 as libc::c_int;
    while i_2 < 32 as libc::c_int / 8 as libc::c_int / 8 as libc::c_int {
        sum[(8 as libc::c_int * i_2) as usize] = _mm256_add_ps(
            sum[(8 as libc::c_int * i_2) as usize],
            sum[(8 as libc::c_int * i_2 + 4 as libc::c_int) as usize],
        );
        i_2 += 1;
        i_2;
    }
    let t0: __m128 = _mm_add_ps(
        _mm256_castps256_ps128(sum[0 as libc::c_int as usize]),
        _mm256_extractf128_ps(sum[0 as libc::c_int as usize], 1 as libc::c_int),
    );
    let t1: __m128 = _mm_hadd_ps(t0, t0);
    sumf = _mm_cvtss_f32(_mm_hadd_ps(t1, t1)) as ggml_float;
    let mut i_3: libc::c_int = np;
    while i_3 < n {
        sumf += (ggml_lookup_fp16_to_fp32(*x.offset(i_3 as isize))
            * ggml_lookup_fp16_to_fp32(*y.offset(i_3 as isize))) as ggml_float;
        i_3 += 1;
        i_3;
    }
    *s = sumf as libc::c_float;
}
unsafe extern "C" fn ggml_vec_dot_q4_0_q8_0(
    n: libc::c_int,
    mut s: *mut libc::c_float,
    mut vx: *const libc::c_void,
    mut vy: *const libc::c_void,
) {
    let qk: libc::c_int = 32 as libc::c_int;
    let nb: libc::c_int = n / qk;
    let mut x: *const block_q4_0 = vx as *const block_q4_0;
    let mut y: *const block_q8_0 = vy as *const block_q8_0;
    let mut acc: __m256 = _mm256_setzero_ps();
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nb {
        let d: __m256 = _mm256_set1_ps(
            ggml_lookup_fp16_to_fp32((*x.offset(i as isize)).d)
                * ggml_lookup_fp16_to_fp32((*y.offset(i as isize)).d),
        );
        let mut bx: __m256i = bytes_from_nibbles_32(((*x.offset(i as isize)).qs).as_ptr());
        let off: __m256i = _mm256_set1_epi8(8 as libc::c_int as libc::c_char);
        bx = _mm256_sub_epi8(bx, off);
        let mut by: __m256i =
            _mm256_loadu_si256(((*y.offset(i as isize)).qs).as_ptr() as *const __m256i);
        let q: __m256 = mul_sum_i8_pairs_float(bx, by);
        acc = _mm256_fmadd_ps(d, q, acc);
        i += 1;
        i;
    }
    *s = hsum_float_8(acc);
}
unsafe extern "C" fn ggml_vec_dot_q4_1_q8_1(
    n: libc::c_int,
    mut s: *mut libc::c_float,
    mut vx: *const libc::c_void,
    mut vy: *const libc::c_void,
) {
    let qk: libc::c_int = 32 as libc::c_int;
    let nb: libc::c_int = n / qk;
    let mut x: *const block_q4_1 = vx as *const block_q4_1;
    let mut y: *const block_q8_1 = vy as *const block_q8_1;
    let mut acc: __m256 = _mm256_setzero_ps();
    let mut summs: libc::c_float = 0 as libc::c_int as libc::c_float;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nb {
        let d0: libc::c_float = ggml_lookup_fp16_to_fp32((*x.offset(i as isize)).d);
        let d1: libc::c_float = (*y.offset(i as isize)).d;
        summs += ggml_lookup_fp16_to_fp32((*x.offset(i as isize)).m) * (*y.offset(i as isize)).s;
        let d0v: __m256 = _mm256_set1_ps(d0);
        let d1v: __m256 = _mm256_set1_ps(d1);
        let d0d1: __m256 = _mm256_mul_ps(d0v, d1v);
        let bx: __m256i = bytes_from_nibbles_32(((*x.offset(i as isize)).qs).as_ptr());
        let by: __m256i =
            _mm256_loadu_si256(((*y.offset(i as isize)).qs).as_ptr() as *const __m256i);
        let xy: __m256 = mul_sum_us8_pairs_float(bx, by);
        acc = _mm256_fmadd_ps(d0d1, xy, acc);
        i += 1;
        i;
    }
    *s = hsum_float_8(acc) + summs;
}
unsafe extern "C" fn ggml_vec_dot_q5_0_q8_0(
    n: libc::c_int,
    mut s: *mut libc::c_float,
    mut vx: *const libc::c_void,
    mut vy: *const libc::c_void,
) {
    let qk: libc::c_int = 32 as libc::c_int;
    let nb: libc::c_int = n / qk;
    let mut x: *const block_q5_0 = vx as *const block_q5_0;
    let mut y: *const block_q8_0 = vy as *const block_q8_0;
    let mut acc: __m256 = _mm256_setzero_ps();
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nb {
        let d: __m256 = _mm256_set1_ps(
            ggml_lookup_fp16_to_fp32((*x.offset(i as isize)).d)
                * ggml_lookup_fp16_to_fp32((*y.offset(i as isize)).d),
        );
        let mut bx: __m256i = bytes_from_nibbles_32(((*x.offset(i as isize)).qs).as_ptr());
        let mut bxhi: __m256i = bytes_from_bits_32(((*x.offset(i as isize)).qh).as_ptr());
        bxhi = _mm256_andnot_si256(bxhi, _mm256_set1_epi8(0xf0 as libc::c_int as libc::c_char));
        bx = _mm256_or_si256(bx, bxhi);
        let mut by: __m256i =
            _mm256_loadu_si256(((*y.offset(i as isize)).qs).as_ptr() as *const __m256i);
        let q: __m256 = mul_sum_i8_pairs_float(bx, by);
        acc = _mm256_fmadd_ps(d, q, acc);
        i += 1;
        i;
    }
    *s = hsum_float_8(acc);
}
unsafe extern "C" fn ggml_vec_dot_q5_1_q8_1(
    n: libc::c_int,
    mut s: *mut libc::c_float,
    mut vx: *const libc::c_void,
    mut vy: *const libc::c_void,
) {
    let qk: libc::c_int = 32 as libc::c_int;
    let nb: libc::c_int = n / qk;
    let mut x: *const block_q5_1 = vx as *const block_q5_1;
    let mut y: *const block_q8_1 = vy as *const block_q8_1;
    let mut acc: __m256 = _mm256_setzero_ps();
    let mut summs: libc::c_float = 0.0f32;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nb {
        let dx: __m256 = _mm256_set1_ps(ggml_lookup_fp16_to_fp32((*x.offset(i as isize)).d));
        summs += ggml_lookup_fp16_to_fp32((*x.offset(i as isize)).m) * (*y.offset(i as isize)).s;
        let mut bx: __m256i = bytes_from_nibbles_32(((*x.offset(i as isize)).qs).as_ptr());
        let mut bxhi: __m256i = bytes_from_bits_32(((*x.offset(i as isize)).qh).as_ptr());
        bxhi = _mm256_and_si256(bxhi, _mm256_set1_epi8(0x10 as libc::c_int as libc::c_char));
        bx = _mm256_or_si256(bx, bxhi);
        let dy: __m256 = _mm256_set1_ps((*y.offset(i as isize)).d);
        let by: __m256i =
            _mm256_loadu_si256(((*y.offset(i as isize)).qs).as_ptr() as *const __m256i);
        let q: __m256 = mul_sum_us8_pairs_float(bx, by);
        acc = _mm256_fmadd_ps(q, _mm256_mul_ps(dx, dy), acc);
        i += 1;
        i;
    }
    *s = hsum_float_8(acc) + summs;
}
unsafe extern "C" fn ggml_vec_dot_q8_0_q8_0(
    n: libc::c_int,
    mut s: *mut libc::c_float,
    mut vx: *const libc::c_void,
    mut vy: *const libc::c_void,
) {
    let qk: libc::c_int = 32 as libc::c_int;
    let nb: libc::c_int = n / qk;
    let mut x: *const block_q8_0 = vx as *const block_q8_0;
    let mut y: *const block_q8_0 = vy as *const block_q8_0;
    let mut acc: __m256 = _mm256_setzero_ps();
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nb {
        let d: __m256 = _mm256_set1_ps(
            ggml_lookup_fp16_to_fp32((*x.offset(i as isize)).d)
                * ggml_lookup_fp16_to_fp32((*y.offset(i as isize)).d),
        );
        let mut bx: __m256i =
            _mm256_loadu_si256(((*x.offset(i as isize)).qs).as_ptr() as *const __m256i);
        let mut by: __m256i =
            _mm256_loadu_si256(((*y.offset(i as isize)).qs).as_ptr() as *const __m256i);
        let q: __m256 = mul_sum_i8_pairs_float(bx, by);
        acc = _mm256_fmadd_ps(d, q, acc);
        i += 1;
        i;
    }
    *s = hsum_float_8(acc);
}
#[inline]
unsafe extern "C" fn ggml_vec_dot_f16_unroll(
    n: libc::c_int,
    xs: libc::c_int,
    mut s: *mut libc::c_float,
    mut xv: *mut libc::c_void,
    mut y: *mut ggml_fp16_t,
) {
    let mut sumf: [ggml_float; 2] = [0.0f64, 0.];
    let mut x: [*mut ggml_fp16_t; 2] = [0 as *mut ggml_fp16_t; 2];
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < 2 as libc::c_int {
        x[i as usize] = (xv as *mut libc::c_char).offset((i * xs) as isize) as *mut ggml_fp16_t;
        i += 1;
        i;
    }
    let np: libc::c_int = n & !(32 as libc::c_int - 1 as libc::c_int);
    let mut sum: [[__m256; 4]; 2] = [
        [
            _mm256_setzero_ps(),
            _mm256_setzero_ps(),
            _mm256_setzero_ps(),
            _mm256_setzero_ps(),
        ],
        [_mm256_setzero_ps(); 4],
    ];
    let mut ax: [__m256; 4] = [_mm256_setzero_ps(); 4];
    let mut ay: [__m256; 4] = [_mm256_setzero_ps(); 4];
    let mut i_0: libc::c_int = 0 as libc::c_int;
    while i_0 < np {
        let mut j: libc::c_int = 0 as libc::c_int;
        while j < 32 as libc::c_int / 8 as libc::c_int {
            ay[j as usize] = _mm256_cvtph_ps(_mm_loadu_si128(
                y.offset(i_0 as isize)
                    .offset((j * 8 as libc::c_int) as isize) as *mut __m128i,
            ));
            let mut k: libc::c_int = 0 as libc::c_int;
            while k < 2 as libc::c_int {
                ax[j as usize] = _mm256_cvtph_ps(_mm_loadu_si128(
                    (x[k as usize])
                        .offset(i_0 as isize)
                        .offset((j * 8 as libc::c_int) as isize)
                        as *mut __m128i,
                ));
                sum[k as usize][j as usize] =
                    _mm256_fmadd_ps(ax[j as usize], ay[j as usize], sum[k as usize][j as usize]);
                k += 1;
                k;
            }
            j += 1;
            j;
        }
        i_0 += 32 as libc::c_int;
    }
    let mut k_0: libc::c_int = 0 as libc::c_int;
    while k_0 < 2 as libc::c_int {
        let mut i_1: libc::c_int = 0 as libc::c_int;
        while i_1 < 32 as libc::c_int / 8 as libc::c_int / 2 as libc::c_int {
            sum[k_0 as usize][(2 as libc::c_int * i_1) as usize] = _mm256_add_ps(
                sum[k_0 as usize][(2 as libc::c_int * i_1) as usize],
                sum[k_0 as usize][(2 as libc::c_int * i_1 + 1 as libc::c_int) as usize],
            );
            i_1 += 1;
            i_1;
        }
        let mut i_2: libc::c_int = 0 as libc::c_int;
        while i_2 < 32 as libc::c_int / 8 as libc::c_int / 4 as libc::c_int {
            sum[k_0 as usize][(4 as libc::c_int * i_2) as usize] = _mm256_add_ps(
                sum[k_0 as usize][(4 as libc::c_int * i_2) as usize],
                sum[k_0 as usize][(4 as libc::c_int * i_2 + 2 as libc::c_int) as usize],
            );
            i_2 += 1;
            i_2;
        }
        let mut i_3: libc::c_int = 0 as libc::c_int;
        while i_3 < 32 as libc::c_int / 8 as libc::c_int / 8 as libc::c_int {
            sum[k_0 as usize][(8 as libc::c_int * i_3) as usize] = _mm256_add_ps(
                sum[k_0 as usize][(8 as libc::c_int * i_3) as usize],
                sum[k_0 as usize][(8 as libc::c_int * i_3 + 4 as libc::c_int) as usize],
            );
            i_3 += 1;
            i_3;
        }
        let t0: __m128 = _mm_add_ps(
            _mm256_castps256_ps128(sum[k_0 as usize][0 as libc::c_int as usize]),
            _mm256_extractf128_ps(
                sum[k_0 as usize][0 as libc::c_int as usize],
                1 as libc::c_int,
            ),
        );
        let t1: __m128 = _mm_hadd_ps(t0, t0);
        sumf[k_0 as usize] = _mm_cvtss_f32(_mm_hadd_ps(t1, t1)) as ggml_float;
        k_0 += 1;
        k_0;
    }
    let mut i_4: libc::c_int = np;
    while i_4 < n {
        let mut j_0: libc::c_int = 0 as libc::c_int;
        while j_0 < 2 as libc::c_int {
            sumf[j_0 as usize] +=
                (ggml_lookup_fp16_to_fp32(*(x[j_0 as usize]).offset(i_4 as isize))
                    * ggml_lookup_fp16_to_fp32(*y.offset(i_4 as isize)))
                    as ggml_float;
            j_0 += 1;
            j_0;
        }
        i_4 += 1;
        i_4;
    }
    let mut i_5: libc::c_int = 0 as libc::c_int;
    while i_5 < 2 as libc::c_int {
        *s.offset(i_5 as isize) = sumf[i_5 as usize] as libc::c_float;
        i_5 += 1;
        i_5;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_mad_f32(
    n: libc::c_int,
    mut y: *mut libc::c_float,
    mut x: *const libc::c_float,
    v: libc::c_float,
) {
    let np: libc::c_int = n & !(32 as libc::c_int - 1 as libc::c_int);
    let mut vx: __m256 = _mm256_set1_ps(v);
    let mut ax: [__m256; 4] = [_mm256_setzero_ps(); 4];
    let mut ay: [__m256; 4] = [_mm256_setzero_ps(); 4];
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < np {
        let mut j: libc::c_int = 0 as libc::c_int;
        while j < 32 as libc::c_int / 8 as libc::c_int {
            ax[j as usize] =
                _mm256_loadu_ps(x.offset(i as isize).offset((j * 8 as libc::c_int) as isize));
            ay[j as usize] =
                _mm256_loadu_ps(y.offset(i as isize).offset((j * 8 as libc::c_int) as isize));
            ay[j as usize] = _mm256_fmadd_ps(ax[j as usize], vx, ay[j as usize]);
            _mm256_storeu_ps(
                y.offset(i as isize).offset((j * 8 as libc::c_int) as isize),
                ay[j as usize],
            );
            j += 1;
            j;
        }
        i += 32 as libc::c_int;
    }
    let mut i_0: libc::c_int = np;
    while i_0 < n {
        *y.offset(i_0 as isize) += *x.offset(i_0 as isize) * v;
        i_0 += 1;
        i_0;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_scale_f32(
    n: libc::c_int,
    mut y: *mut libc::c_float,
    v: libc::c_float,
) {
    let np: libc::c_int = n & !(32 as libc::c_int - 1 as libc::c_int);
    let mut vx: __m256 = _mm256_set1_ps(v);
    let mut ay: [__m256; 4] = [_mm256_setzero_ps(); 4];
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < np {
        let mut j: libc::c_int = 0 as libc::c_int;
        while j < 32 as libc::c_int / 8 as libc::c_int {
            ay[j as usize] =
                _mm256_loadu_ps(y.offset(i as isize).offset((j * 8 as libc::c_int) as isize));
            ay[j as usize] = _mm256_mul_ps(ay[j as usize], vx);
            _mm256_storeu_ps(
                y.offset(i as isize).offset((j * 8 as libc::c_int) as isize),
                ay[j as usize],
            );
            j += 1;
            j;
        }
        i += 32 as libc::c_int;
    }
    let mut i_0: libc::c_int = np;
    while i_0 < n {
        *y.offset(i_0 as isize) *= v;
        i_0 += 1;
        i_0;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_norm_f32(
    n: libc::c_int,
    mut s: *mut libc::c_float,
    mut x: *const libc::c_float,
) {
    ggml_vec_dot_f32(n, s, x, x);
    *s = sqrtf(*s);
}
#[inline]
unsafe extern "C" fn ggml_vec_sqr_f32(
    n: libc::c_int,
    mut y: *mut libc::c_float,
    mut x: *const libc::c_float,
) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *y.offset(i as isize) = *x.offset(i as isize) * *x.offset(i as isize);
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_sqrt_f32(
    n: libc::c_int,
    mut y: *mut libc::c_float,
    mut x: *const libc::c_float,
) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *y.offset(i as isize) = sqrtf(*x.offset(i as isize));
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_log_f32(
    n: libc::c_int,
    mut y: *mut libc::c_float,
    mut x: *const libc::c_float,
) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *y.offset(i as isize) = logf(*x.offset(i as isize));
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_abs_f32(
    n: libc::c_int,
    mut y: *mut libc::c_float,
    mut x: *const libc::c_float,
) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *y.offset(i as isize) = fabsf(*x.offset(i as isize));
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_sgn_f32(
    n: libc::c_int,
    mut y: *mut libc::c_float,
    mut x: *const libc::c_float,
) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *y.offset(i as isize) = if *x.offset(i as isize) > 0.0f32 {
            1.0f32
        } else if *x.offset(i as isize) < 0.0f32 {
            -1.0f32
        } else {
            0.0f32
        };
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_step_f32(
    n: libc::c_int,
    mut y: *mut libc::c_float,
    mut x: *const libc::c_float,
) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *y.offset(i as isize) = if *x.offset(i as isize) > 0.0f32 {
            1.0f32
        } else {
            0.0f32
        };
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_relu_f32(
    n: libc::c_int,
    mut y: *mut libc::c_float,
    mut x: *const libc::c_float,
) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *y.offset(i as isize) = if *x.offset(i as isize) > 0.0f32 {
            *x.offset(i as isize)
        } else {
            0.0f32
        };
        i += 1;
        i;
    }
}
static mut GELU_COEF_A: libc::c_float = 0.044715f32;
static mut SQRT_2_OVER_PI: libc::c_float = 0.79788456080286535587989211986876f32;
#[inline]
unsafe extern "C" fn ggml_gelu_f32(mut x: libc::c_float) -> libc::c_float {
    return 0.5f32 * x * (1.0f32 + tanhf(SQRT_2_OVER_PI * x * (1.0f32 + GELU_COEF_A * x * x)));
}
#[inline]
unsafe extern "C" fn ggml_vec_gelu_f16(
    n: libc::c_int,
    mut y: *mut ggml_fp16_t,
    mut x: *const ggml_fp16_t,
) {
    let mut i16: *const uint16_t = x as *const uint16_t;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        *y.offset(i as isize) = table_gelu_f16[*i16.offset(i as isize) as usize];
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_gelu_f32(
    n: libc::c_int,
    mut y: *mut libc::c_float,
    mut x: *const libc::c_float,
) {
    let mut t: uint16_t = 0;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        let mut fp16: ggml_fp16_t = ({
            ::core::mem::transmute::<
                _,
                [libc::c_short;
                    ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
            >(_mm_cvtps_ph(
                _mm_setr_ps(
                    *x.offset(i as isize),
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                ),
                0 as libc::c_int,
            ))[0 as libc::c_int as usize] as libc::c_ushort
        });
        memcpy(
            &mut t as *mut uint16_t as *mut libc::c_void,
            &mut fp16 as *mut ggml_fp16_t as *const libc::c_void,
            ::core::mem::size_of::<uint16_t>() as libc::c_ulong,
        );
        *y.offset(i as isize) = ggml_lookup_fp16_to_fp32(table_gelu_f16[t as usize]);
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_silu_f32(mut x: libc::c_float) -> libc::c_float {
    return x / (1.0f32 + expf(-x));
}
#[inline]
unsafe extern "C" fn ggml_vec_silu_f32(
    n: libc::c_int,
    mut y: *mut libc::c_float,
    mut x: *const libc::c_float,
) {
    let mut t: uint16_t = 0;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        let mut fp16: ggml_fp16_t = ({
            ::core::mem::transmute::<
                _,
                [libc::c_short;
                    ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
            >(_mm_cvtps_ph(
                _mm_setr_ps(
                    *x.offset(i as isize),
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                ),
                0 as libc::c_int,
            ))[0 as libc::c_int as usize] as libc::c_ushort
        });
        memcpy(
            &mut t as *mut uint16_t as *mut libc::c_void,
            &mut fp16 as *mut ggml_fp16_t as *const libc::c_void,
            ::core::mem::size_of::<uint16_t>() as libc::c_ulong,
        );
        *y.offset(i as isize) = ggml_lookup_fp16_to_fp32(table_silu_f16[t as usize]);
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_silu_backward_f32(
    mut x: libc::c_float,
    mut dy: libc::c_float,
) -> libc::c_float {
    let s: libc::c_float = 1.0f32 / (1.0f32 + expf(-x));
    return dy * s * (1.0f32 + x * (1.0f32 - s));
}
#[inline]
unsafe extern "C" fn ggml_vec_silu_backward_f32(
    n: libc::c_int,
    mut dx: *mut libc::c_float,
    mut x: *const libc::c_float,
    mut dy: *const libc::c_float,
) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        let mut fp16: ggml_fp16_t = ({
            ::core::mem::transmute::<
                _,
                [libc::c_short;
                    ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
            >(_mm_cvtps_ph(
                _mm_setr_ps(
                    *x.offset(i as isize),
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                    0 as libc::c_int as libc::c_float,
                ),
                0 as libc::c_int,
            ))[0 as libc::c_int as usize] as libc::c_ushort
        });
        let mut usedx: libc::c_float = ggml_lookup_fp16_to_fp32(fp16);
        *dx.offset(i as isize) = ggml_silu_backward_f32(usedx, *dy.offset(i as isize));
        i += 1;
        i;
    }
}
#[inline]
unsafe extern "C" fn ggml_vec_sum_f32(
    n: libc::c_int,
    mut s: *mut libc::c_float,
    mut x: *const libc::c_float,
) {
    let mut sum: ggml_float = 0.0f64;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        sum += *x.offset(i as isize) as ggml_float;
        i += 1;
        i;
    }
    *s = sum as libc::c_float;
}
#[inline]
unsafe extern "C" fn ggml_vec_sum_ggf(
    n: libc::c_int,
    mut s: *mut ggml_float,
    mut x: *const libc::c_float,
) {
    let mut sum: ggml_float = 0.0f64;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        sum += *x.offset(i as isize) as ggml_float;
        i += 1;
        i;
    }
    *s = sum;
}
#[inline]
unsafe extern "C" fn ggml_vec_max_f32(
    n: libc::c_int,
    mut s: *mut libc::c_float,
    mut x: *const libc::c_float,
) {
    let mut max: libc::c_float = -::core::f32::INFINITY;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        max = if max > *x.offset(i as isize) {
            max
        } else {
            *x.offset(i as isize)
        };
        i += 1;
        i;
    }
    *s = max;
}
#[inline]
unsafe extern "C" fn ggml_vec_norm_inv_f32(
    n: libc::c_int,
    mut s: *mut libc::c_float,
    mut x: *const libc::c_float,
) {
    ggml_vec_norm_f32(n, s, x);
    *s = 1.0f32 / *s;
}
static mut GGML_BLCK_SIZE: [libc::c_int; 13] = [
    1 as libc::c_int,
    1 as libc::c_int,
    32 as libc::c_int,
    32 as libc::c_int,
    0,
    0,
    32 as libc::c_int,
    32 as libc::c_int,
    32 as libc::c_int,
    32 as libc::c_int,
    1 as libc::c_int,
    1 as libc::c_int,
    1 as libc::c_int,
];
static mut GGML_TYPE_SIZE: [size_t; 13] = [
    ::core::mem::size_of::<libc::c_float>() as libc::c_ulong,
    ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong,
    ::core::mem::size_of::<block_q4_0>() as libc::c_ulong,
    ::core::mem::size_of::<block_q4_1>() as libc::c_ulong,
    0,
    0,
    ::core::mem::size_of::<block_q5_0>() as libc::c_ulong,
    ::core::mem::size_of::<block_q5_1>() as libc::c_ulong,
    ::core::mem::size_of::<block_q8_0>() as libc::c_ulong,
    ::core::mem::size_of::<block_q8_1>() as libc::c_ulong,
    ::core::mem::size_of::<int8_t>() as libc::c_ulong,
    ::core::mem::size_of::<int16_t>() as libc::c_ulong,
    ::core::mem::size_of::<int32_t>() as libc::c_ulong,
];
static mut GGML_TYPE_NAME: [*const libc::c_char; 13] = [
    b"f32\0" as *const u8 as *const libc::c_char,
    b"f16\0" as *const u8 as *const libc::c_char,
    b"q4_0\0" as *const u8 as *const libc::c_char,
    b"q4_1\0" as *const u8 as *const libc::c_char,
    0 as *const libc::c_char,
    0 as *const libc::c_char,
    b"q5_0\0" as *const u8 as *const libc::c_char,
    b"q5_1\0" as *const u8 as *const libc::c_char,
    b"q8_0\0" as *const u8 as *const libc::c_char,
    b"q8_1\0" as *const u8 as *const libc::c_char,
    b"i8\0" as *const u8 as *const libc::c_char,
    b"i16\0" as *const u8 as *const libc::c_char,
    b"i32\0" as *const u8 as *const libc::c_char,
];
static mut GGML_IS_QUANTIZED: [bool; 13] = [
    0 as libc::c_int != 0,
    0 as libc::c_int != 0,
    1 as libc::c_int != 0,
    1 as libc::c_int != 0,
    false,
    false,
    1 as libc::c_int != 0,
    1 as libc::c_int != 0,
    1 as libc::c_int != 0,
    1 as libc::c_int != 0,
    0 as libc::c_int != 0,
    0 as libc::c_int != 0,
    0 as libc::c_int != 0,
];
static mut GGML_OP_LABEL: [*const libc::c_char; 51] = [
    b"NONE\0" as *const u8 as *const libc::c_char,
    b"DUP\0" as *const u8 as *const libc::c_char,
    b"ADD\0" as *const u8 as *const libc::c_char,
    b"ADD1\0" as *const u8 as *const libc::c_char,
    b"ACC\0" as *const u8 as *const libc::c_char,
    b"SUB\0" as *const u8 as *const libc::c_char,
    b"MUL\0" as *const u8 as *const libc::c_char,
    b"DIV\0" as *const u8 as *const libc::c_char,
    b"SQR\0" as *const u8 as *const libc::c_char,
    b"SQRT\0" as *const u8 as *const libc::c_char,
    b"LOG\0" as *const u8 as *const libc::c_char,
    b"SUM\0" as *const u8 as *const libc::c_char,
    b"SUM_ROWS\0" as *const u8 as *const libc::c_char,
    b"MEAN\0" as *const u8 as *const libc::c_char,
    b"REPEAT\0" as *const u8 as *const libc::c_char,
    b"ABS\0" as *const u8 as *const libc::c_char,
    b"SGN\0" as *const u8 as *const libc::c_char,
    b"NEG\0" as *const u8 as *const libc::c_char,
    b"STEP\0" as *const u8 as *const libc::c_char,
    b"RELU\0" as *const u8 as *const libc::c_char,
    b"GELU\0" as *const u8 as *const libc::c_char,
    b"SILU\0" as *const u8 as *const libc::c_char,
    b"SILU_BACK\0" as *const u8 as *const libc::c_char,
    b"NORM\0" as *const u8 as *const libc::c_char,
    b"RMS_NORM\0" as *const u8 as *const libc::c_char,
    b"RMS_NORM_BACK\0" as *const u8 as *const libc::c_char,
    b"MUL_MAT\0" as *const u8 as *const libc::c_char,
    b"SCALE\0" as *const u8 as *const libc::c_char,
    b"SET\0" as *const u8 as *const libc::c_char,
    b"CPY\0" as *const u8 as *const libc::c_char,
    b"CONT\0" as *const u8 as *const libc::c_char,
    b"RESHAPE\0" as *const u8 as *const libc::c_char,
    b"VIEW\0" as *const u8 as *const libc::c_char,
    b"PERMUTE\0" as *const u8 as *const libc::c_char,
    b"TRANSPOSE\0" as *const u8 as *const libc::c_char,
    b"GET_ROWS\0" as *const u8 as *const libc::c_char,
    b"GET_ROWS_BACK\0" as *const u8 as *const libc::c_char,
    b"DIAG\0" as *const u8 as *const libc::c_char,
    b"DIAG_MASK_INF\0" as *const u8 as *const libc::c_char,
    b"DIAG_MASK_ZERO\0" as *const u8 as *const libc::c_char,
    b"SOFT_MAX\0" as *const u8 as *const libc::c_char,
    b"ROPE\0" as *const u8 as *const libc::c_char,
    b"ROPE_BACK\0" as *const u8 as *const libc::c_char,
    b"ALIBI\0" as *const u8 as *const libc::c_char,
    b"CLAMP\0" as *const u8 as *const libc::c_char,
    b"CONV_1D_1S\0" as *const u8 as *const libc::c_char,
    b"CONV_1D_2S\0" as *const u8 as *const libc::c_char,
    b"FLASH_ATTN\0" as *const u8 as *const libc::c_char,
    b"FLASH_FF\0" as *const u8 as *const libc::c_char,
    b"MAP_UNARY\0" as *const u8 as *const libc::c_char,
    b"MAP_BINARY\0" as *const u8 as *const libc::c_char,
];
static mut GGML_OP_SYMBOL: [*const libc::c_char; 51] = [
    b"none\0" as *const u8 as *const libc::c_char,
    b"x\0" as *const u8 as *const libc::c_char,
    b"x+y\0" as *const u8 as *const libc::c_char,
    b"x+y\0" as *const u8 as *const libc::c_char,
    b"view(x,nb,offset)+=y->x\0" as *const u8 as *const libc::c_char,
    b"x-y\0" as *const u8 as *const libc::c_char,
    b"x*y\0" as *const u8 as *const libc::c_char,
    b"x/y\0" as *const u8 as *const libc::c_char,
    b"x^2\0" as *const u8 as *const libc::c_char,
    b"\xE2\x88\x9Ax\0" as *const u8 as *const libc::c_char,
    b"log(x)\0" as *const u8 as *const libc::c_char,
    b"\xCE\xA3x\0" as *const u8 as *const libc::c_char,
    b"\xCE\xA3x_k\0" as *const u8 as *const libc::c_char,
    b"\xCE\xA3x/n\0" as *const u8 as *const libc::c_char,
    b"repeat(x)\0" as *const u8 as *const libc::c_char,
    b"abs(x)\0" as *const u8 as *const libc::c_char,
    b"sgn(x)\0" as *const u8 as *const libc::c_char,
    b"-x\0" as *const u8 as *const libc::c_char,
    b"step(x)\0" as *const u8 as *const libc::c_char,
    b"relu(x)\0" as *const u8 as *const libc::c_char,
    b"gelu(x)\0" as *const u8 as *const libc::c_char,
    b"silu(x)\0" as *const u8 as *const libc::c_char,
    b"silu_back(x)\0" as *const u8 as *const libc::c_char,
    b"norm(x)\0" as *const u8 as *const libc::c_char,
    b"rms_norm(x)\0" as *const u8 as *const libc::c_char,
    b"rms_norm_back(x)\0" as *const u8 as *const libc::c_char,
    b"X*Y\0" as *const u8 as *const libc::c_char,
    b"x*v\0" as *const u8 as *const libc::c_char,
    b"y-\\>view(x)\0" as *const u8 as *const libc::c_char,
    b"x-\\>y\0" as *const u8 as *const libc::c_char,
    b"cont(x)\0" as *const u8 as *const libc::c_char,
    b"reshape(x)\0" as *const u8 as *const libc::c_char,
    b"view(x)\0" as *const u8 as *const libc::c_char,
    b"permute(x)\0" as *const u8 as *const libc::c_char,
    b"transpose(x)\0" as *const u8 as *const libc::c_char,
    b"get_rows(x)\0" as *const u8 as *const libc::c_char,
    b"get_rows_back(x)\0" as *const u8 as *const libc::c_char,
    b"diag(x)\0" as *const u8 as *const libc::c_char,
    b"diag_mask_inf(x)\0" as *const u8 as *const libc::c_char,
    b"diag_mask_zero(x)\0" as *const u8 as *const libc::c_char,
    b"soft_max(x)\0" as *const u8 as *const libc::c_char,
    b"rope(x)\0" as *const u8 as *const libc::c_char,
    b"rope_back(x)\0" as *const u8 as *const libc::c_char,
    b"alibi(x)\0" as *const u8 as *const libc::c_char,
    b"clamp(x)\0" as *const u8 as *const libc::c_char,
    b"conv_1d_1s(x)\0" as *const u8 as *const libc::c_char,
    b"conv_1d_2s(x)\0" as *const u8 as *const libc::c_char,
    b"flash_attn(x)\0" as *const u8 as *const libc::c_char,
    b"flash_ff(x)\0" as *const u8 as *const libc::c_char,
    b"f(x)\0" as *const u8 as *const libc::c_char,
    b"f(x,y)\0" as *const u8 as *const libc::c_char,
];
static mut g_state: ggml_state = ggml_state {
    contexts: [ggml_context_container {
        used: false,
        context: ggml_context {
            mem_size: 0,
            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
            mem_buffer_owned: false,
            no_alloc: false,
            n_objects: 0,
            objects_begin: 0 as *const ggml_object as *mut ggml_object,
            objects_end: 0 as *const ggml_object as *mut ggml_object,
            scratch: ggml_scratch {
                offs: 0,
                size: 0,
                data: 0 as *const libc::c_void as *mut libc::c_void,
            },
            scratch_save: ggml_scratch {
                offs: 0,
                size: 0,
                data: 0 as *const libc::c_void as *mut libc::c_void,
            },
        },
    }; 64],
};
static mut g_state_barrier: libc::c_int = 0 as libc::c_int;
#[inline]
unsafe extern "C" fn ggml_critical_section_start() {
    let mut processing: libc::c_int = atomic_fetch_add(&mut g_state_barrier, 1 as libc::c_int);
    while processing > 0 as libc::c_int {
        atomic_fetch_sub(&mut g_state_barrier, 1 as libc::c_int);
        sched_yield();
        processing = atomic_fetch_add(&mut g_state_barrier, 1 as libc::c_int);
    }
}
#[inline]
unsafe extern "C" fn ggml_critical_section_end() {
    atomic_fetch_sub(&mut g_state_barrier, 1 as libc::c_int);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_print_object(mut obj: *const ggml_object) {
    printf(
        b" - ggml_object: offset = %zu, size = %zu, next = %p\n\0" as *const u8
            as *const libc::c_char,
        (*obj).offs,
        (*obj).size,
        (*obj).next as *const libc::c_void,
    );
}
#[no_mangle]
pub unsafe extern "C" fn ggml_print_objects(mut ctx: *const ggml_context) {
    let mut obj: *mut ggml_object = (*ctx).objects_begin;
    printf(
        b"%s: objects in context %p:\n\0" as *const u8 as *const libc::c_char,
        (*::core::mem::transmute::<&[u8; 19], &[libc::c_char; 19]>(b"ggml_print_objects\0"))
            .as_ptr(),
        ctx as *const libc::c_void,
    );
    while !obj.is_null() {
        ggml_print_object(obj);
        obj = (*obj).next;
    }
    printf(
        b"%s: --- end ---\n\0" as *const u8 as *const libc::c_char,
        (*::core::mem::transmute::<&[u8; 19], &[libc::c_char; 19]>(b"ggml_print_objects\0"))
            .as_ptr(),
    );
}
#[no_mangle]
pub unsafe extern "C" fn ggml_nelements(mut tensor: *const ggml_tensor) -> int64_t {
    return (*tensor).ne[0 as libc::c_int as usize]
        * (*tensor).ne[1 as libc::c_int as usize]
        * (*tensor).ne[2 as libc::c_int as usize]
        * (*tensor).ne[3 as libc::c_int as usize];
}
#[no_mangle]
pub unsafe extern "C" fn ggml_nrows(mut tensor: *const ggml_tensor) -> libc::c_int {
    return ((*tensor).ne[1 as libc::c_int as usize]
        * (*tensor).ne[2 as libc::c_int as usize]
        * (*tensor).ne[3 as libc::c_int as usize]) as libc::c_int;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_nbytes(mut tensor: *const ggml_tensor) -> size_t {
    return (ggml_nelements(tensor) as libc::c_ulong)
        .wrapping_mul(GGML_TYPE_SIZE[(*tensor).type_0 as usize])
        .wrapping_div(GGML_BLCK_SIZE[(*tensor).type_0 as usize] as libc::c_ulong);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_blck_size(mut type_0: ggml_type) -> libc::c_int {
    return GGML_BLCK_SIZE[type_0 as usize];
}
#[no_mangle]
pub unsafe extern "C" fn ggml_type_size(mut type_0: ggml_type) -> size_t {
    return GGML_TYPE_SIZE[type_0 as usize];
}
#[no_mangle]
pub unsafe extern "C" fn ggml_type_sizef(mut type_0: ggml_type) -> libc::c_float {
    return GGML_TYPE_SIZE[type_0 as usize] as libc::c_float
        / GGML_BLCK_SIZE[type_0 as usize] as libc::c_float;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_type_name(mut type_0: ggml_type) -> *const libc::c_char {
    return GGML_TYPE_NAME[type_0 as usize];
}
#[no_mangle]
pub unsafe extern "C" fn ggml_element_size(mut tensor: *const ggml_tensor) -> size_t {
    return GGML_TYPE_SIZE[(*tensor).type_0 as usize];
}
#[inline]
unsafe extern "C" fn ggml_is_scalar(mut tensor: *const ggml_tensor) -> bool {
    return (*tensor).ne[0 as libc::c_int as usize] == 1 as libc::c_int as libc::c_long
        && (*tensor).ne[1 as libc::c_int as usize] == 1 as libc::c_int as libc::c_long
        && (*tensor).ne[2 as libc::c_int as usize] == 1 as libc::c_int as libc::c_long
        && (*tensor).ne[3 as libc::c_int as usize] == 1 as libc::c_int as libc::c_long;
}
#[inline]
unsafe extern "C" fn ggml_is_vector(mut tensor: *const ggml_tensor) -> bool {
    return (*tensor).ne[1 as libc::c_int as usize] == 1 as libc::c_int as libc::c_long
        && (*tensor).ne[2 as libc::c_int as usize] == 1 as libc::c_int as libc::c_long
        && (*tensor).ne[3 as libc::c_int as usize] == 1 as libc::c_int as libc::c_long;
}
#[inline]
unsafe extern "C" fn ggml_is_matrix(mut tensor: *const ggml_tensor) -> bool {
    return (*tensor).ne[2 as libc::c_int as usize] == 1 as libc::c_int as libc::c_long
        && (*tensor).ne[3 as libc::c_int as usize] == 1 as libc::c_int as libc::c_long;
}
#[inline]
unsafe extern "C" fn ggml_can_mul_mat(
    mut t0: *const ggml_tensor,
    mut t1: *const ggml_tensor,
) -> bool {
    return (*t0).ne[0 as libc::c_int as usize] == (*t1).ne[0 as libc::c_int as usize]
        && (*t0).ne[2 as libc::c_int as usize] == (*t1).ne[2 as libc::c_int as usize]
        && (*t0).ne[3 as libc::c_int as usize] == (*t1).ne[3 as libc::c_int as usize];
}
#[no_mangle]
pub unsafe extern "C" fn ggml_is_quantized(mut type_0: ggml_type) -> bool {
    return GGML_IS_QUANTIZED[type_0 as usize];
}
#[no_mangle]
pub unsafe extern "C" fn ggml_ftype_to_ggml_type(mut ftype: ggml_ftype) -> ggml_type {
    let mut wtype: ggml_type = GGML_TYPE_COUNT;
    match ftype as libc::c_int {
        0 => {
            wtype = GGML_TYPE_F32;
        }
        1 => {
            wtype = GGML_TYPE_F16;
        }
        2 => {
            wtype = GGML_TYPE_Q4_0;
        }
        3 => {
            wtype = GGML_TYPE_Q4_1;
        }
        8 => {
            wtype = GGML_TYPE_Q5_0;
        }
        9 => {
            wtype = GGML_TYPE_Q5_1;
        }
        7 => {
            wtype = GGML_TYPE_Q8_0;
        }
        -1 => {
            wtype = GGML_TYPE_COUNT;
        }
        4 => {
            wtype = GGML_TYPE_COUNT;
        }
        _ => {}
    }
    if !(wtype as libc::c_uint != GGML_TYPE_COUNT as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            3759 as libc::c_int,
            b"wtype != GGML_TYPE_COUNT\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    return wtype;
}
#[inline]
unsafe extern "C" fn ggml_is_transposed(mut tensor: *const ggml_tensor) -> bool {
    return (*tensor).nb[0 as libc::c_int as usize] > (*tensor).nb[1 as libc::c_int as usize];
}
#[inline]
unsafe extern "C" fn ggml_is_contiguous(mut tensor: *const ggml_tensor) -> bool {
    return (*tensor).nb[0 as libc::c_int as usize] == GGML_TYPE_SIZE[(*tensor).type_0 as usize]
        && (*tensor).nb[1 as libc::c_int as usize]
            == ((*tensor).nb[0 as libc::c_int as usize])
                .wrapping_mul((*tensor).ne[0 as libc::c_int as usize] as libc::c_ulong)
                .wrapping_div(GGML_BLCK_SIZE[(*tensor).type_0 as usize] as libc::c_ulong)
        && (*tensor).nb[2 as libc::c_int as usize]
            == ((*tensor).nb[1 as libc::c_int as usize])
                .wrapping_mul((*tensor).ne[1 as libc::c_int as usize] as libc::c_ulong)
        && (*tensor).nb[3 as libc::c_int as usize]
            == ((*tensor).nb[2 as libc::c_int as usize])
                .wrapping_mul((*tensor).ne[2 as libc::c_int as usize] as libc::c_ulong);
}
#[inline]
unsafe extern "C" fn ggml_is_padded_1d(mut tensor: *const ggml_tensor) -> bool {
    return (*tensor).nb[0 as libc::c_int as usize] == GGML_TYPE_SIZE[(*tensor).type_0 as usize]
        && (*tensor).nb[2 as libc::c_int as usize]
            == ((*tensor).nb[1 as libc::c_int as usize])
                .wrapping_mul((*tensor).ne[1 as libc::c_int as usize] as libc::c_ulong)
        && (*tensor).nb[3 as libc::c_int as usize]
            == ((*tensor).nb[2 as libc::c_int as usize])
                .wrapping_mul((*tensor).ne[2 as libc::c_int as usize] as libc::c_ulong);
}
#[inline]
unsafe extern "C" fn ggml_are_same_shape(
    mut t0: *const ggml_tensor,
    mut t1: *const ggml_tensor,
) -> bool {
    return (*t0).ne[0 as libc::c_int as usize] == (*t1).ne[0 as libc::c_int as usize]
        && (*t0).ne[1 as libc::c_int as usize] == (*t1).ne[1 as libc::c_int as usize]
        && (*t0).ne[2 as libc::c_int as usize] == (*t1).ne[2 as libc::c_int as usize]
        && (*t0).ne[3 as libc::c_int as usize] == (*t1).ne[3 as libc::c_int as usize];
}
#[inline]
unsafe extern "C" fn ggml_can_repeat(
    mut t0: *const ggml_tensor,
    mut t1: *const ggml_tensor,
) -> bool {
    return (*t1).ne[0 as libc::c_int as usize] % (*t0).ne[0 as libc::c_int as usize]
        == 0 as libc::c_int as libc::c_long
        && (*t1).ne[1 as libc::c_int as usize] % (*t0).ne[1 as libc::c_int as usize]
            == 0 as libc::c_int as libc::c_long
        && (*t1).ne[2 as libc::c_int as usize] % (*t0).ne[2 as libc::c_int as usize]
            == 0 as libc::c_int as libc::c_long
        && (*t1).ne[3 as libc::c_int as usize] % (*t0).ne[3 as libc::c_int as usize]
            == 0 as libc::c_int as libc::c_long;
}
#[inline]
unsafe extern "C" fn ggml_can_repeat_rows(
    mut t0: *const ggml_tensor,
    mut t1: *const ggml_tensor,
) -> bool {
    return (*t0).ne[0 as libc::c_int as usize] == (*t1).ne[0 as libc::c_int as usize]
        && ggml_can_repeat(t0, t1) as libc::c_int != 0;
}
#[inline]
unsafe extern "C" fn ggml_up32(mut n: libc::c_int) -> libc::c_int {
    return n + 31 as libc::c_int & !(31 as libc::c_int);
}
#[inline]
unsafe extern "C" fn ggml_up(mut n: libc::c_int, mut m: libc::c_int) -> libc::c_int {
    if !(m & m - 1 as libc::c_int == 0 as libc::c_int) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            3824 as libc::c_int,
            b"(m & (m - 1)) == 0\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    return n + m - 1 as libc::c_int & !(m - 1 as libc::c_int);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_init(mut params: ggml_init_params) -> *mut ggml_context {
    ggml_critical_section_start();
    static mut is_first_call: bool = 1 as libc::c_int != 0;
    if is_first_call {
        ggml_time_init();
        let t_start: uint64_t = ggml_time_us() as uint64_t;
        let mut ii: ggml_fp16_t = 0;
        let mut i: libc::c_int = 0 as libc::c_int;
        while i < (1 as libc::c_int) << 16 as libc::c_int {
            let mut ui: uint16_t = i as uint16_t;
            memcpy(
                &mut ii as *mut ggml_fp16_t as *mut libc::c_void,
                &mut ui as *mut uint16_t as *const libc::c_void,
                ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong,
            );
            table_f32_f16[i as usize] = _cvtsh_ss(ii);
            let f: libc::c_float = table_f32_f16[i as usize];
            table_gelu_f16[i as usize] = ({
                ::core::mem::transmute::<
                    _,
                    [libc::c_short;
                        ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
                >(_mm_cvtps_ph(
                    _mm_setr_ps(
                        ggml_gelu_f32(f),
                        0 as libc::c_int as libc::c_float,
                        0 as libc::c_int as libc::c_float,
                        0 as libc::c_int as libc::c_float,
                    ),
                    0 as libc::c_int,
                ))[0 as libc::c_int as usize] as libc::c_ushort
            });
            table_silu_f16[i as usize] = ({
                ::core::mem::transmute::<
                    _,
                    [libc::c_short;
                        ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
                >(_mm_cvtps_ph(
                    _mm_setr_ps(
                        ggml_silu_f32(f),
                        0 as libc::c_int as libc::c_float,
                        0 as libc::c_int as libc::c_float,
                        0 as libc::c_int as libc::c_float,
                    ),
                    0 as libc::c_int,
                ))[0 as libc::c_int as usize] as libc::c_ushort
            });
            table_exp_f16[i as usize] = ({
                ::core::mem::transmute::<
                    _,
                    [libc::c_short;
                        ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
                >(_mm_cvtps_ph(
                    _mm_setr_ps(
                        expf(f),
                        0 as libc::c_int as libc::c_float,
                        0 as libc::c_int as libc::c_float,
                        0 as libc::c_int as libc::c_float,
                    ),
                    0 as libc::c_int,
                ))[0 as libc::c_int as usize] as libc::c_ushort
            });
            i += 1;
            i;
        }
        let t_end: uint64_t = ggml_time_us() as uint64_t;
        let t_start_0: uint64_t = ggml_time_us() as uint64_t;
        g_state = {
            let mut init = ggml_state {
                contexts: [
                    {
                        let mut init = ggml_context_container {
                            used: 0 as libc::c_int != 0,
                            context: ggml_context {
                                mem_size: 0,
                                mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                                mem_buffer_owned: false,
                                no_alloc: false,
                                n_objects: 0,
                                objects_begin: 0 as *const ggml_object as *mut ggml_object,
                                objects_end: 0 as *const ggml_object as *mut ggml_object,
                                scratch: ggml_scratch {
                                    offs: 0,
                                    size: 0,
                                    data: 0 as *const libc::c_void as *mut libc::c_void,
                                },
                                scratch_save: ggml_scratch {
                                    offs: 0,
                                    size: 0,
                                    data: 0 as *const libc::c_void as *mut libc::c_void,
                                },
                            },
                        };
                        init
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                    ggml_context_container {
                        used: false,
                        context: ggml_context {
                            mem_size: 0,
                            mem_buffer: 0 as *const libc::c_void as *mut libc::c_void,
                            mem_buffer_owned: false,
                            no_alloc: false,
                            n_objects: 0,
                            objects_begin: 0 as *const ggml_object as *mut ggml_object,
                            objects_end: 0 as *const ggml_object as *mut ggml_object,
                            scratch: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                            scratch_save: ggml_scratch {
                                offs: 0,
                                size: 0,
                                data: 0 as *const libc::c_void as *mut libc::c_void,
                            },
                        },
                    },
                ],
            };
            init
        };
        let mut i_0: libc::c_int = 0 as libc::c_int;
        while i_0 < 64 as libc::c_int {
            g_state.contexts[i_0 as usize].used = 0 as libc::c_int != 0;
            i_0 += 1;
            i_0;
        }
        let t_end_0: uint64_t = ggml_time_us() as uint64_t;
        is_first_call = 0 as libc::c_int != 0;
    }
    let mut ctx: *mut ggml_context = 0 as *mut ggml_context;
    let mut i_1: libc::c_int = 0 as libc::c_int;
    while i_1 < 64 as libc::c_int {
        if !g_state.contexts[i_1 as usize].used {
            g_state.contexts[i_1 as usize].used = 1 as libc::c_int != 0;
            ctx = &mut (*(g_state.contexts).as_mut_ptr().offset(i_1 as isize)).context;
            break;
        } else {
            i_1 += 1;
            i_1;
        }
    }
    if ctx.is_null() {
        ggml_critical_section_end();
        return 0 as *mut ggml_context;
    }
    let mem_size: size_t = (params.mem_size)
        .wrapping_add(16 as libc::c_int as libc::c_ulong)
        .wrapping_sub(1 as libc::c_int as libc::c_ulong)
        & !(16 as libc::c_int - 1 as libc::c_int) as libc::c_ulong;
    *ctx = {
        let mut init = ggml_context {
            mem_size: mem_size,
            mem_buffer: if !(params.mem_buffer).is_null() {
                params.mem_buffer
            } else {
                ggml_aligned_malloc(mem_size)
            },
            mem_buffer_owned: if !(params.mem_buffer).is_null() {
                0 as libc::c_int
            } else {
                1 as libc::c_int
            } != 0,
            no_alloc: params.no_alloc,
            n_objects: 0 as libc::c_int,
            objects_begin: 0 as *mut ggml_object,
            objects_end: 0 as *mut ggml_object,
            scratch: {
                let mut init = ggml_scratch {
                    offs: 0 as libc::c_int as size_t,
                    size: 0 as libc::c_int as size_t,
                    data: 0 as *mut libc::c_void,
                };
                init
            },
            scratch_save: {
                let mut init = ggml_scratch {
                    offs: 0 as libc::c_int as size_t,
                    size: 0 as libc::c_int as size_t,
                    data: 0 as *mut libc::c_void,
                };
                init
            },
        };
        init
    };
    if ((*ctx).mem_buffer).is_null() {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            3924 as libc::c_int,
            b"ctx->mem_buffer != NULL\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(((*ctx).mem_buffer as uintptr_t).wrapping_rem(16 as libc::c_int as libc::c_ulong)
        == 0 as libc::c_int as libc::c_ulong)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            3926 as libc::c_int,
            b"((uintptr_t) (ctx->mem_buffer))%GGML_MEM_ALIGN == 0\0" as *const u8
                as *const libc::c_char,
        );
        abort();
    }
    ggml_critical_section_end();
    return ctx;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_free(mut ctx: *mut ggml_context) {
    ggml_critical_section_start();
    let mut found: bool = 0 as libc::c_int != 0;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < 64 as libc::c_int {
        if &mut (*(g_state.contexts).as_mut_ptr().offset(i as isize)).context as *mut ggml_context
            == ctx
        {
            g_state.contexts[i as usize].used = 0 as libc::c_int != 0;
            if (*ctx).mem_buffer_owned {
                free((*ctx).mem_buffer);
            }
            found = 1 as libc::c_int != 0;
            break;
        } else {
            i += 1;
            i;
        }
    }
    !found;
    ggml_critical_section_end();
}
#[no_mangle]
pub unsafe extern "C" fn ggml_used_mem(mut ctx: *const ggml_context) -> size_t {
    return if ((*ctx).objects_end).is_null() {
        0 as libc::c_int as libc::c_ulong
    } else {
        ((*(*ctx).objects_end).offs).wrapping_add((*(*ctx).objects_end).size)
    };
}
#[no_mangle]
pub unsafe extern "C" fn ggml_set_scratch(
    mut ctx: *mut ggml_context,
    mut scratch: ggml_scratch,
) -> size_t {
    let result: size_t = if !((*ctx).scratch.data).is_null() {
        (*ctx).scratch.offs
    } else {
        0 as libc::c_int as libc::c_ulong
    };
    (*ctx).scratch = scratch;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_scratch_save(mut ctx: *mut ggml_context) {
    (*ctx).scratch_save = (*ctx).scratch;
    (*ctx).scratch.data = 0 as *mut libc::c_void;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_scratch_load(mut ctx: *mut ggml_context) {
    (*ctx).scratch = (*ctx).scratch_save;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_new_tensor_impl(
    mut ctx: *mut ggml_context,
    mut type_0: ggml_type,
    mut n_dims: libc::c_int,
    mut ne: *const int64_t,
    mut data: *mut libc::c_void,
) -> *mut ggml_tensor {
    let mut obj_cur: *mut ggml_object = (*ctx).objects_end;
    let cur_offs: size_t = if obj_cur.is_null() {
        0 as libc::c_int as libc::c_ulong
    } else {
        (*obj_cur).offs
    };
    let cur_size: size_t = if obj_cur.is_null() {
        0 as libc::c_int as libc::c_ulong
    } else {
        (*obj_cur).size
    };
    let cur_end: size_t = cur_offs.wrapping_add(cur_size);
    let mut size_needed: size_t = 0 as libc::c_int as size_t;
    if data.is_null() && !(*ctx).no_alloc {
        size_needed = (size_needed as libc::c_ulong).wrapping_add(
            (GGML_TYPE_SIZE[type_0 as usize]).wrapping_mul(
                (*ne.offset(0 as libc::c_int as isize)
                    / GGML_BLCK_SIZE[type_0 as usize] as libc::c_long)
                    as libc::c_ulong,
            ),
        ) as size_t as size_t;
        let mut i: libc::c_int = 1 as libc::c_int;
        while i < n_dims {
            size_needed = (size_needed as libc::c_ulong)
                .wrapping_mul(*ne.offset(i as isize) as libc::c_ulong)
                as size_t as size_t;
            i += 1;
            i;
        }
        size_needed = size_needed
            .wrapping_add(16 as libc::c_int as libc::c_ulong)
            .wrapping_sub(1 as libc::c_int as libc::c_ulong)
            .wrapping_div(16 as libc::c_int as libc::c_ulong)
            .wrapping_mul(16 as libc::c_int as libc::c_ulong);
    }
    let mem_buffer: *mut libc::c_char = (*ctx).mem_buffer as *mut libc::c_char;
    let obj_new: *mut ggml_object = mem_buffer.offset(cur_end as isize) as *mut ggml_object;
    if ((*ctx).scratch.data).is_null() || !data.is_null() {
        size_needed = (size_needed as libc::c_ulong)
            .wrapping_add(::core::mem::size_of::<ggml_tensor>() as libc::c_ulong)
            as size_t as size_t;
        if cur_end
            .wrapping_add(size_needed)
            .wrapping_add(GGML_OBJECT_SIZE)
            > (*ctx).mem_size
        {
            printf(
                b"%s: not enough space in the context's memory pool (needed %zu, available %zu)\n\0"
                    as *const u8 as *const libc::c_char,
                (*::core::mem::transmute::<&[u8; 21], &[libc::c_char; 21]>(
                    b"ggml_new_tensor_impl\0",
                ))
                .as_ptr(),
                cur_end
                    .wrapping_add(size_needed)
                    .wrapping_add(GGML_OBJECT_SIZE),
                (*ctx).mem_size,
            );
            return 0 as *mut ggml_tensor;
        }
        *obj_new = {
            let mut init = ggml_object {
                offs: cur_end.wrapping_add(GGML_OBJECT_SIZE),
                size: size_needed,
                next: 0 as *mut ggml_object,
                padding: [0; 8],
            };
            init
        };
    } else {
        if ((*ctx).scratch.offs).wrapping_add(size_needed) > (*ctx).scratch.size {
            printf(
                b"%s: not enough space in the scratch memory\n\0" as *const u8
                    as *const libc::c_char,
                (*::core::mem::transmute::<&[u8; 21], &[libc::c_char; 21]>(
                    b"ggml_new_tensor_impl\0",
                ))
                .as_ptr(),
            );
            return 0 as *mut ggml_tensor;
        }
        if cur_end
            .wrapping_add(::core::mem::size_of::<ggml_tensor>() as libc::c_ulong)
            .wrapping_add(GGML_OBJECT_SIZE)
            > (*ctx).mem_size
        {
            printf(
                b"%s: not enough space in the context's memory pool (needed %zu, available %zu)\n\0"
                    as *const u8 as *const libc::c_char,
                (*::core::mem::transmute::<&[u8; 21], &[libc::c_char; 21]>(
                    b"ggml_new_tensor_impl\0",
                ))
                .as_ptr(),
                cur_end
                    .wrapping_add(::core::mem::size_of::<ggml_tensor>() as libc::c_ulong)
                    .wrapping_add(GGML_OBJECT_SIZE),
                (*ctx).mem_size,
            );
            return 0 as *mut ggml_tensor;
        }
        data = ((*ctx).scratch.data as *mut libc::c_char).offset((*ctx).scratch.offs as isize)
            as *mut libc::c_void;
        *obj_new = {
            let mut init = ggml_object {
                offs: cur_end.wrapping_add(GGML_OBJECT_SIZE),
                size: ::core::mem::size_of::<ggml_tensor>() as libc::c_ulong,
                next: 0 as *mut ggml_object,
                padding: [0; 8],
            };
            init
        };
        (*ctx).scratch.offs =
            ((*ctx).scratch.offs as libc::c_ulong).wrapping_add(size_needed) as size_t as size_t;
    }
    if !obj_cur.is_null() {
        (*obj_cur).next = obj_new;
    } else {
        (*ctx).objects_begin = obj_new;
    }
    (*ctx).objects_end = obj_new;
    let result: *mut ggml_tensor = mem_buffer.offset((*obj_new).offs as isize) as *mut ggml_tensor;
    if !((result as uintptr_t).wrapping_rem(16 as libc::c_int as libc::c_ulong)
        == 0 as libc::c_int as libc::c_ulong)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            4074 as libc::c_int,
            b"((uintptr_t) (result))%GGML_MEM_ALIGN == 0\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    *result = {
        let mut init = ggml_tensor {
            type_0: type_0,
            backend: GGML_BACKEND_CPU,
            n_dims: n_dims,
            ne: [
                1 as libc::c_int as int64_t,
                1 as libc::c_int as int64_t,
                1 as libc::c_int as int64_t,
                1 as libc::c_int as int64_t,
            ],
            nb: [
                0 as libc::c_int as size_t,
                0 as libc::c_int as size_t,
                0 as libc::c_int as size_t,
                0 as libc::c_int as size_t,
            ],
            op: GGML_OP_NONE,
            is_param: 0 as libc::c_int != 0,
            grad: 0 as *mut ggml_tensor,
            src0: 0 as *mut ggml_tensor,
            src1: 0 as *mut ggml_tensor,
            opt: [
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
            ],
            n_tasks: 0 as libc::c_int,
            perf_runs: 0 as libc::c_int,
            perf_cycles: 0 as libc::c_int as int64_t,
            perf_time_us: 0 as libc::c_int as int64_t,
            data: if data.is_null() && !(*ctx).no_alloc {
                result.offset(1 as libc::c_int as isize) as *mut libc::c_void
            } else {
                data
            },
            name: [
                0 as libc::c_int as libc::c_char,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
            ],
            padding: [
                0 as libc::c_int as libc::c_char,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
            ],
        };
        init
    };
    let mut i_0: libc::c_int = 0 as libc::c_int;
    while i_0 < n_dims {
        (*result).ne[i_0 as usize] = *ne.offset(i_0 as isize);
        i_0 += 1;
        i_0;
    }
    (*result).nb[0 as libc::c_int as usize] = GGML_TYPE_SIZE[type_0 as usize];
    (*result).nb[1 as libc::c_int as usize] = ((*result).nb[0 as libc::c_int as usize])
        .wrapping_mul(
            ((*result).ne[0 as libc::c_int as usize]
                / GGML_BLCK_SIZE[type_0 as usize] as libc::c_long) as libc::c_ulong,
        );
    let mut i_1: libc::c_int = 2 as libc::c_int;
    while i_1 < 4 as libc::c_int {
        (*result).nb[i_1 as usize] = ((*result).nb[(i_1 - 1 as libc::c_int) as usize])
            .wrapping_mul((*result).ne[(i_1 - 1 as libc::c_int) as usize] as libc::c_ulong);
        i_1 += 1;
        i_1;
    }
    (*ctx).n_objects += 1;
    (*ctx).n_objects;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_new_tensor(
    mut ctx: *mut ggml_context,
    mut type_0: ggml_type,
    mut n_dims: libc::c_int,
    mut ne: *const int64_t,
) -> *mut ggml_tensor {
    return ggml_new_tensor_impl(ctx, type_0, n_dims, ne, 0 as *mut libc::c_void);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_new_tensor_1d(
    mut ctx: *mut ggml_context,
    mut type_0: ggml_type,
    mut ne0: int64_t,
) -> *mut ggml_tensor {
    return ggml_new_tensor(ctx, type_0, 1 as libc::c_int, &mut ne0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_new_tensor_2d(
    mut ctx: *mut ggml_context,
    mut type_0: ggml_type,
    mut ne0: int64_t,
    mut ne1: int64_t,
) -> *mut ggml_tensor {
    let ne: [int64_t; 2] = [ne0, ne1];
    return ggml_new_tensor(ctx, type_0, 2 as libc::c_int, ne.as_ptr());
}
#[no_mangle]
pub unsafe extern "C" fn ggml_new_tensor_3d(
    mut ctx: *mut ggml_context,
    mut type_0: ggml_type,
    mut ne0: int64_t,
    mut ne1: int64_t,
    mut ne2: int64_t,
) -> *mut ggml_tensor {
    let ne: [int64_t; 3] = [ne0, ne1, ne2];
    return ggml_new_tensor(ctx, type_0, 3 as libc::c_int, ne.as_ptr());
}
#[no_mangle]
pub unsafe extern "C" fn ggml_new_tensor_4d(
    mut ctx: *mut ggml_context,
    mut type_0: ggml_type,
    mut ne0: int64_t,
    mut ne1: int64_t,
    mut ne2: int64_t,
    mut ne3: int64_t,
) -> *mut ggml_tensor {
    let ne: [int64_t; 4] = [ne0, ne1, ne2, ne3];
    return ggml_new_tensor(ctx, type_0, 4 as libc::c_int, ne.as_ptr());
}
#[no_mangle]
pub unsafe extern "C" fn ggml_new_i32(
    mut ctx: *mut ggml_context,
    mut value: int32_t,
) -> *mut ggml_tensor {
    ggml_scratch_save(ctx);
    let mut result: *mut ggml_tensor =
        ggml_new_tensor_1d(ctx, GGML_TYPE_I32, 1 as libc::c_int as int64_t);
    ggml_scratch_load(ctx);
    ggml_set_i32(result, value);
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_new_f32(
    mut ctx: *mut ggml_context,
    mut value: libc::c_float,
) -> *mut ggml_tensor {
    ggml_scratch_save(ctx);
    let mut result: *mut ggml_tensor =
        ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1 as libc::c_int as int64_t);
    ggml_scratch_load(ctx);
    ggml_set_f32(result, value);
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_dup_tensor(
    mut ctx: *mut ggml_context,
    mut src: *const ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_new_tensor_impl(
        ctx,
        (*src).type_0,
        (*src).n_dims,
        ((*src).ne).as_ptr(),
        0 as *mut libc::c_void,
    );
}
#[no_mangle]
pub unsafe extern "C" fn ggml_set_zero(mut tensor: *mut ggml_tensor) -> *mut ggml_tensor {
    memset((*tensor).data, 0 as libc::c_int, ggml_nbytes(tensor));
    return tensor;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_set_i32(
    mut tensor: *mut ggml_tensor,
    mut value: int32_t,
) -> *mut ggml_tensor {
    let n: libc::c_int = ggml_nrows(tensor);
    let nc: libc::c_int = (*tensor).ne[0 as libc::c_int as usize] as libc::c_int;
    let n1: size_t = (*tensor).nb[1 as libc::c_int as usize];
    let data: *mut libc::c_char = (*tensor).data as *mut libc::c_char;
    match (*tensor).type_0 as libc::c_uint {
        10 => {
            let mut i: libc::c_int = 0 as libc::c_int;
            while i < n {
                ggml_vec_set_i8(
                    nc,
                    data.offset((i as libc::c_ulong).wrapping_mul(n1) as isize) as *mut int8_t,
                    value as int8_t,
                );
                i += 1;
                i;
            }
        }
        11 => {
            let mut i_0: libc::c_int = 0 as libc::c_int;
            while i_0 < n {
                ggml_vec_set_i16(
                    nc,
                    data.offset((i_0 as libc::c_ulong).wrapping_mul(n1) as isize) as *mut int16_t,
                    value as int16_t,
                );
                i_0 += 1;
                i_0;
            }
        }
        12 => {
            let mut i_1: libc::c_int = 0 as libc::c_int;
            while i_1 < n {
                ggml_vec_set_i32(
                    nc,
                    data.offset((i_1 as libc::c_ulong).wrapping_mul(n1) as isize) as *mut int32_t,
                    value,
                );
                i_1 += 1;
                i_1;
            }
        }
        1 => {
            let mut i_2: libc::c_int = 0 as libc::c_int;
            while i_2 < n {
                ggml_vec_set_f16(
                    nc,
                    data.offset((i_2 as libc::c_ulong).wrapping_mul(n1) as isize)
                        as *mut ggml_fp16_t,
                    value,
                );
                i_2 += 1;
                i_2;
            }
        }
        0 => {
            let mut i_3: libc::c_int = 0 as libc::c_int;
            while i_3 < n {
                ggml_vec_set_f32(
                    nc,
                    data.offset((i_3 as libc::c_ulong).wrapping_mul(n1) as isize)
                        as *mut libc::c_float,
                    value as libc::c_float,
                );
                i_3 += 1;
                i_3;
            }
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4238 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    }
    return tensor;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_set_f32(
    mut tensor: *mut ggml_tensor,
    mut value: libc::c_float,
) -> *mut ggml_tensor {
    let n: libc::c_int = ggml_nrows(tensor);
    let nc: libc::c_int = (*tensor).ne[0 as libc::c_int as usize] as libc::c_int;
    let n1: size_t = (*tensor).nb[1 as libc::c_int as usize];
    let data: *mut libc::c_char = (*tensor).data as *mut libc::c_char;
    match (*tensor).type_0 as libc::c_uint {
        10 => {
            let mut i: libc::c_int = 0 as libc::c_int;
            while i < n {
                ggml_vec_set_i8(
                    nc,
                    data.offset((i as libc::c_ulong).wrapping_mul(n1) as isize) as *mut int8_t,
                    value as int8_t,
                );
                i += 1;
                i;
            }
        }
        11 => {
            let mut i_0: libc::c_int = 0 as libc::c_int;
            while i_0 < n {
                ggml_vec_set_i16(
                    nc,
                    data.offset((i_0 as libc::c_ulong).wrapping_mul(n1) as isize) as *mut int16_t,
                    value as int16_t,
                );
                i_0 += 1;
                i_0;
            }
        }
        12 => {
            let mut i_1: libc::c_int = 0 as libc::c_int;
            while i_1 < n {
                ggml_vec_set_i32(
                    nc,
                    data.offset((i_1 as libc::c_ulong).wrapping_mul(n1) as isize) as *mut int32_t,
                    value as int32_t,
                );
                i_1 += 1;
                i_1;
            }
        }
        1 => {
            let mut i_2: libc::c_int = 0 as libc::c_int;
            while i_2 < n {
                ggml_vec_set_f16(
                    nc,
                    data.offset((i_2 as libc::c_ulong).wrapping_mul(n1) as isize)
                        as *mut ggml_fp16_t,
                    value as int32_t,
                );
                i_2 += 1;
                i_2;
            }
        }
        0 => {
            let mut i_3: libc::c_int = 0 as libc::c_int;
            while i_3 < n {
                ggml_vec_set_f32(
                    nc,
                    data.offset((i_3 as libc::c_ulong).wrapping_mul(n1) as isize)
                        as *mut libc::c_float,
                    value,
                );
                i_3 += 1;
                i_3;
            }
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4290 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    }
    return tensor;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_get_i32_1d(
    mut tensor: *const ggml_tensor,
    mut i: libc::c_int,
) -> int32_t {
    match (*tensor).type_0 as libc::c_uint {
        10 => {
            if !((*tensor).nb[0 as libc::c_int as usize]
                == ::core::mem::size_of::<int8_t>() as libc::c_ulong)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4301 as libc::c_int,
                    b"tensor->nb[0] == sizeof(int8_t)\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            return *((*tensor).data as *mut int8_t).offset(i as isize) as int32_t;
        }
        11 => {
            if !((*tensor).nb[0 as libc::c_int as usize]
                == ::core::mem::size_of::<int16_t>() as libc::c_ulong)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4306 as libc::c_int,
                    b"tensor->nb[0] == sizeof(int16_t)\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            return *((*tensor).data as *mut int16_t).offset(i as isize) as int32_t;
        }
        12 => {
            if !((*tensor).nb[0 as libc::c_int as usize]
                == ::core::mem::size_of::<int32_t>() as libc::c_ulong)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4311 as libc::c_int,
                    b"tensor->nb[0] == sizeof(int32_t)\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            return *((*tensor).data as *mut int32_t).offset(i as isize);
        }
        1 => {
            if !((*tensor).nb[0 as libc::c_int as usize]
                == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4316 as libc::c_int,
                    b"tensor->nb[0] == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            return ggml_lookup_fp16_to_fp32(
                *((*tensor).data as *mut ggml_fp16_t).offset(i as isize),
            ) as int32_t;
        }
        0 => {
            if !((*tensor).nb[0 as libc::c_int as usize]
                == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4321 as libc::c_int,
                    b"tensor->nb[0] == sizeof(float)\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            return *((*tensor).data as *mut libc::c_float).offset(i as isize) as int32_t;
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4326 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    }
    return 0.0f32 as int32_t;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_set_i32_1d(
    mut tensor: *const ggml_tensor,
    mut i: libc::c_int,
    mut value: int32_t,
) {
    match (*tensor).type_0 as libc::c_uint {
        10 => {
            if !((*tensor).nb[0 as libc::c_int as usize]
                == ::core::mem::size_of::<int8_t>() as libc::c_ulong)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4337 as libc::c_int,
                    b"tensor->nb[0] == sizeof(int8_t)\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            *((*tensor).data as *mut int8_t).offset(i as isize) = value as int8_t;
        }
        11 => {
            if !((*tensor).nb[0 as libc::c_int as usize]
                == ::core::mem::size_of::<int16_t>() as libc::c_ulong)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4342 as libc::c_int,
                    b"tensor->nb[0] == sizeof(int16_t)\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            *((*tensor).data as *mut int16_t).offset(i as isize) = value as int16_t;
        }
        12 => {
            if !((*tensor).nb[0 as libc::c_int as usize]
                == ::core::mem::size_of::<int32_t>() as libc::c_ulong)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4347 as libc::c_int,
                    b"tensor->nb[0] == sizeof(int32_t)\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            *((*tensor).data as *mut int32_t).offset(i as isize) = value;
        }
        1 => {
            if !((*tensor).nb[0 as libc::c_int as usize]
                == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4352 as libc::c_int,
                    b"tensor->nb[0] == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            *((*tensor).data as *mut ggml_fp16_t).offset(i as isize) = ({
                ::core::mem::transmute::<
                    _,
                    [libc::c_short;
                        ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
                >(_mm_cvtps_ph(
                    _mm_setr_ps(
                        value as libc::c_float,
                        0 as libc::c_int as libc::c_float,
                        0 as libc::c_int as libc::c_float,
                        0 as libc::c_int as libc::c_float,
                    ),
                    0 as libc::c_int,
                ))[0 as libc::c_int as usize] as libc::c_ushort
            });
        }
        0 => {
            if !((*tensor).nb[0 as libc::c_int as usize]
                == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4357 as libc::c_int,
                    b"tensor->nb[0] == sizeof(float)\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            *((*tensor).data as *mut libc::c_float).offset(i as isize) = value as libc::c_float;
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4362 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
#[no_mangle]
pub unsafe extern "C" fn ggml_get_f32_1d(
    mut tensor: *const ggml_tensor,
    mut i: libc::c_int,
) -> libc::c_float {
    match (*tensor).type_0 as libc::c_uint {
        10 => {
            if !((*tensor).nb[0 as libc::c_int as usize]
                == ::core::mem::size_of::<int8_t>() as libc::c_ulong)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4371 as libc::c_int,
                    b"tensor->nb[0] == sizeof(int8_t)\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            return *((*tensor).data as *mut int8_t).offset(i as isize) as libc::c_float;
        }
        11 => {
            if !((*tensor).nb[0 as libc::c_int as usize]
                == ::core::mem::size_of::<int16_t>() as libc::c_ulong)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4376 as libc::c_int,
                    b"tensor->nb[0] == sizeof(int16_t)\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            return *((*tensor).data as *mut int16_t).offset(i as isize) as libc::c_float;
        }
        12 => {
            if !((*tensor).nb[0 as libc::c_int as usize]
                == ::core::mem::size_of::<int32_t>() as libc::c_ulong)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4381 as libc::c_int,
                    b"tensor->nb[0] == sizeof(int32_t)\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            return *((*tensor).data as *mut int32_t).offset(i as isize) as libc::c_float;
        }
        1 => {
            if !((*tensor).nb[0 as libc::c_int as usize]
                == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4386 as libc::c_int,
                    b"tensor->nb[0] == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            return ggml_lookup_fp16_to_fp32(
                *((*tensor).data as *mut ggml_fp16_t).offset(i as isize),
            );
        }
        0 => {
            if !((*tensor).nb[0 as libc::c_int as usize]
                == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4391 as libc::c_int,
                    b"tensor->nb[0] == sizeof(float)\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            return *((*tensor).data as *mut libc::c_float).offset(i as isize);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4396 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    }
    return 0.0f32;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_set_f32_1d(
    mut tensor: *const ggml_tensor,
    mut i: libc::c_int,
    mut value: libc::c_float,
) {
    match (*tensor).type_0 as libc::c_uint {
        10 => {
            if !((*tensor).nb[0 as libc::c_int as usize]
                == ::core::mem::size_of::<int8_t>() as libc::c_ulong)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4407 as libc::c_int,
                    b"tensor->nb[0] == sizeof(int8_t)\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            *((*tensor).data as *mut int8_t).offset(i as isize) = value as int8_t;
        }
        11 => {
            if !((*tensor).nb[0 as libc::c_int as usize]
                == ::core::mem::size_of::<int16_t>() as libc::c_ulong)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4412 as libc::c_int,
                    b"tensor->nb[0] == sizeof(int16_t)\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            *((*tensor).data as *mut int16_t).offset(i as isize) = value as int16_t;
        }
        12 => {
            if !((*tensor).nb[0 as libc::c_int as usize]
                == ::core::mem::size_of::<int32_t>() as libc::c_ulong)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4417 as libc::c_int,
                    b"tensor->nb[0] == sizeof(int32_t)\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            *((*tensor).data as *mut int32_t).offset(i as isize) = value as int32_t;
        }
        1 => {
            if !((*tensor).nb[0 as libc::c_int as usize]
                == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4422 as libc::c_int,
                    b"tensor->nb[0] == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            *((*tensor).data as *mut ggml_fp16_t).offset(i as isize) = ({
                ::core::mem::transmute::<
                    _,
                    [libc::c_short;
                        ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
                >(_mm_cvtps_ph(
                    _mm_setr_ps(
                        value,
                        0 as libc::c_int as libc::c_float,
                        0 as libc::c_int as libc::c_float,
                        0 as libc::c_int as libc::c_float,
                    ),
                    0 as libc::c_int,
                ))[0 as libc::c_int as usize] as libc::c_ushort
            });
        }
        0 => {
            if !((*tensor).nb[0 as libc::c_int as usize]
                == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4427 as libc::c_int,
                    b"tensor->nb[0] == sizeof(float)\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            *((*tensor).data as *mut libc::c_float).offset(i as isize) = value;
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    4432 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
#[no_mangle]
pub unsafe extern "C" fn ggml_get_data(mut tensor: *const ggml_tensor) -> *mut libc::c_void {
    return (*tensor).data;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_get_data_f32(mut tensor: *const ggml_tensor) -> *mut libc::c_float {
    return (*tensor).data as *mut libc::c_float;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_get_name(mut tensor: *const ggml_tensor) -> *const libc::c_char {
    return ((*tensor).name).as_ptr();
}
#[no_mangle]
pub unsafe extern "C" fn ggml_set_name(
    mut tensor: *mut ggml_tensor,
    mut name: *const libc::c_char,
) {
    strncpy(
        ((*tensor).name).as_mut_ptr(),
        name,
        ::core::mem::size_of::<[libc::c_char; 32]>() as libc::c_ulong,
    );
    (*tensor).name[(::core::mem::size_of::<[libc::c_char; 32]>() as libc::c_ulong)
        .wrapping_sub(1 as libc::c_int as libc::c_ulong) as usize] = '\0' as i32 as libc::c_char;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_view_tensor(
    mut ctx: *mut ggml_context,
    mut src: *const ggml_tensor,
) -> *mut ggml_tensor {
    let mut result: *mut ggml_tensor = ggml_new_tensor_impl(
        ctx,
        (*src).type_0,
        (*src).n_dims,
        ((*src).ne).as_ptr(),
        (*src).data,
    );
    (*result).nb[0 as libc::c_int as usize] = (*src).nb[0 as libc::c_int as usize];
    (*result).nb[1 as libc::c_int as usize] = (*src).nb[1 as libc::c_int as usize];
    (*result).nb[2 as libc::c_int as usize] = (*src).nb[2 as libc::c_int as usize];
    (*result).nb[3 as libc::c_int as usize] = (*src).nb[3 as libc::c_int as usize];
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_dup_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_DUP;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_dup(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_dup_impl(ctx, a, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_dup_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_dup_impl(ctx, a, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_add_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    if !ggml_are_same_shape(a, b) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            4511 as libc::c_int,
            b"ggml_are_same_shape(a, b)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && (!((*a).grad).is_null() || !((*b).grad).is_null()) {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_ADD;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_add(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_add_impl(ctx, a, b, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_add_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_add_impl(ctx, a, b, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_add1_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    if !ggml_is_scalar(b) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            4550 as libc::c_int,
            b"ggml_is_scalar(b)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_is_padded_1d(a) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            4551 as libc::c_int,
            b"ggml_is_padded_1d(a)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && (!((*a).grad).is_null() || !((*b).grad).is_null()) {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_ADD1;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_add1(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_add1_impl(ctx, a, b, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_add1_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_add1_impl(ctx, a, b, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_acc_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
    mut nb1: size_t,
    mut nb2: size_t,
    mut nb3: size_t,
    mut offset: size_t,
    mut inplace: bool,
) -> *mut ggml_tensor {
    if !(ggml_nelements(b) <= ggml_nelements(a)) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            4594 as libc::c_int,
            b"ggml_nelements(b) <= ggml_nelements(a)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_is_contiguous(a) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            4595 as libc::c_int,
            b"ggml_is_contiguous(a)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*a).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            4596 as libc::c_int,
            b"a->type == GGML_TYPE_F32\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*b).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            4597 as libc::c_int,
            b"b->type == GGML_TYPE_F32\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && (!((*a).grad).is_null() || !((*b).grad).is_null()) {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    ggml_scratch_save(ctx);
    let mut c: *mut ggml_tensor =
        ggml_new_tensor_1d(ctx, GGML_TYPE_I32, 5 as libc::c_int as int64_t);
    *((*c).data as *mut int32_t).offset(0 as libc::c_int as isize) = nb1 as int32_t;
    *((*c).data as *mut int32_t).offset(1 as libc::c_int as isize) = nb2 as int32_t;
    *((*c).data as *mut int32_t).offset(2 as libc::c_int as isize) = nb3 as int32_t;
    *((*c).data as *mut int32_t).offset(3 as libc::c_int as isize) = offset as int32_t;
    *((*c).data as *mut int32_t).offset(4 as libc::c_int as isize) = if inplace as libc::c_int != 0
    {
        1 as libc::c_int
    } else {
        0 as libc::c_int
    };
    ggml_scratch_load(ctx);
    (*result).op = GGML_OP_ACC;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    (*result).opt[0 as libc::c_int as usize] = c;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_acc(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
    mut nb1: size_t,
    mut nb2: size_t,
    mut nb3: size_t,
    mut offset: size_t,
) -> *mut ggml_tensor {
    return ggml_acc_impl(ctx, a, b, nb1, nb2, nb3, offset, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_acc_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
    mut nb1: size_t,
    mut nb2: size_t,
    mut nb3: size_t,
    mut offset: size_t,
) -> *mut ggml_tensor {
    return ggml_acc_impl(ctx, a, b, nb1, nb2, nb3, offset, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_sub_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    if !ggml_are_same_shape(a, b) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            4657 as libc::c_int,
            b"ggml_are_same_shape(a, b)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && (!((*a).grad).is_null() || !((*b).grad).is_null()) {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_SUB;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_sub(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_sub_impl(ctx, a, b, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_sub_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_sub_impl(ctx, a, b, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_mul_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    if !ggml_can_repeat_rows(b, a) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            4698 as libc::c_int,
            b"ggml_can_repeat_rows(b, a)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && (!((*a).grad).is_null() || !((*b).grad).is_null()) {
        if !ggml_are_same_shape(a, b) {
            fprintf(
                stderr,
                b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                b"ggml.c\0" as *const u8 as *const libc::c_char,
                4704 as libc::c_int,
                b"ggml_are_same_shape(a, b)\0" as *const u8 as *const libc::c_char,
            );
            abort();
        }
        is_node = 1 as libc::c_int != 0;
    }
    if inplace {
        if !(is_node as libc::c_int == 0 as libc::c_int) {
            fprintf(
                stderr,
                b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                b"ggml.c\0" as *const u8 as *const libc::c_char,
                4709 as libc::c_int,
                b"is_node == false\0" as *const u8 as *const libc::c_char,
            );
            abort();
        }
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_MUL;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_mul(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_mul_impl(ctx, a, b, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_mul_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_mul_impl(ctx, a, b, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_div_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    if !ggml_are_same_shape(a, b) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            4743 as libc::c_int,
            b"ggml_are_same_shape(a, b)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && (!((*a).grad).is_null() || !((*b).grad).is_null()) {
        is_node = 1 as libc::c_int != 0;
    }
    if inplace {
        if !(is_node as libc::c_int == 0 as libc::c_int) {
            fprintf(
                stderr,
                b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                b"ggml.c\0" as *const u8 as *const libc::c_char,
                4752 as libc::c_int,
                b"is_node == false\0" as *const u8 as *const libc::c_char,
            );
            abort();
        }
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_DIV;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_div(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_div_impl(ctx, a, b, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_div_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_div_impl(ctx, a, b, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_sqr_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_SQR;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_sqr(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_sqr_impl(ctx, a, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_sqr_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_sqr_impl(ctx, a, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_sqrt_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_SQRT;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_sqrt(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_sqrt_impl(ctx, a, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_sqrt_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_sqrt_impl(ctx, a, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_log_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_LOG;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_log(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_log_impl(ctx, a, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_log_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_log_impl(ctx, a, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_sum(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor =
        ggml_new_tensor_1d(ctx, (*a).type_0, 1 as libc::c_int as int64_t);
    (*result).op = GGML_OP_SUM;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_sum_rows(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut ne: [int64_t; 4] = [
        1 as libc::c_int as int64_t,
        1 as libc::c_int as int64_t,
        1 as libc::c_int as int64_t,
        1 as libc::c_int as int64_t,
    ];
    let mut i: libc::c_int = 1 as libc::c_int;
    while i < (*a).n_dims {
        ne[i as usize] = (*a).ne[i as usize];
        i += 1;
        i;
    }
    let mut result: *mut ggml_tensor =
        ggml_new_tensor(ctx, (*a).type_0, (*a).n_dims, ne.as_mut_ptr());
    (*result).op = GGML_OP_SUM_ROWS;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_mean(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        if 0 as libc::c_int == 0 {
            fprintf(
                stderr,
                b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                b"ggml.c\0" as *const u8 as *const libc::c_char,
                4938 as libc::c_int,
                b"false\0" as *const u8 as *const libc::c_char,
            );
            abort();
        }
        is_node = 1 as libc::c_int != 0;
    }
    let mut ne: [int64_t; 4] = [
        1 as libc::c_int as int64_t,
        (*a).ne[1 as libc::c_int as usize],
        (*a).ne[2 as libc::c_int as usize],
        (*a).ne[3 as libc::c_int as usize],
    ];
    let mut result: *mut ggml_tensor =
        ggml_new_tensor(ctx, GGML_TYPE_F32, (*a).n_dims, ne.as_mut_ptr());
    (*result).op = GGML_OP_MEAN;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_repeat(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    if !ggml_can_repeat(a, b) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            4959 as libc::c_int,
            b"ggml_can_repeat(a, b)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    if ggml_are_same_shape(a, b) as libc::c_int != 0 && !is_node {
        return a;
    }
    let mut result: *mut ggml_tensor =
        ggml_new_tensor(ctx, (*a).type_0, (*b).n_dims, ((*b).ne).as_mut_ptr());
    (*result).op = GGML_OP_REPEAT;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_abs_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_ABS;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_abs(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_abs_impl(ctx, a, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_abs_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_abs_impl(ctx, a, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_sgn_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_SGN;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_sgn(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_sgn_impl(ctx, a, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_sgn_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_sgn_impl(ctx, a, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_neg_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_NEG;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_neg(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_neg_impl(ctx, a, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_neg_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_neg_impl(ctx, a, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_step_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_STEP;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_step(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_step_impl(ctx, a, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_step_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_step_impl(ctx, a, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_relu_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_RELU;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_relu(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_relu_impl(ctx, a, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_relu_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_relu_impl(ctx, a, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_gelu_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_GELU;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_gelu(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_gelu_impl(ctx, a, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_gelu_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_gelu_impl(ctx, a, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_silu_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_SILU;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_silu(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_silu_impl(ctx, a, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_silu_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_silu_impl(ctx, a, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_silu_back(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() || !((*b).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = ggml_dup_tensor(ctx, a);
    (*result).op = GGML_OP_SILU_BACK;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_norm_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && !((*a).grad).is_null() {
        if 0 as libc::c_int == 0 {
            fprintf(
                stderr,
                b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                b"ggml.c\0" as *const u8 as *const libc::c_char,
                5252 as libc::c_int,
                b"false\0" as *const u8 as *const libc::c_char,
            );
            abort();
        }
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_NORM;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_norm(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_norm_impl(ctx, a, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_norm_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_norm_impl(ctx, a, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_rms_norm_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_RMS_NORM;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_rms_norm(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_rms_norm_impl(ctx, a, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_rms_norm_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_rms_norm_impl(ctx, a, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_rms_norm_back(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = ggml_dup_tensor(ctx, a);
    (*result).op = GGML_OP_RMS_NORM_BACK;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_mul_mat(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    if !ggml_can_mul_mat(a, b) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5338 as libc::c_int,
            b"ggml_can_mul_mat(a, b)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if ggml_is_transposed(a) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5339 as libc::c_int,
            b"!ggml_is_transposed(a)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() || !((*b).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let ne: [int64_t; 4] = [
        (*a).ne[1 as libc::c_int as usize],
        (*b).ne[1 as libc::c_int as usize],
        (*a).ne[2 as libc::c_int as usize],
        (*b).ne[3 as libc::c_int as usize],
    ];
    let mut result: *mut ggml_tensor = ggml_new_tensor(
        ctx,
        GGML_TYPE_F32,
        if (*a).n_dims < (*b).n_dims {
            (*a).n_dims
        } else {
            (*b).n_dims
        },
        ne.as_ptr(),
    );
    (*result).op = GGML_OP_MUL_MAT;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_scale_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    if !ggml_is_scalar(b) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5365 as libc::c_int,
            b"ggml_is_scalar(b)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_is_padded_1d(a) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5366 as libc::c_int,
            b"ggml_is_padded_1d(a)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && (!((*a).grad).is_null() || !((*b).grad).is_null()) {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_SCALE;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_scale(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_scale_impl(ctx, a, b, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_scale_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_scale_impl(ctx, a, b, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_set_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
    mut nb1: size_t,
    mut nb2: size_t,
    mut nb3: size_t,
    mut offset: size_t,
    mut inplace: bool,
) -> *mut ggml_tensor {
    if !(ggml_nelements(a) >= ggml_nelements(b)) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5409 as libc::c_int,
            b"ggml_nelements(a) >= ggml_nelements(b)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && (!((*a).grad).is_null() || !((*b).grad).is_null()) {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    ggml_scratch_save(ctx);
    let mut c: *mut ggml_tensor =
        ggml_new_tensor_1d(ctx, GGML_TYPE_I32, 5 as libc::c_int as int64_t);
    *((*c).data as *mut int32_t).offset(0 as libc::c_int as isize) = nb1 as int32_t;
    *((*c).data as *mut int32_t).offset(1 as libc::c_int as isize) = nb2 as int32_t;
    *((*c).data as *mut int32_t).offset(2 as libc::c_int as isize) = nb3 as int32_t;
    *((*c).data as *mut int32_t).offset(3 as libc::c_int as isize) = offset as int32_t;
    *((*c).data as *mut int32_t).offset(4 as libc::c_int as isize) = if inplace as libc::c_int != 0
    {
        1 as libc::c_int
    } else {
        0 as libc::c_int
    };
    ggml_scratch_load(ctx);
    (*result).op = GGML_OP_SET;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    (*result).opt[0 as libc::c_int as usize] = c;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_set(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
    mut nb1: size_t,
    mut nb2: size_t,
    mut nb3: size_t,
    mut offset: size_t,
) -> *mut ggml_tensor {
    return ggml_set_impl(ctx, a, b, nb1, nb2, nb3, offset, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_set_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
    mut nb1: size_t,
    mut nb2: size_t,
    mut nb3: size_t,
    mut offset: size_t,
) -> *mut ggml_tensor {
    return ggml_set_impl(ctx, a, b, nb1, nb2, nb3, offset, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_set_1d(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
    mut offset: size_t,
) -> *mut ggml_tensor {
    return ggml_set_impl(
        ctx,
        a,
        b,
        (*a).nb[1 as libc::c_int as usize],
        (*a).nb[2 as libc::c_int as usize],
        (*a).nb[3 as libc::c_int as usize],
        offset,
        0 as libc::c_int != 0,
    );
}
#[no_mangle]
pub unsafe extern "C" fn ggml_set_1d_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
    mut offset: size_t,
) -> *mut ggml_tensor {
    return ggml_set_impl(
        ctx,
        a,
        b,
        (*a).nb[1 as libc::c_int as usize],
        (*a).nb[2 as libc::c_int as usize],
        (*a).nb[3 as libc::c_int as usize],
        offset,
        1 as libc::c_int != 0,
    );
}
#[no_mangle]
pub unsafe extern "C" fn ggml_set_2d(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
    mut nb1: size_t,
    mut offset: size_t,
) -> *mut ggml_tensor {
    return ggml_set_impl(
        ctx,
        a,
        b,
        nb1,
        (*a).nb[2 as libc::c_int as usize],
        (*a).nb[3 as libc::c_int as usize],
        offset,
        0 as libc::c_int != 0,
    );
}
#[no_mangle]
pub unsafe extern "C" fn ggml_set_2d_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
    mut nb1: size_t,
    mut offset: size_t,
) -> *mut ggml_tensor {
    return ggml_set_impl(
        ctx,
        a,
        b,
        nb1,
        (*a).nb[2 as libc::c_int as usize],
        (*a).nb[3 as libc::c_int as usize],
        offset,
        0 as libc::c_int != 0,
    );
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cpy_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    if !(ggml_nelements(a) == ggml_nelements(b)) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5505 as libc::c_int,
            b"ggml_nelements(a) == ggml_nelements(b)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && (!((*a).grad).is_null() || !((*b).grad).is_null()) {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = ggml_view_tensor(ctx, b);
    (*result).op = GGML_OP_CPY;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cpy(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_cpy_impl(ctx, a, b, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cpy_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_cpy_impl(ctx, a, b, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cont_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_CONT;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cont(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_cont_impl(ctx, a, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cont_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_cont_impl(ctx, a, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_reshape(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    if !ggml_is_contiguous(a) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5578 as libc::c_int,
            b"ggml_is_contiguous(a)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_is_contiguous(b) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5579 as libc::c_int,
            b"ggml_is_contiguous(b)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ggml_nelements(a) == ggml_nelements(b)) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5580 as libc::c_int,
            b"ggml_nelements(a) == ggml_nelements(b)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    !((*b).grad).is_null();
    let mut result: *mut ggml_tensor = ggml_new_tensor_impl(
        ctx,
        (*a).type_0,
        (*b).n_dims,
        ((*b).ne).as_mut_ptr(),
        (*a).data,
    );
    (*result).op = GGML_OP_RESHAPE;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_reshape_1d(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut ne0: int64_t,
) -> *mut ggml_tensor {
    if !ggml_is_contiguous(a) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5607 as libc::c_int,
            b"ggml_is_contiguous(a)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ggml_nelements(a) == ne0) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5608 as libc::c_int,
            b"ggml_nelements(a) == ne0\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let ne: [int64_t; 1] = [ne0];
    let mut result: *mut ggml_tensor =
        ggml_new_tensor_impl(ctx, (*a).type_0, 1 as libc::c_int, ne.as_ptr(), (*a).data);
    (*result).op = GGML_OP_RESHAPE;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_reshape_2d(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut ne0: int64_t,
    mut ne1: int64_t,
) -> *mut ggml_tensor {
    if !ggml_is_contiguous(a) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5632 as libc::c_int,
            b"ggml_is_contiguous(a)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ggml_nelements(a) == ne0 * ne1) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5633 as libc::c_int,
            b"ggml_nelements(a) == ne0*ne1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let ne: [int64_t; 2] = [ne0, ne1];
    let mut result: *mut ggml_tensor =
        ggml_new_tensor_impl(ctx, (*a).type_0, 2 as libc::c_int, ne.as_ptr(), (*a).data);
    (*result).op = GGML_OP_RESHAPE;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_reshape_3d(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut ne0: int64_t,
    mut ne1: int64_t,
    mut ne2: int64_t,
) -> *mut ggml_tensor {
    if !ggml_is_contiguous(a) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5658 as libc::c_int,
            b"ggml_is_contiguous(a)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ggml_nelements(a) == ne0 * ne1 * ne2) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5659 as libc::c_int,
            b"ggml_nelements(a) == ne0*ne1*ne2\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let ne: [int64_t; 3] = [ne0, ne1, ne2];
    let mut result: *mut ggml_tensor =
        ggml_new_tensor_impl(ctx, (*a).type_0, 3 as libc::c_int, ne.as_ptr(), (*a).data);
    (*result).op = GGML_OP_RESHAPE;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_reshape_4d(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut ne0: int64_t,
    mut ne1: int64_t,
    mut ne2: int64_t,
    mut ne3: int64_t,
) -> *mut ggml_tensor {
    if !ggml_is_contiguous(a) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5686 as libc::c_int,
            b"ggml_is_contiguous(a)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ggml_nelements(a) == ne0 * ne1 * ne2 * ne3) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5687 as libc::c_int,
            b"ggml_nelements(a) == ne0*ne1*ne2*ne3\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let ne: [int64_t; 4] = [ne0, ne1, ne2, ne3];
    let mut result: *mut ggml_tensor =
        ggml_new_tensor_impl(ctx, (*a).type_0, 4 as libc::c_int, ne.as_ptr(), (*a).data);
    (*result).op = GGML_OP_RESHAPE;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_view_1d(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut ne0: int64_t,
    mut offset: size_t,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = ggml_new_tensor_impl(
        ctx,
        (*a).type_0,
        1 as libc::c_int,
        &mut ne0,
        ((*a).data as *mut libc::c_char).offset(offset as isize) as *mut libc::c_void,
    );
    (*result).op = GGML_OP_VIEW;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    if is_node {
        memcpy(
            ((*result).padding).as_mut_ptr() as *mut libc::c_void,
            &mut offset as *mut size_t as *const libc::c_void,
            ::core::mem::size_of::<size_t>() as libc::c_ulong,
        );
    }
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_view_2d(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut ne0: int64_t,
    mut ne1: int64_t,
    mut nb1: size_t,
    mut offset: size_t,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let ne: [int64_t; 4] = [
        ne0,
        ne1,
        1 as libc::c_int as int64_t,
        1 as libc::c_int as int64_t,
    ];
    let mut result: *mut ggml_tensor = ggml_new_tensor_impl(
        ctx,
        (*a).type_0,
        2 as libc::c_int,
        ne.as_ptr(),
        ((*a).data as *mut libc::c_char).offset(offset as isize) as *mut libc::c_void,
    );
    (*result).nb[1 as libc::c_int as usize] = nb1;
    (*result).nb[2 as libc::c_int as usize] =
        ((*result).nb[1 as libc::c_int as usize]).wrapping_mul(ne1 as libc::c_ulong);
    (*result).nb[3 as libc::c_int as usize] = (*result).nb[2 as libc::c_int as usize];
    (*result).op = GGML_OP_VIEW;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    if is_node {
        memcpy(
            ((*result).padding).as_mut_ptr() as *mut libc::c_void,
            &mut offset as *mut size_t as *const libc::c_void,
            ::core::mem::size_of::<size_t>() as libc::c_ulong,
        );
    }
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_view_3d(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut ne0: int64_t,
    mut ne1: int64_t,
    mut ne2: int64_t,
    mut nb1: size_t,
    mut nb2: size_t,
    mut offset: size_t,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let ne: [int64_t; 4] = [ne0, ne1, ne2, 1 as libc::c_int as int64_t];
    let mut result: *mut ggml_tensor = ggml_new_tensor_impl(
        ctx,
        (*a).type_0,
        3 as libc::c_int,
        ne.as_ptr(),
        ((*a).data as *mut libc::c_char).offset(offset as isize) as *mut libc::c_void,
    );
    (*result).nb[1 as libc::c_int as usize] = nb1;
    (*result).nb[2 as libc::c_int as usize] = nb2;
    (*result).nb[3 as libc::c_int as usize] =
        ((*result).nb[2 as libc::c_int as usize]).wrapping_mul(ne2 as libc::c_ulong);
    (*result).op = GGML_OP_VIEW;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    if is_node {
        memcpy(
            ((*result).padding).as_mut_ptr() as *mut libc::c_void,
            &mut offset as *mut size_t as *const libc::c_void,
            ::core::mem::size_of::<size_t>() as libc::c_ulong,
        );
    }
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_view_4d(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut ne0: int64_t,
    mut ne1: int64_t,
    mut ne2: int64_t,
    mut ne3: int64_t,
    mut nb1: size_t,
    mut nb2: size_t,
    mut nb3: size_t,
    mut offset: size_t,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let ne: [int64_t; 4] = [ne0, ne1, ne2, ne3];
    let mut result: *mut ggml_tensor = ggml_new_tensor_impl(
        ctx,
        (*a).type_0,
        4 as libc::c_int,
        ne.as_ptr(),
        ((*a).data as *mut libc::c_char).offset(offset as isize) as *mut libc::c_void,
    );
    (*result).nb[1 as libc::c_int as usize] = nb1;
    (*result).nb[2 as libc::c_int as usize] = nb2;
    (*result).nb[3 as libc::c_int as usize] = nb3;
    (*result).op = GGML_OP_VIEW;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    if is_node {
        memcpy(
            ((*result).padding).as_mut_ptr() as *mut libc::c_void,
            &mut offset as *mut size_t as *const libc::c_void,
            ::core::mem::size_of::<size_t>() as libc::c_ulong,
        );
    }
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_permute(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut axis0: libc::c_int,
    mut axis1: libc::c_int,
    mut axis2: libc::c_int,
    mut axis3: libc::c_int,
) -> *mut ggml_tensor {
    if !(axis0 >= 0 as libc::c_int && axis0 < 4 as libc::c_int) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5857 as libc::c_int,
            b"axis0 >= 0 && axis0 < GGML_MAX_DIMS\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(axis1 >= 0 as libc::c_int && axis1 < 4 as libc::c_int) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5858 as libc::c_int,
            b"axis1 >= 0 && axis1 < GGML_MAX_DIMS\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(axis2 >= 0 as libc::c_int && axis2 < 4 as libc::c_int) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5859 as libc::c_int,
            b"axis2 >= 0 && axis2 < GGML_MAX_DIMS\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(axis3 >= 0 as libc::c_int && axis3 < 4 as libc::c_int) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5860 as libc::c_int,
            b"axis3 >= 0 && axis3 < GGML_MAX_DIMS\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(axis0 != axis1) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5862 as libc::c_int,
            b"axis0 != axis1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(axis0 != axis2) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5863 as libc::c_int,
            b"axis0 != axis2\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(axis0 != axis3) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5864 as libc::c_int,
            b"axis0 != axis3\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(axis1 != axis2) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5865 as libc::c_int,
            b"axis1 != axis2\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(axis1 != axis3) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5866 as libc::c_int,
            b"axis1 != axis3\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(axis2 != axis3) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5867 as libc::c_int,
            b"axis2 != axis3\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = ggml_view_tensor(ctx, a);
    let mut ne: [libc::c_int; 4] = [0; 4];
    let mut nb: [libc::c_int; 4] = [0; 4];
    ne[axis0 as usize] = (*a).ne[0 as libc::c_int as usize] as libc::c_int;
    ne[axis1 as usize] = (*a).ne[1 as libc::c_int as usize] as libc::c_int;
    ne[axis2 as usize] = (*a).ne[2 as libc::c_int as usize] as libc::c_int;
    ne[axis3 as usize] = (*a).ne[3 as libc::c_int as usize] as libc::c_int;
    nb[axis0 as usize] = (*a).nb[0 as libc::c_int as usize] as libc::c_int;
    nb[axis1 as usize] = (*a).nb[1 as libc::c_int as usize] as libc::c_int;
    nb[axis2 as usize] = (*a).nb[2 as libc::c_int as usize] as libc::c_int;
    nb[axis3 as usize] = (*a).nb[3 as libc::c_int as usize] as libc::c_int;
    (*result).ne[0 as libc::c_int as usize] = ne[0 as libc::c_int as usize] as int64_t;
    (*result).ne[1 as libc::c_int as usize] = ne[1 as libc::c_int as usize] as int64_t;
    (*result).ne[2 as libc::c_int as usize] = ne[2 as libc::c_int as usize] as int64_t;
    (*result).ne[3 as libc::c_int as usize] = ne[3 as libc::c_int as usize] as int64_t;
    (*result).nb[0 as libc::c_int as usize] = nb[0 as libc::c_int as usize] as size_t;
    (*result).nb[1 as libc::c_int as usize] = nb[1 as libc::c_int as usize] as size_t;
    (*result).nb[2 as libc::c_int as usize] = nb[2 as libc::c_int as usize] as size_t;
    (*result).nb[3 as libc::c_int as usize] = nb[3 as libc::c_int as usize] as size_t;
    (*result).op = GGML_OP_PERMUTE;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    if is_node {
        (*result).padding[0 as libc::c_int as usize] = axis0 as libc::c_char;
        (*result).padding[1 as libc::c_int as usize] = axis1 as libc::c_char;
        (*result).padding[2 as libc::c_int as usize] = axis2 as libc::c_char;
        (*result).padding[3 as libc::c_int as usize] = axis3 as libc::c_char;
    }
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_transpose(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = ggml_view_tensor(ctx, a);
    (*result).ne[0 as libc::c_int as usize] = (*a).ne[1 as libc::c_int as usize];
    (*result).ne[1 as libc::c_int as usize] = (*a).ne[0 as libc::c_int as usize];
    (*result).nb[0 as libc::c_int as usize] = (*a).nb[1 as libc::c_int as usize];
    (*result).nb[1 as libc::c_int as usize] = (*a).nb[0 as libc::c_int as usize];
    (*result).op = GGML_OP_TRANSPOSE;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_get_rows(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    if !(ggml_is_matrix(a) as libc::c_int != 0
        && ggml_is_vector(b) as libc::c_int != 0
        && (*b).type_0 as libc::c_uint == GGML_TYPE_I32 as libc::c_int as libc::c_uint)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5948 as libc::c_int,
            b"ggml_is_matrix(a) && ggml_is_vector(b) && b->type == GGML_TYPE_I32\0" as *const u8
                as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() || !((*b).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = ggml_new_tensor_2d(
        ctx,
        GGML_TYPE_F32,
        (*a).ne[0 as libc::c_int as usize],
        (*b).ne[0 as libc::c_int as usize],
    );
    (*result).op = GGML_OP_GET_ROWS;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_get_rows_back(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
    mut c: *mut ggml_tensor,
) -> *mut ggml_tensor {
    if !(ggml_is_matrix(a) as libc::c_int != 0
        && ggml_is_vector(b) as libc::c_int != 0
        && (*b).type_0 as libc::c_uint == GGML_TYPE_I32 as libc::c_int as libc::c_uint)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5975 as libc::c_int,
            b"ggml_is_matrix(a) && ggml_is_vector(b) && b->type == GGML_TYPE_I32\0" as *const u8
                as *const libc::c_char,
        );
        abort();
    }
    if !(ggml_is_matrix(c) as libc::c_int != 0
        && (*a).ne[0 as libc::c_int as usize] == (*c).ne[0 as libc::c_int as usize])
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            5976 as libc::c_int,
            b"ggml_is_matrix(c) && (a->ne[0] == c->ne[0])\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() || !((*b).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = ggml_new_tensor_2d(
        ctx,
        GGML_TYPE_F32,
        (*c).ne[0 as libc::c_int as usize],
        (*c).ne[1 as libc::c_int as usize],
    );
    (*result).op = GGML_OP_GET_ROWS_BACK;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    (*result).opt[0 as libc::c_int as usize] = c;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_diag(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    if !((*a).ne[1 as libc::c_int as usize] == 1 as libc::c_int as libc::c_long) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            6002 as libc::c_int,
            b"a->ne[1] == 1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let ne: [int64_t; 4] = [
        (*a).ne[0 as libc::c_int as usize],
        (*a).ne[0 as libc::c_int as usize],
        (*a).ne[2 as libc::c_int as usize],
        (*a).ne[3 as libc::c_int as usize],
    ];
    let mut result: *mut ggml_tensor = ggml_new_tensor(
        ctx,
        (*a).type_0,
        if (*a).n_dims > 2 as libc::c_int {
            (*a).n_dims
        } else {
            2 as libc::c_int
        },
        ne.as_ptr(),
    );
    (*result).op = GGML_OP_DIAG;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_diag_mask_inf_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut n_past: libc::c_int,
    mut inplace: bool,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    ggml_scratch_save(ctx);
    let mut b: *mut ggml_tensor =
        ggml_new_tensor_1d(ctx, GGML_TYPE_I32, 2 as libc::c_int as int64_t);
    *((*b).data as *mut int32_t).offset(0 as libc::c_int as isize) = n_past;
    *((*b).data as *mut int32_t).offset(1 as libc::c_int as isize) = if inplace as libc::c_int != 0
    {
        1 as libc::c_int
    } else {
        0 as libc::c_int
    };
    ggml_scratch_load(ctx);
    (*result).op = GGML_OP_DIAG_MASK_INF;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_diag_mask_inf(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut n_past: libc::c_int,
) -> *mut ggml_tensor {
    return ggml_diag_mask_inf_impl(ctx, a, n_past, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_diag_mask_inf_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut n_past: libc::c_int,
) -> *mut ggml_tensor {
    return ggml_diag_mask_inf_impl(ctx, a, n_past, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_diag_mask_zero_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut n_past: libc::c_int,
    mut inplace: bool,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    ggml_scratch_save(ctx);
    let mut b: *mut ggml_tensor =
        ggml_new_tensor_1d(ctx, GGML_TYPE_I32, 2 as libc::c_int as int64_t);
    ggml_set_name(b, b"n_past, inplace\0" as *const u8 as *const libc::c_char);
    *((*b).data as *mut int32_t).offset(0 as libc::c_int as isize) = n_past;
    *((*b).data as *mut int32_t).offset(1 as libc::c_int as isize) = if inplace as libc::c_int != 0
    {
        1 as libc::c_int
    } else {
        0 as libc::c_int
    };
    ggml_scratch_load(ctx);
    (*result).op = GGML_OP_DIAG_MASK_ZERO;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_diag_mask_zero(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut n_past: libc::c_int,
) -> *mut ggml_tensor {
    return ggml_diag_mask_zero_impl(ctx, a, n_past, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_diag_mask_zero_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut n_past: libc::c_int,
) -> *mut ggml_tensor {
    return ggml_diag_mask_zero_impl(ctx, a, n_past, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_soft_max_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut inplace: bool,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_SOFT_MAX;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = 0 as *mut ggml_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_soft_max(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_soft_max_impl(ctx, a, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_soft_max_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
) -> *mut ggml_tensor {
    return ggml_soft_max_impl(ctx, a, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_rope_impl(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut n_past: libc::c_int,
    mut n_dims: libc::c_int,
    mut mode: libc::c_int,
    mut inplace: bool,
) -> *mut ggml_tensor {
    if !(n_past >= 0 as libc::c_int) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            6158 as libc::c_int,
            b"n_past >= 0\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    ggml_scratch_save(ctx);
    let mut b: *mut ggml_tensor =
        ggml_new_tensor_1d(ctx, GGML_TYPE_I32, 3 as libc::c_int as int64_t);
    *((*b).data as *mut int32_t).offset(0 as libc::c_int as isize) = n_past;
    *((*b).data as *mut int32_t).offset(1 as libc::c_int as isize) = n_dims;
    *((*b).data as *mut int32_t).offset(2 as libc::c_int as isize) = mode;
    ggml_scratch_load(ctx);
    (*result).op = GGML_OP_ROPE;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_rope(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut n_past: libc::c_int,
    mut n_dims: libc::c_int,
    mut mode: libc::c_int,
) -> *mut ggml_tensor {
    return ggml_rope_impl(ctx, a, n_past, n_dims, mode, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_rope_inplace(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut n_past: libc::c_int,
    mut n_dims: libc::c_int,
    mut mode: libc::c_int,
) -> *mut ggml_tensor {
    return ggml_rope_impl(ctx, a, n_past, n_dims, mode, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_rope_back(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut n_past: libc::c_int,
    mut n_dims: libc::c_int,
    mut mode: libc::c_int,
) -> *mut ggml_tensor {
    if !(n_past >= 0 as libc::c_int) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            6211 as libc::c_int,
            b"n_past >= 0\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        if 0 as libc::c_int == 0 {
            fprintf(
                stderr,
                b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                b"ggml.c\0" as *const u8 as *const libc::c_char,
                6215 as libc::c_int,
                b"false\0" as *const u8 as *const libc::c_char,
            );
            abort();
        }
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = ggml_dup_tensor(ctx, a);
    ggml_scratch_save(ctx);
    let mut b: *mut ggml_tensor =
        ggml_new_tensor_1d(ctx, GGML_TYPE_I32, 3 as libc::c_int as int64_t);
    ggml_set_name(
        b,
        b"n_past, n_dims, mode\0" as *const u8 as *const libc::c_char,
    );
    *((*b).data as *mut int32_t).offset(0 as libc::c_int as isize) = n_past;
    *((*b).data as *mut int32_t).offset(1 as libc::c_int as isize) = n_dims;
    *((*b).data as *mut int32_t).offset(2 as libc::c_int as isize) = mode;
    ggml_scratch_load(ctx);
    (*result).op = GGML_OP_ROPE_BACK;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_alibi(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut n_past: libc::c_int,
    mut n_head: libc::c_int,
    mut bias_max: libc::c_float,
) -> *mut ggml_tensor {
    if !(n_past >= 0 as libc::c_int) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            6248 as libc::c_int,
            b"n_past >= 0\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        if 0 as libc::c_int == 0 {
            fprintf(
                stderr,
                b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                b"ggml.c\0" as *const u8 as *const libc::c_char,
                6252 as libc::c_int,
                b"false\0" as *const u8 as *const libc::c_char,
            );
            abort();
        }
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = ggml_view_tensor(ctx, a);
    ggml_scratch_save(ctx);
    let mut b: *mut ggml_tensor =
        ggml_new_tensor_1d(ctx, GGML_TYPE_I32, 2 as libc::c_int as int64_t);
    *((*b).data as *mut int32_t).offset(0 as libc::c_int as isize) = n_past;
    *((*b).data as *mut int32_t).offset(1 as libc::c_int as isize) = n_head;
    if !(::core::mem::size_of::<libc::c_float>() as libc::c_ulong
        == ::core::mem::size_of::<int32_t>() as libc::c_ulong)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            6266 as libc::c_int,
            b"sizeof(float) == sizeof(int32_t)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    *((*b).data as *mut libc::c_float).offset(2 as libc::c_int as isize) = bias_max;
    ggml_scratch_load(ctx);
    (*result).op = GGML_OP_ALIBI;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_clamp(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut min: libc::c_float,
    mut max: libc::c_float,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() {
        if 0 as libc::c_int == 0 {
            fprintf(
                stderr,
                b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                b"ggml.c\0" as *const u8 as *const libc::c_char,
                6289 as libc::c_int,
                b"false\0" as *const u8 as *const libc::c_char,
            );
            abort();
        }
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor = ggml_view_tensor(ctx, a);
    ggml_scratch_save(ctx);
    let mut b: *mut ggml_tensor =
        ggml_new_tensor_1d(ctx, GGML_TYPE_I32, 3 as libc::c_int as int64_t);
    *((*b).data as *mut libc::c_float).offset(0 as libc::c_int as isize) = min;
    *((*b).data as *mut libc::c_float).offset(1 as libc::c_int as isize) = max;
    ggml_scratch_load(ctx);
    (*result).op = GGML_OP_CLAMP;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_conv_1d_1s(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    if !ggml_is_matrix(b) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            6319 as libc::c_int,
            b"ggml_is_matrix(b)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*a).ne[1 as libc::c_int as usize] == (*b).ne[1 as libc::c_int as usize]) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            6320 as libc::c_int,
            b"a->ne[1] == b->ne[1]\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*a).ne[3 as libc::c_int as usize] == 1 as libc::c_int as libc::c_long) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            6321 as libc::c_int,
            b"a->ne[3] == 1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() || !((*b).grad).is_null() {
        if 0 as libc::c_int == 0 {
            fprintf(
                stderr,
                b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                b"ggml.c\0" as *const u8 as *const libc::c_char,
                6325 as libc::c_int,
                b"false\0" as *const u8 as *const libc::c_char,
            );
            abort();
        }
        is_node = 1 as libc::c_int != 0;
    }
    let ne: [int64_t; 4] = [
        (*b).ne[0 as libc::c_int as usize],
        (*a).ne[2 as libc::c_int as usize],
        1 as libc::c_int as int64_t,
        1 as libc::c_int as int64_t,
    ];
    let mut result: *mut ggml_tensor =
        ggml_new_tensor(ctx, GGML_TYPE_F32, 2 as libc::c_int, ne.as_ptr());
    (*result).op = GGML_OP_CONV_1D_1S;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_conv_1d_2s(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
) -> *mut ggml_tensor {
    if !ggml_is_matrix(b) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            6346 as libc::c_int,
            b"ggml_is_matrix(b)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*a).ne[1 as libc::c_int as usize] == (*b).ne[1 as libc::c_int as usize]) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            6347 as libc::c_int,
            b"a->ne[1] == b->ne[1]\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*a).ne[3 as libc::c_int as usize] == 1 as libc::c_int as libc::c_long) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            6348 as libc::c_int,
            b"a->ne[3] == 1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null() || !((*b).grad).is_null() {
        if 0 as libc::c_int == 0 {
            fprintf(
                stderr,
                b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                b"ggml.c\0" as *const u8 as *const libc::c_char,
                6352 as libc::c_int,
                b"false\0" as *const u8 as *const libc::c_char,
            );
            abort();
        }
        is_node = 1 as libc::c_int != 0;
    }
    let ne: [int64_t; 4] = [
        (*b).ne[0 as libc::c_int as usize] / 2 as libc::c_int as libc::c_long,
        (*a).ne[2 as libc::c_int as usize],
        1 as libc::c_int as int64_t,
        1 as libc::c_int as int64_t,
    ];
    let mut result: *mut ggml_tensor =
        ggml_new_tensor(ctx, GGML_TYPE_F32, 2 as libc::c_int, ne.as_ptr());
    (*result).op = GGML_OP_CONV_1D_2S;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_flash_attn(
    mut ctx: *mut ggml_context,
    mut q: *mut ggml_tensor,
    mut k: *mut ggml_tensor,
    mut v: *mut ggml_tensor,
    mut masked: bool,
) -> *mut ggml_tensor {
    if !ggml_can_mul_mat(k, q) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            6375 as libc::c_int,
            b"ggml_can_mul_mat(k, q)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*q).grad).is_null() || !((*k).grad).is_null() || !((*v).grad).is_null() {
        if 0 as libc::c_int == 0 {
            fprintf(
                stderr,
                b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                b"ggml.c\0" as *const u8 as *const libc::c_char,
                6381 as libc::c_int,
                b"false\0" as *const u8 as *const libc::c_char,
            );
            abort();
        }
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor =
        ggml_new_tensor(ctx, GGML_TYPE_F32, 4 as libc::c_int, ((*q).ne).as_mut_ptr());
    (*result).op = GGML_OP_FLASH_ATTN;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = q;
    (*result).src1 = k;
    (*result).opt[0 as libc::c_int as usize] = v;
    (*result).opt[1 as libc::c_int as usize] = ggml_new_i32(
        ctx,
        if masked as libc::c_int != 0 {
            1 as libc::c_int
        } else {
            0 as libc::c_int
        },
    );
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_flash_ff(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b0: *mut ggml_tensor,
    mut b1: *mut ggml_tensor,
    mut c0: *mut ggml_tensor,
    mut c1: *mut ggml_tensor,
) -> *mut ggml_tensor {
    if !ggml_can_mul_mat(b0, a) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            6407 as libc::c_int,
            b"ggml_can_mul_mat(b0, a)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !((*a).grad).is_null()
        || !((*b0).grad).is_null()
        || !((*b1).grad).is_null()
        || !((*c0).grad).is_null()
        || !((*c1).grad).is_null()
    {
        if 0 as libc::c_int == 0 {
            fprintf(
                stderr,
                b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                b"ggml.c\0" as *const u8 as *const libc::c_char,
                6413 as libc::c_int,
                b"false\0" as *const u8 as *const libc::c_char,
            );
            abort();
        }
        is_node = 1 as libc::c_int != 0;
    }
    let mut result: *mut ggml_tensor =
        ggml_new_tensor(ctx, GGML_TYPE_F32, 4 as libc::c_int, ((*a).ne).as_mut_ptr());
    (*result).op = GGML_OP_FLASH_FF;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b0;
    (*result).opt[0 as libc::c_int as usize] = b1;
    (*result).opt[1 as libc::c_int as usize] = c0;
    (*result).opt[2 as libc::c_int as usize] = c1;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_map_unary_impl_f32(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    fun: ggml_unary_op_f32_t,
    mut inplace: bool,
) -> *mut ggml_tensor {
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && !((*a).grad).is_null() {
        is_node = 1 as libc::c_int != 0;
    }
    let mut addr_tensor: *mut ggml_tensor = ggml_new_tensor_1d(
        ctx,
        GGML_TYPE_I32,
        (::core::mem::size_of::<*mut libc::c_void>() as libc::c_ulong)
            .wrapping_div(::core::mem::size_of::<int32_t>() as libc::c_ulong) as int64_t,
    );
    let ref mut fresh2 = *((*addr_tensor).data as *mut Option<unsafe extern "C" fn() -> ()>);
    *fresh2 =
        ::core::mem::transmute::<ggml_unary_op_f32_t, Option<unsafe extern "C" fn() -> ()>>(fun);
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_MAP_UNARY;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).opt[0 as libc::c_int as usize] = addr_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_map_unary_f32(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    fun: ggml_unary_op_f32_t,
) -> *mut ggml_tensor {
    return ggml_map_unary_impl_f32(ctx, a, fun, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_map_unary_inplace_f32(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    fun: ggml_unary_op_f32_t,
) -> *mut ggml_tensor {
    return ggml_map_unary_impl_f32(ctx, a, fun, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_map_binary_impl_f32(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
    fun: ggml_binary_op_f32_t,
    mut inplace: bool,
) -> *mut ggml_tensor {
    if !ggml_are_same_shape(a, b) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            6478 as libc::c_int,
            b"ggml_are_same_shape(a, b)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut is_node: bool = 0 as libc::c_int != 0;
    if !inplace && (!((*a).grad).is_null() || !((*b).grad).is_null()) {
        is_node = 1 as libc::c_int != 0;
    }
    let mut addr_tensor: *mut ggml_tensor = ggml_new_tensor_1d(
        ctx,
        GGML_TYPE_I32,
        (::core::mem::size_of::<*mut libc::c_void>() as libc::c_ulong)
            .wrapping_div(::core::mem::size_of::<int32_t>() as libc::c_ulong) as int64_t,
    );
    let ref mut fresh3 = *((*addr_tensor).data as *mut Option<unsafe extern "C" fn() -> ()>);
    *fresh3 =
        ::core::mem::transmute::<ggml_binary_op_f32_t, Option<unsafe extern "C" fn() -> ()>>(fun);
    let mut result: *mut ggml_tensor = if inplace as libc::c_int != 0 {
        ggml_view_tensor(ctx, a)
    } else {
        ggml_dup_tensor(ctx, a)
    };
    (*result).op = GGML_OP_MAP_BINARY;
    (*result).grad = if is_node as libc::c_int != 0 {
        ggml_dup_tensor(ctx, result)
    } else {
        0 as *mut ggml_tensor
    };
    (*result).src0 = a;
    (*result).src1 = b;
    (*result).opt[0 as libc::c_int as usize] = addr_tensor;
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_map_binary_f32(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
    fun: ggml_binary_op_f32_t,
) -> *mut ggml_tensor {
    return ggml_map_binary_impl_f32(ctx, a, b, fun, 0 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_map_binary_inplace_f32(
    mut ctx: *mut ggml_context,
    mut a: *mut ggml_tensor,
    mut b: *mut ggml_tensor,
    fun: ggml_binary_op_f32_t,
) -> *mut ggml_tensor {
    return ggml_map_binary_impl_f32(ctx, a, b, fun, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_set_param(mut ctx: *mut ggml_context, mut tensor: *mut ggml_tensor) {
    (*tensor).is_param = 1 as libc::c_int != 0;
    if !((*tensor).grad).is_null() {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            6522 as libc::c_int,
            b"tensor->grad == NULL\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    (*tensor).grad = ggml_dup_tensor(ctx, tensor);
}
unsafe extern "C" fn ggml_compute_forward_dup_same_cont(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !(ggml_nelements(dst) == ggml_nelements(src0)) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            6532 as libc::c_int,
            b"ggml_nelements(dst) == ggml_nelements(src0)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ggml_is_contiguous(dst) as libc::c_int != 0
        && ggml_is_contiguous(src0) as libc::c_int != 0)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            6533 as libc::c_int,
            b"ggml_is_contiguous(dst) && ggml_is_contiguous(src0)\0" as *const u8
                as *const libc::c_char,
        );
        abort();
    }
    if !((*src0).type_0 as libc::c_uint == (*dst).type_0 as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            6534 as libc::c_int,
            b"src0->type == dst->type\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let nb00: size_t = (*src0).nb[0 as libc::c_int as usize];
    let nb0: size_t = (*dst).nb[0 as libc::c_int as usize];
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let ne: libc::c_int = ggml_nelements(dst) as libc::c_int;
    let dr: libc::c_int = (ne + nth - 1 as libc::c_int) / nth;
    let ie0: libc::c_int = dr * ith;
    let ie1: libc::c_int = if ie0 + dr < ne { ie0 + dr } else { ne };
    if ie0 < ie1 {
        memcpy(
            ((*dst).data as *mut libc::c_char)
                .offset((ie0 as libc::c_ulong).wrapping_mul(nb0) as isize)
                as *mut libc::c_void,
            ((*src0).data as *mut libc::c_char)
                .offset((ie0 as libc::c_ulong).wrapping_mul(nb00) as isize)
                as *const libc::c_void,
            ((ie1 - ie0) as libc::c_ulong).wrapping_mul(GGML_TYPE_SIZE[(*src0).type_0 as usize]),
        );
    }
}
unsafe extern "C" fn ggml_compute_forward_dup_f16(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !(ggml_nelements(dst) == ggml_nelements(src0)) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            6564 as libc::c_int,
            b"ggml_nelements(dst) == ggml_nelements(src0)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let ne00: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne01: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne02: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let ne03: int64_t = (*src0).ne[3 as libc::c_int as usize];
    let ne0: int64_t = (*dst).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*dst).ne[1 as libc::c_int as usize];
    let ne2: int64_t = (*dst).ne[2 as libc::c_int as usize];
    let ne3: int64_t = (*dst).ne[3 as libc::c_int as usize];
    let nb00: size_t = (*src0).nb[0 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let nb0: size_t = (*dst).nb[0 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    if ggml_is_contiguous(src0) as libc::c_int != 0
        && ggml_is_contiguous(dst) as libc::c_int != 0
        && (*src0).type_0 as libc::c_uint == (*dst).type_0 as libc::c_uint
    {
        ggml_compute_forward_dup_same_cont(params, src0, dst);
        return;
    }
    let nr: libc::c_int = ne01 as libc::c_int;
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    if (*src0).type_0 as libc::c_uint == (*dst).type_0 as libc::c_uint
        && ne00 == ne0
        && nb00 == GGML_TYPE_SIZE[(*src0).type_0 as usize]
        && nb0 == GGML_TYPE_SIZE[(*dst).type_0 as usize]
    {
        let rs: size_t = (ne00 as libc::c_ulong).wrapping_mul(nb00);
        let mut i03: int64_t = 0 as libc::c_int as int64_t;
        while i03 < ne03 {
            let mut i02: int64_t = 0 as libc::c_int as int64_t;
            while i02 < ne02 {
                let mut i01: int64_t = ir0 as int64_t;
                while i01 < ir1 as libc::c_long {
                    memcpy(
                        ((*dst).data as *mut libc::c_char)
                            .offset((i01 as libc::c_ulong).wrapping_mul(nb1) as isize)
                            .offset((i02 as libc::c_ulong).wrapping_mul(nb2) as isize)
                            .offset((i03 as libc::c_ulong).wrapping_mul(nb3) as isize)
                            as *mut libc::c_void,
                        ((*src0).data as *mut libc::c_char)
                            .offset((i01 as libc::c_ulong).wrapping_mul(nb01) as isize)
                            .offset((i02 as libc::c_ulong).wrapping_mul(nb02) as isize)
                            .offset((i03 as libc::c_ulong).wrapping_mul(nb03) as isize)
                            as *const libc::c_void,
                        rs,
                    );
                    i01 += 1;
                    i01;
                }
                i02 += 1;
                i02;
            }
            i03 += 1;
            i03;
        }
        return;
    }
    if ggml_is_contiguous(dst) {
        if nb00 == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong {
            if (*dst).type_0 as libc::c_uint == GGML_TYPE_F16 as libc::c_int as libc::c_uint {
                let mut id: size_t = 0 as libc::c_int as size_t;
                let rs_0: size_t = (ne00 as libc::c_ulong).wrapping_mul(nb00);
                let mut dst_ptr: *mut libc::c_char = (*dst).data as *mut libc::c_char;
                let mut i03_0: libc::c_int = 0 as libc::c_int;
                while (i03_0 as libc::c_long) < ne03 {
                    let mut i02_0: libc::c_int = 0 as libc::c_int;
                    while (i02_0 as libc::c_long) < ne02 {
                        id = (id as libc::c_ulong)
                            .wrapping_add(rs_0.wrapping_mul(ir0 as libc::c_ulong))
                            as size_t as size_t;
                        let mut i01_0: libc::c_int = ir0;
                        while i01_0 < ir1 {
                            let mut src0_ptr: *const libc::c_char = ((*src0).data
                                as *mut libc::c_char)
                                .offset((i01_0 as libc::c_ulong).wrapping_mul(nb01) as isize)
                                .offset((i02_0 as libc::c_ulong).wrapping_mul(nb02) as isize)
                                .offset((i03_0 as libc::c_ulong).wrapping_mul(nb03) as isize);
                            memcpy(
                                dst_ptr.offset(id as isize) as *mut libc::c_void,
                                src0_ptr as *const libc::c_void,
                                rs_0,
                            );
                            id = (id as libc::c_ulong).wrapping_add(rs_0) as size_t as size_t;
                            i01_0 += 1;
                            i01_0;
                        }
                        id = (id as libc::c_ulong).wrapping_add(
                            rs_0.wrapping_mul((ne01 - ir1 as libc::c_long) as libc::c_ulong),
                        ) as size_t as size_t;
                        i02_0 += 1;
                        i02_0;
                    }
                    i03_0 += 1;
                    i03_0;
                }
            } else if (*dst).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint
            {
                let mut id_0: size_t = 0 as libc::c_int as size_t;
                let mut dst_ptr_0: *mut libc::c_float = (*dst).data as *mut libc::c_float;
                let mut i03_1: libc::c_int = 0 as libc::c_int;
                while (i03_1 as libc::c_long) < ne03 {
                    let mut i02_1: libc::c_int = 0 as libc::c_int;
                    while (i02_1 as libc::c_long) < ne02 {
                        id_0 = (id_0 as libc::c_ulong)
                            .wrapping_add((ne00 * ir0 as libc::c_long) as libc::c_ulong)
                            as size_t as size_t;
                        let mut i01_1: libc::c_int = ir0;
                        while i01_1 < ir1 {
                            let mut src0_ptr_0: *const ggml_fp16_t = ((*src0).data
                                as *mut libc::c_char)
                                .offset((i01_1 as libc::c_ulong).wrapping_mul(nb01) as isize)
                                .offset((i02_1 as libc::c_ulong).wrapping_mul(nb02) as isize)
                                .offset((i03_1 as libc::c_ulong).wrapping_mul(nb03) as isize)
                                as *mut ggml_fp16_t;
                            let mut i00: libc::c_int = 0 as libc::c_int;
                            while (i00 as libc::c_long) < ne00 {
                                *dst_ptr_0.offset(id_0 as isize) =
                                    ggml_lookup_fp16_to_fp32(*src0_ptr_0.offset(i00 as isize));
                                id_0 = id_0.wrapping_add(1);
                                id_0;
                                i00 += 1;
                                i00;
                            }
                            i01_1 += 1;
                            i01_1;
                        }
                        id_0 = (id_0 as libc::c_ulong)
                            .wrapping_add((ne00 * (ne01 - ir1 as libc::c_long)) as libc::c_ulong)
                            as size_t as size_t;
                        i02_1 += 1;
                        i02_1;
                    }
                    i03_1 += 1;
                    i03_1;
                }
            } else if ggml_is_quantized((*dst).type_0) {
                let quantize_row_q: quantize_row_q_t =
                    quantize_fns[(*dst).type_0 as usize].quantize_row_q;
                let mut src0_f32: *mut libc::c_float = ((*params).wdata as *mut libc::c_float)
                    .offset(
                        (ne00 as libc::c_ulong)
                            .wrapping_add(CACHE_LINE_SIZE_F32)
                            .wrapping_mul(ith as libc::c_ulong) as isize,
                    );
                let mut id_1: size_t = 0 as libc::c_int as size_t;
                let mut rs_1: size_t = nb0.wrapping_mul(
                    (ne00 / GGML_BLCK_SIZE[(*dst).type_0 as usize] as libc::c_long)
                        as libc::c_ulong,
                );
                let mut dst_ptr_1: *mut libc::c_char = (*dst).data as *mut libc::c_char;
                let mut i03_2: libc::c_int = 0 as libc::c_int;
                while (i03_2 as libc::c_long) < ne03 {
                    let mut i02_2: libc::c_int = 0 as libc::c_int;
                    while (i02_2 as libc::c_long) < ne02 {
                        id_1 = (id_1 as libc::c_ulong)
                            .wrapping_add(rs_1.wrapping_mul(ir0 as libc::c_ulong))
                            as size_t as size_t;
                        let mut i01_2: libc::c_int = ir0;
                        while i01_2 < ir1 {
                            let mut src0_ptr_1: *const ggml_fp16_t = ((*src0).data
                                as *mut libc::c_char)
                                .offset((i01_2 as libc::c_ulong).wrapping_mul(nb01) as isize)
                                .offset((i02_2 as libc::c_ulong).wrapping_mul(nb02) as isize)
                                .offset((i03_2 as libc::c_ulong).wrapping_mul(nb03) as isize)
                                as *mut ggml_fp16_t;
                            let mut i00_0: libc::c_int = 0 as libc::c_int;
                            while (i00_0 as libc::c_long) < ne00 {
                                *src0_f32.offset(i00_0 as isize) =
                                    ggml_lookup_fp16_to_fp32(*src0_ptr_1.offset(i00_0 as isize));
                                i00_0 += 1;
                                i00_0;
                            }
                            quantize_row_q.expect("non-null function pointer")(
                                src0_f32,
                                dst_ptr_1.offset(id_1 as isize) as *mut libc::c_void,
                                ne00 as libc::c_int,
                            );
                            id_1 = (id_1 as libc::c_ulong).wrapping_add(rs_1) as size_t as size_t;
                            i01_2 += 1;
                            i01_2;
                        }
                        id_1 = (id_1 as libc::c_ulong).wrapping_add(
                            rs_1.wrapping_mul((ne01 - ir1 as libc::c_long) as libc::c_ulong),
                        ) as size_t as size_t;
                        i02_2 += 1;
                        i02_2;
                    }
                    i03_2 += 1;
                    i03_2;
                }
            } else if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    6686 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        } else if (*dst).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint {
            let mut id_2: size_t = 0 as libc::c_int as size_t;
            let mut dst_ptr_2: *mut libc::c_float = (*dst).data as *mut libc::c_float;
            let mut i03_3: libc::c_int = 0 as libc::c_int;
            while (i03_3 as libc::c_long) < ne03 {
                let mut i02_3: libc::c_int = 0 as libc::c_int;
                while (i02_3 as libc::c_long) < ne02 {
                    id_2 = (id_2 as libc::c_ulong)
                        .wrapping_add((ne00 * ir0 as libc::c_long) as libc::c_ulong)
                        as size_t as size_t;
                    let mut i01_3: libc::c_int = ir0;
                    while i01_3 < ir1 {
                        let mut i00_1: libc::c_int = 0 as libc::c_int;
                        while (i00_1 as libc::c_long) < ne00 {
                            let mut src0_ptr_2: *const ggml_fp16_t = ((*src0).data
                                as *mut libc::c_char)
                                .offset((i00_1 as libc::c_ulong).wrapping_mul(nb00) as isize)
                                .offset((i01_3 as libc::c_ulong).wrapping_mul(nb01) as isize)
                                .offset((i02_3 as libc::c_ulong).wrapping_mul(nb02) as isize)
                                .offset((i03_3 as libc::c_ulong).wrapping_mul(nb03) as isize)
                                as *mut ggml_fp16_t;
                            *dst_ptr_2.offset(id_2 as isize) =
                                ggml_lookup_fp16_to_fp32(*src0_ptr_2);
                            id_2 = id_2.wrapping_add(1);
                            id_2;
                            i00_1 += 1;
                            i00_1;
                        }
                        i01_3 += 1;
                        i01_3;
                    }
                    id_2 = (id_2 as libc::c_ulong)
                        .wrapping_add((ne00 * (ne01 - ir1 as libc::c_long)) as libc::c_ulong)
                        as size_t as size_t;
                    i02_3 += 1;
                    i02_3;
                }
                i03_3 += 1;
                i03_3;
            }
        } else if (*dst).type_0 as libc::c_uint == GGML_TYPE_F16 as libc::c_int as libc::c_uint {
            let mut id_3: size_t = 0 as libc::c_int as size_t;
            let mut dst_ptr_3: *mut ggml_fp16_t = (*dst).data as *mut ggml_fp16_t;
            let mut i03_4: libc::c_int = 0 as libc::c_int;
            while (i03_4 as libc::c_long) < ne03 {
                let mut i02_4: libc::c_int = 0 as libc::c_int;
                while (i02_4 as libc::c_long) < ne02 {
                    id_3 = (id_3 as libc::c_ulong)
                        .wrapping_add((ne00 * ir0 as libc::c_long) as libc::c_ulong)
                        as size_t as size_t;
                    let mut i01_4: libc::c_int = ir0;
                    while i01_4 < ir1 {
                        let mut i00_2: libc::c_int = 0 as libc::c_int;
                        while (i00_2 as libc::c_long) < ne00 {
                            let mut src0_ptr_3: *const ggml_fp16_t = ((*src0).data
                                as *mut libc::c_char)
                                .offset((i00_2 as libc::c_ulong).wrapping_mul(nb00) as isize)
                                .offset((i01_4 as libc::c_ulong).wrapping_mul(nb01) as isize)
                                .offset((i02_4 as libc::c_ulong).wrapping_mul(nb02) as isize)
                                .offset((i03_4 as libc::c_ulong).wrapping_mul(nb03) as isize)
                                as *mut ggml_fp16_t;
                            *dst_ptr_3.offset(id_3 as isize) = *src0_ptr_3;
                            id_3 = id_3.wrapping_add(1);
                            id_3;
                            i00_2 += 1;
                            i00_2;
                        }
                        i01_4 += 1;
                        i01_4;
                    }
                    id_3 = (id_3 as libc::c_ulong)
                        .wrapping_add((ne00 * (ne01 - ir1 as libc::c_long)) as libc::c_ulong)
                        as size_t as size_t;
                    i02_4 += 1;
                    i02_4;
                }
                i03_4 += 1;
                i03_4;
            }
        } else if 0 as libc::c_int == 0 {
            fprintf(
                stderr,
                b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                b"ggml.c\0" as *const u8 as *const libc::c_char,
                6728 as libc::c_int,
                b"false\0" as *const u8 as *const libc::c_char,
            );
            abort();
        }
        return;
    }
    let mut i10: int64_t = 0 as libc::c_int as int64_t;
    let mut i11: int64_t = 0 as libc::c_int as int64_t;
    let mut i12: int64_t = 0 as libc::c_int as int64_t;
    let mut i13: int64_t = 0 as libc::c_int as int64_t;
    if (*dst).type_0 as libc::c_uint == GGML_TYPE_F16 as libc::c_int as libc::c_uint {
        let mut i03_5: int64_t = 0 as libc::c_int as int64_t;
        while i03_5 < ne03 {
            let mut i02_5: int64_t = 0 as libc::c_int as int64_t;
            while i02_5 < ne02 {
                i10 += ne00 * ir0 as libc::c_long;
                while i10 >= ne0 {
                    i10 -= ne0;
                    i11 += 1;
                    if i11 == ne1 {
                        i11 = 0 as libc::c_int as int64_t;
                        i12 += 1;
                        if i12 == ne2 {
                            i12 = 0 as libc::c_int as int64_t;
                            i13 += 1;
                            if i13 == ne3 {
                                i13 = 0 as libc::c_int as int64_t;
                            }
                        }
                    }
                }
                let mut i01_5: int64_t = ir0 as int64_t;
                while i01_5 < ir1 as libc::c_long {
                    let mut i00_3: int64_t = 0 as libc::c_int as int64_t;
                    while i00_3 < ne00 {
                        let mut src0_ptr_4: *const libc::c_char = ((*src0).data
                            as *mut libc::c_char)
                            .offset((i00_3 as libc::c_ulong).wrapping_mul(nb00) as isize)
                            .offset((i01_5 as libc::c_ulong).wrapping_mul(nb01) as isize)
                            .offset((i02_5 as libc::c_ulong).wrapping_mul(nb02) as isize)
                            .offset((i03_5 as libc::c_ulong).wrapping_mul(nb03) as isize);
                        let mut dst_ptr_4: *mut libc::c_char = ((*dst).data as *mut libc::c_char)
                            .offset((i10 as libc::c_ulong).wrapping_mul(nb0) as isize)
                            .offset((i11 as libc::c_ulong).wrapping_mul(nb1) as isize)
                            .offset((i12 as libc::c_ulong).wrapping_mul(nb2) as isize)
                            .offset((i13 as libc::c_ulong).wrapping_mul(nb3) as isize);
                        memcpy(
                            dst_ptr_4 as *mut libc::c_void,
                            src0_ptr_4 as *const libc::c_void,
                            ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong,
                        );
                        i10 += 1;
                        if i10 == ne00 {
                            i10 = 0 as libc::c_int as int64_t;
                            i11 += 1;
                            if i11 == ne01 {
                                i11 = 0 as libc::c_int as int64_t;
                                i12 += 1;
                                if i12 == ne02 {
                                    i12 = 0 as libc::c_int as int64_t;
                                    i13 += 1;
                                    if i13 == ne03 {
                                        i13 = 0 as libc::c_int as int64_t;
                                    }
                                }
                            }
                        }
                        i00_3 += 1;
                        i00_3;
                    }
                    i01_5 += 1;
                    i01_5;
                }
                i10 += ne00 * (ne01 - ir1 as libc::c_long);
                while i10 >= ne0 {
                    i10 -= ne0;
                    i11 += 1;
                    if i11 == ne1 {
                        i11 = 0 as libc::c_int as int64_t;
                        i12 += 1;
                        if i12 == ne2 {
                            i12 = 0 as libc::c_int as int64_t;
                            i13 += 1;
                            if i13 == ne3 {
                                i13 = 0 as libc::c_int as int64_t;
                            }
                        }
                    }
                }
                i02_5 += 1;
                i02_5;
            }
            i03_5 += 1;
            i03_5;
        }
    } else if (*dst).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint {
        let mut i03_6: int64_t = 0 as libc::c_int as int64_t;
        while i03_6 < ne03 {
            let mut i02_6: int64_t = 0 as libc::c_int as int64_t;
            while i02_6 < ne02 {
                i10 += ne00 * ir0 as libc::c_long;
                while i10 >= ne0 {
                    i10 -= ne0;
                    i11 += 1;
                    if i11 == ne1 {
                        i11 = 0 as libc::c_int as int64_t;
                        i12 += 1;
                        if i12 == ne2 {
                            i12 = 0 as libc::c_int as int64_t;
                            i13 += 1;
                            if i13 == ne3 {
                                i13 = 0 as libc::c_int as int64_t;
                            }
                        }
                    }
                }
                let mut i01_6: int64_t = ir0 as int64_t;
                while i01_6 < ir1 as libc::c_long {
                    let mut i00_4: int64_t = 0 as libc::c_int as int64_t;
                    while i00_4 < ne00 {
                        let mut src0_ptr_5: *const libc::c_char = ((*src0).data
                            as *mut libc::c_char)
                            .offset((i00_4 as libc::c_ulong).wrapping_mul(nb00) as isize)
                            .offset((i01_6 as libc::c_ulong).wrapping_mul(nb01) as isize)
                            .offset((i02_6 as libc::c_ulong).wrapping_mul(nb02) as isize)
                            .offset((i03_6 as libc::c_ulong).wrapping_mul(nb03) as isize);
                        let mut dst_ptr_5: *mut libc::c_char = ((*dst).data as *mut libc::c_char)
                            .offset((i10 as libc::c_ulong).wrapping_mul(nb0) as isize)
                            .offset((i11 as libc::c_ulong).wrapping_mul(nb1) as isize)
                            .offset((i12 as libc::c_ulong).wrapping_mul(nb2) as isize)
                            .offset((i13 as libc::c_ulong).wrapping_mul(nb3) as isize);
                        *(dst_ptr_5 as *mut libc::c_float) =
                            ggml_lookup_fp16_to_fp32(*(src0_ptr_5 as *const ggml_fp16_t));
                        i10 += 1;
                        if i10 == ne0 {
                            i10 = 0 as libc::c_int as int64_t;
                            i11 += 1;
                            if i11 == ne1 {
                                i11 = 0 as libc::c_int as int64_t;
                                i12 += 1;
                                if i12 == ne2 {
                                    i12 = 0 as libc::c_int as int64_t;
                                    i13 += 1;
                                    if i13 == ne3 {
                                        i13 = 0 as libc::c_int as int64_t;
                                    }
                                }
                            }
                        }
                        i00_4 += 1;
                        i00_4;
                    }
                    i01_6 += 1;
                    i01_6;
                }
                i10 += ne00 * (ne01 - ir1 as libc::c_long);
                while i10 >= ne0 {
                    i10 -= ne0;
                    i11 += 1;
                    if i11 == ne1 {
                        i11 = 0 as libc::c_int as int64_t;
                        i12 += 1;
                        if i12 == ne2 {
                            i12 = 0 as libc::c_int as int64_t;
                            i13 += 1;
                            if i13 == ne3 {
                                i13 = 0 as libc::c_int as int64_t;
                            }
                        }
                    }
                }
                i02_6 += 1;
                i02_6;
            }
            i03_6 += 1;
            i03_6;
        }
    } else if 0 as libc::c_int == 0 {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            6845 as libc::c_int,
            b"false\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
}
unsafe extern "C" fn ggml_compute_forward_dup_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !(ggml_nelements(dst) == ggml_nelements(src0)) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            6853 as libc::c_int,
            b"ggml_nelements(dst) == ggml_nelements(src0)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let ne00: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne01: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne02: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let ne03: int64_t = (*src0).ne[3 as libc::c_int as usize];
    let ne0: int64_t = (*dst).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*dst).ne[1 as libc::c_int as usize];
    let ne2: int64_t = (*dst).ne[2 as libc::c_int as usize];
    let ne3: int64_t = (*dst).ne[3 as libc::c_int as usize];
    let nb00: size_t = (*src0).nb[0 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let nb0: size_t = (*dst).nb[0 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    if ggml_is_contiguous(src0) as libc::c_int != 0
        && ggml_is_contiguous(dst) as libc::c_int != 0
        && (*src0).type_0 as libc::c_uint == (*dst).type_0 as libc::c_uint
    {
        ggml_compute_forward_dup_same_cont(params, src0, dst);
        return;
    }
    let nr: libc::c_int = ne01 as libc::c_int;
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    if (*src0).type_0 as libc::c_uint == (*dst).type_0 as libc::c_uint
        && ne00 == ne0
        && nb00 == GGML_TYPE_SIZE[(*src0).type_0 as usize]
        && nb0 == GGML_TYPE_SIZE[(*dst).type_0 as usize]
    {
        let rs: size_t = (ne00 as libc::c_ulong).wrapping_mul(nb00);
        let mut i03: int64_t = 0 as libc::c_int as int64_t;
        while i03 < ne03 {
            let mut i02: int64_t = 0 as libc::c_int as int64_t;
            while i02 < ne02 {
                let mut i01: int64_t = ir0 as int64_t;
                while i01 < ir1 as libc::c_long {
                    memcpy(
                        ((*dst).data as *mut libc::c_char)
                            .offset((i01 as libc::c_ulong).wrapping_mul(nb1) as isize)
                            .offset((i02 as libc::c_ulong).wrapping_mul(nb2) as isize)
                            .offset((i03 as libc::c_ulong).wrapping_mul(nb3) as isize)
                            as *mut libc::c_void,
                        ((*src0).data as *mut libc::c_char)
                            .offset((i01 as libc::c_ulong).wrapping_mul(nb01) as isize)
                            .offset((i02 as libc::c_ulong).wrapping_mul(nb02) as isize)
                            .offset((i03 as libc::c_ulong).wrapping_mul(nb03) as isize)
                            as *const libc::c_void,
                        rs,
                    );
                    i01 += 1;
                    i01;
                }
                i02 += 1;
                i02;
            }
            i03 += 1;
            i03;
        }
        return;
    }
    if ggml_is_contiguous(dst) {
        if nb00 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong {
            if (*dst).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint {
                let mut id: size_t = 0 as libc::c_int as size_t;
                let rs_0: size_t = (ne00 as libc::c_ulong).wrapping_mul(nb00);
                let mut dst_ptr: *mut libc::c_char = (*dst).data as *mut libc::c_char;
                let mut i03_0: libc::c_int = 0 as libc::c_int;
                while (i03_0 as libc::c_long) < ne03 {
                    let mut i02_0: libc::c_int = 0 as libc::c_int;
                    while (i02_0 as libc::c_long) < ne02 {
                        id = (id as libc::c_ulong)
                            .wrapping_add(rs_0.wrapping_mul(ir0 as libc::c_ulong))
                            as size_t as size_t;
                        let mut i01_0: libc::c_int = ir0;
                        while i01_0 < ir1 {
                            let mut src0_ptr: *const libc::c_char = ((*src0).data
                                as *mut libc::c_char)
                                .offset((i01_0 as libc::c_ulong).wrapping_mul(nb01) as isize)
                                .offset((i02_0 as libc::c_ulong).wrapping_mul(nb02) as isize)
                                .offset((i03_0 as libc::c_ulong).wrapping_mul(nb03) as isize);
                            memcpy(
                                dst_ptr.offset(id as isize) as *mut libc::c_void,
                                src0_ptr as *const libc::c_void,
                                rs_0,
                            );
                            id = (id as libc::c_ulong).wrapping_add(rs_0) as size_t as size_t;
                            i01_0 += 1;
                            i01_0;
                        }
                        id = (id as libc::c_ulong).wrapping_add(
                            rs_0.wrapping_mul((ne01 - ir1 as libc::c_long) as libc::c_ulong),
                        ) as size_t as size_t;
                        i02_0 += 1;
                        i02_0;
                    }
                    i03_0 += 1;
                    i03_0;
                }
            } else if (*dst).type_0 as libc::c_uint == GGML_TYPE_F16 as libc::c_int as libc::c_uint
            {
                let mut id_0: size_t = 0 as libc::c_int as size_t;
                let mut dst_ptr_0: *mut ggml_fp16_t = (*dst).data as *mut ggml_fp16_t;
                let mut i03_1: libc::c_int = 0 as libc::c_int;
                while (i03_1 as libc::c_long) < ne03 {
                    let mut i02_1: libc::c_int = 0 as libc::c_int;
                    while (i02_1 as libc::c_long) < ne02 {
                        id_0 = (id_0 as libc::c_ulong)
                            .wrapping_add((ne00 * ir0 as libc::c_long) as libc::c_ulong)
                            as size_t as size_t;
                        let mut i01_1: libc::c_int = ir0;
                        while i01_1 < ir1 {
                            let mut i00: libc::c_int = 0 as libc::c_int;
                            while (i00 as libc::c_long) < ne00 {
                                let mut src0_ptr_0: *const libc::c_float = ((*src0).data
                                    as *mut libc::c_char)
                                    .offset((i00 as libc::c_ulong).wrapping_mul(nb00) as isize)
                                    .offset((i01_1 as libc::c_ulong).wrapping_mul(nb01) as isize)
                                    .offset((i02_1 as libc::c_ulong).wrapping_mul(nb02) as isize)
                                    .offset((i03_1 as libc::c_ulong).wrapping_mul(nb03) as isize)
                                    as *mut libc::c_float;
                                *dst_ptr_0.offset(id_0 as isize) = ({
                                    ::core::mem::transmute::<
                                        _,
                                        [libc::c_short;
                                            ::core::mem::size_of::<__v8hi>()
                                                / ::core::mem::size_of::<libc::c_short>()],
                                    >(_mm_cvtps_ph(
                                        _mm_setr_ps(
                                            *src0_ptr_0,
                                            0 as libc::c_int as libc::c_float,
                                            0 as libc::c_int as libc::c_float,
                                            0 as libc::c_int as libc::c_float,
                                        ),
                                        0 as libc::c_int,
                                    ))[0 as libc::c_int as usize]
                                        as libc::c_ushort
                                });
                                id_0 = id_0.wrapping_add(1);
                                id_0;
                                i00 += 1;
                                i00;
                            }
                            i01_1 += 1;
                            i01_1;
                        }
                        id_0 = (id_0 as libc::c_ulong)
                            .wrapping_add((ne00 * (ne01 - ir1 as libc::c_long)) as libc::c_ulong)
                            as size_t as size_t;
                        i02_1 += 1;
                        i02_1;
                    }
                    i03_1 += 1;
                    i03_1;
                }
            } else if ggml_is_quantized((*dst).type_0) {
                let quantize_row_q: quantize_row_q_t =
                    quantize_fns[(*dst).type_0 as usize].quantize_row_q;
                let mut id_1: size_t = 0 as libc::c_int as size_t;
                let mut rs_1: size_t = nb0.wrapping_mul(
                    (ne00 / GGML_BLCK_SIZE[(*dst).type_0 as usize] as libc::c_long)
                        as libc::c_ulong,
                );
                let mut dst_ptr_1: *mut libc::c_char = (*dst).data as *mut libc::c_char;
                let mut i03_2: libc::c_int = 0 as libc::c_int;
                while (i03_2 as libc::c_long) < ne03 {
                    let mut i02_2: libc::c_int = 0 as libc::c_int;
                    while (i02_2 as libc::c_long) < ne02 {
                        id_1 = (id_1 as libc::c_ulong)
                            .wrapping_add(rs_1.wrapping_mul(ir0 as libc::c_ulong))
                            as size_t as size_t;
                        let mut i01_2: libc::c_int = ir0;
                        while i01_2 < ir1 {
                            let mut src0_ptr_1: *const libc::c_float = ((*src0).data
                                as *mut libc::c_char)
                                .offset((i01_2 as libc::c_ulong).wrapping_mul(nb01) as isize)
                                .offset((i02_2 as libc::c_ulong).wrapping_mul(nb02) as isize)
                                .offset((i03_2 as libc::c_ulong).wrapping_mul(nb03) as isize)
                                as *mut libc::c_float;
                            quantize_row_q.expect("non-null function pointer")(
                                src0_ptr_1,
                                dst_ptr_1.offset(id_1 as isize) as *mut libc::c_void,
                                ne00 as libc::c_int,
                            );
                            id_1 = (id_1 as libc::c_ulong).wrapping_add(rs_1) as size_t as size_t;
                            i01_2 += 1;
                            i01_2;
                        }
                        id_1 = (id_1 as libc::c_ulong).wrapping_add(
                            rs_1.wrapping_mul((ne01 - ir1 as libc::c_long) as libc::c_ulong),
                        ) as size_t as size_t;
                        i02_2 += 1;
                        i02_2;
                    }
                    i03_2 += 1;
                    i03_2;
                }
            } else if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    6969 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        } else if (*dst).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint {
            let mut id_2: size_t = 0 as libc::c_int as size_t;
            let mut dst_ptr_2: *mut libc::c_float = (*dst).data as *mut libc::c_float;
            let mut i03_3: libc::c_int = 0 as libc::c_int;
            while (i03_3 as libc::c_long) < ne03 {
                let mut i02_3: libc::c_int = 0 as libc::c_int;
                while (i02_3 as libc::c_long) < ne02 {
                    id_2 = (id_2 as libc::c_ulong)
                        .wrapping_add((ne00 * ir0 as libc::c_long) as libc::c_ulong)
                        as size_t as size_t;
                    let mut i01_3: libc::c_int = ir0;
                    while i01_3 < ir1 {
                        let mut i00_0: libc::c_int = 0 as libc::c_int;
                        while (i00_0 as libc::c_long) < ne00 {
                            let mut src0_ptr_2: *const libc::c_float = ((*src0).data
                                as *mut libc::c_char)
                                .offset((i00_0 as libc::c_ulong).wrapping_mul(nb00) as isize)
                                .offset((i01_3 as libc::c_ulong).wrapping_mul(nb01) as isize)
                                .offset((i02_3 as libc::c_ulong).wrapping_mul(nb02) as isize)
                                .offset((i03_3 as libc::c_ulong).wrapping_mul(nb03) as isize)
                                as *mut libc::c_float;
                            *dst_ptr_2.offset(id_2 as isize) = *src0_ptr_2;
                            id_2 = id_2.wrapping_add(1);
                            id_2;
                            i00_0 += 1;
                            i00_0;
                        }
                        i01_3 += 1;
                        i01_3;
                    }
                    id_2 = (id_2 as libc::c_ulong)
                        .wrapping_add((ne00 * (ne01 - ir1 as libc::c_long)) as libc::c_ulong)
                        as size_t as size_t;
                    i02_3 += 1;
                    i02_3;
                }
                i03_3 += 1;
                i03_3;
            }
        } else if (*dst).type_0 as libc::c_uint == GGML_TYPE_F16 as libc::c_int as libc::c_uint {
            let mut id_3: size_t = 0 as libc::c_int as size_t;
            let mut dst_ptr_3: *mut ggml_fp16_t = (*dst).data as *mut ggml_fp16_t;
            let mut i03_4: libc::c_int = 0 as libc::c_int;
            while (i03_4 as libc::c_long) < ne03 {
                let mut i02_4: libc::c_int = 0 as libc::c_int;
                while (i02_4 as libc::c_long) < ne02 {
                    id_3 = (id_3 as libc::c_ulong)
                        .wrapping_add((ne00 * ir0 as libc::c_long) as libc::c_ulong)
                        as size_t as size_t;
                    let mut i01_4: libc::c_int = ir0;
                    while i01_4 < ir1 {
                        let mut i00_1: libc::c_int = 0 as libc::c_int;
                        while (i00_1 as libc::c_long) < ne00 {
                            let mut src0_ptr_3: *const libc::c_float = ((*src0).data
                                as *mut libc::c_char)
                                .offset((i00_1 as libc::c_ulong).wrapping_mul(nb00) as isize)
                                .offset((i01_4 as libc::c_ulong).wrapping_mul(nb01) as isize)
                                .offset((i02_4 as libc::c_ulong).wrapping_mul(nb02) as isize)
                                .offset((i03_4 as libc::c_ulong).wrapping_mul(nb03) as isize)
                                as *mut libc::c_float;
                            *dst_ptr_3.offset(id_3 as isize) = ({
                                ::core::mem::transmute::<
                                    _,
                                    [libc::c_short;
                                        ::core::mem::size_of::<__v8hi>()
                                            / ::core::mem::size_of::<libc::c_short>()],
                                >(_mm_cvtps_ph(
                                    _mm_setr_ps(
                                        *src0_ptr_3,
                                        0 as libc::c_int as libc::c_float,
                                        0 as libc::c_int as libc::c_float,
                                        0 as libc::c_int as libc::c_float,
                                    ),
                                    0 as libc::c_int,
                                ))[0 as libc::c_int as usize]
                                    as libc::c_ushort
                            });
                            id_3 = id_3.wrapping_add(1);
                            id_3;
                            i00_1 += 1;
                            i00_1;
                        }
                        i01_4 += 1;
                        i01_4;
                    }
                    id_3 = (id_3 as libc::c_ulong)
                        .wrapping_add((ne00 * (ne01 - ir1 as libc::c_long)) as libc::c_ulong)
                        as size_t as size_t;
                    i02_4 += 1;
                    i02_4;
                }
                i03_4 += 1;
                i03_4;
            }
        } else if 0 as libc::c_int == 0 {
            fprintf(
                stderr,
                b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                b"ggml.c\0" as *const u8 as *const libc::c_char,
                7011 as libc::c_int,
                b"false\0" as *const u8 as *const libc::c_char,
            );
            abort();
        }
        return;
    }
    let mut i10: int64_t = 0 as libc::c_int as int64_t;
    let mut i11: int64_t = 0 as libc::c_int as int64_t;
    let mut i12: int64_t = 0 as libc::c_int as int64_t;
    let mut i13: int64_t = 0 as libc::c_int as int64_t;
    if (*dst).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint {
        let mut i03_5: int64_t = 0 as libc::c_int as int64_t;
        while i03_5 < ne03 {
            let mut i02_5: int64_t = 0 as libc::c_int as int64_t;
            while i02_5 < ne02 {
                i10 += ne00 * ir0 as libc::c_long;
                while i10 >= ne0 {
                    i10 -= ne0;
                    i11 += 1;
                    if i11 == ne1 {
                        i11 = 0 as libc::c_int as int64_t;
                        i12 += 1;
                        if i12 == ne2 {
                            i12 = 0 as libc::c_int as int64_t;
                            i13 += 1;
                            if i13 == ne3 {
                                i13 = 0 as libc::c_int as int64_t;
                            }
                        }
                    }
                }
                let mut i01_5: int64_t = ir0 as int64_t;
                while i01_5 < ir1 as libc::c_long {
                    let mut i00_2: int64_t = 0 as libc::c_int as int64_t;
                    while i00_2 < ne00 {
                        let mut src0_ptr_4: *const libc::c_char = ((*src0).data
                            as *mut libc::c_char)
                            .offset((i00_2 as libc::c_ulong).wrapping_mul(nb00) as isize)
                            .offset((i01_5 as libc::c_ulong).wrapping_mul(nb01) as isize)
                            .offset((i02_5 as libc::c_ulong).wrapping_mul(nb02) as isize)
                            .offset((i03_5 as libc::c_ulong).wrapping_mul(nb03) as isize);
                        let mut dst_ptr_4: *mut libc::c_char = ((*dst).data as *mut libc::c_char)
                            .offset((i10 as libc::c_ulong).wrapping_mul(nb0) as isize)
                            .offset((i11 as libc::c_ulong).wrapping_mul(nb1) as isize)
                            .offset((i12 as libc::c_ulong).wrapping_mul(nb2) as isize)
                            .offset((i13 as libc::c_ulong).wrapping_mul(nb3) as isize);
                        memcpy(
                            dst_ptr_4 as *mut libc::c_void,
                            src0_ptr_4 as *const libc::c_void,
                            ::core::mem::size_of::<libc::c_float>() as libc::c_ulong,
                        );
                        i10 += 1;
                        if i10 == ne0 {
                            i10 = 0 as libc::c_int as int64_t;
                            i11 += 1;
                            if i11 == ne1 {
                                i11 = 0 as libc::c_int as int64_t;
                                i12 += 1;
                                if i12 == ne2 {
                                    i12 = 0 as libc::c_int as int64_t;
                                    i13 += 1;
                                    if i13 == ne3 {
                                        i13 = 0 as libc::c_int as int64_t;
                                    }
                                }
                            }
                        }
                        i00_2 += 1;
                        i00_2;
                    }
                    i01_5 += 1;
                    i01_5;
                }
                i10 += ne00 * (ne01 - ir1 as libc::c_long);
                while i10 >= ne0 {
                    i10 -= ne0;
                    i11 += 1;
                    if i11 == ne1 {
                        i11 = 0 as libc::c_int as int64_t;
                        i12 += 1;
                        if i12 == ne2 {
                            i12 = 0 as libc::c_int as int64_t;
                            i13 += 1;
                            if i13 == ne3 {
                                i13 = 0 as libc::c_int as int64_t;
                            }
                        }
                    }
                }
                i02_5 += 1;
                i02_5;
            }
            i03_5 += 1;
            i03_5;
        }
    } else if (*dst).type_0 as libc::c_uint == GGML_TYPE_F16 as libc::c_int as libc::c_uint {
        let mut i03_6: int64_t = 0 as libc::c_int as int64_t;
        while i03_6 < ne03 {
            let mut i02_6: int64_t = 0 as libc::c_int as int64_t;
            while i02_6 < ne02 {
                i10 += ne00 * ir0 as libc::c_long;
                while i10 >= ne0 {
                    i10 -= ne0;
                    i11 += 1;
                    if i11 == ne1 {
                        i11 = 0 as libc::c_int as int64_t;
                        i12 += 1;
                        if i12 == ne2 {
                            i12 = 0 as libc::c_int as int64_t;
                            i13 += 1;
                            if i13 == ne3 {
                                i13 = 0 as libc::c_int as int64_t;
                            }
                        }
                    }
                }
                let mut i01_6: int64_t = ir0 as int64_t;
                while i01_6 < ir1 as libc::c_long {
                    let mut i00_3: int64_t = 0 as libc::c_int as int64_t;
                    while i00_3 < ne00 {
                        let mut src0_ptr_5: *const libc::c_char = ((*src0).data
                            as *mut libc::c_char)
                            .offset((i00_3 as libc::c_ulong).wrapping_mul(nb00) as isize)
                            .offset((i01_6 as libc::c_ulong).wrapping_mul(nb01) as isize)
                            .offset((i02_6 as libc::c_ulong).wrapping_mul(nb02) as isize)
                            .offset((i03_6 as libc::c_ulong).wrapping_mul(nb03) as isize);
                        let mut dst_ptr_5: *mut libc::c_char = ((*dst).data as *mut libc::c_char)
                            .offset((i10 as libc::c_ulong).wrapping_mul(nb0) as isize)
                            .offset((i11 as libc::c_ulong).wrapping_mul(nb1) as isize)
                            .offset((i12 as libc::c_ulong).wrapping_mul(nb2) as isize)
                            .offset((i13 as libc::c_ulong).wrapping_mul(nb3) as isize);
                        *(dst_ptr_5 as *mut ggml_fp16_t) = ({
                            ::core::mem::transmute::<
                                _,
                                [libc::c_short;
                                    ::core::mem::size_of::<__v8hi>()
                                        / ::core::mem::size_of::<libc::c_short>()],
                            >(_mm_cvtps_ph(
                                _mm_setr_ps(
                                    *(src0_ptr_5 as *const libc::c_float),
                                    0 as libc::c_int as libc::c_float,
                                    0 as libc::c_int as libc::c_float,
                                    0 as libc::c_int as libc::c_float,
                                ),
                                0 as libc::c_int,
                            ))[0 as libc::c_int as usize]
                                as libc::c_ushort
                        });
                        i10 += 1;
                        if i10 == ne0 {
                            i10 = 0 as libc::c_int as int64_t;
                            i11 += 1;
                            if i11 == ne1 {
                                i11 = 0 as libc::c_int as int64_t;
                                i12 += 1;
                                if i12 == ne2 {
                                    i12 = 0 as libc::c_int as int64_t;
                                    i13 += 1;
                                    if i13 == ne3 {
                                        i13 = 0 as libc::c_int as int64_t;
                                    }
                                }
                            }
                        }
                        i00_3 += 1;
                        i00_3;
                    }
                    i01_6 += 1;
                    i01_6;
                }
                i10 += ne00 * (ne01 - ir1 as libc::c_long);
                while i10 >= ne0 {
                    i10 -= ne0;
                    i11 += 1;
                    if i11 == ne1 {
                        i11 = 0 as libc::c_int as int64_t;
                        i12 += 1;
                        if i12 == ne2 {
                            i12 = 0 as libc::c_int as int64_t;
                            i13 += 1;
                            if i13 == ne3 {
                                i13 = 0 as libc::c_int as int64_t;
                            }
                        }
                    }
                }
                i02_6 += 1;
                i02_6;
            }
            i03_6 += 1;
            i03_6;
        }
    } else if 0 as libc::c_int == 0 {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7130 as libc::c_int,
            b"false\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
}
unsafe extern "C" fn ggml_compute_forward_dup(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if ggml_is_contiguous(src0) as libc::c_int != 0
        && ggml_is_contiguous(dst) as libc::c_int != 0
        && (*src0).type_0 as libc::c_uint == (*dst).type_0 as libc::c_uint
    {
        ggml_compute_forward_dup_same_cont(params, src0, dst);
        return;
    }
    match (*src0).type_0 as libc::c_uint {
        1 => {
            ggml_compute_forward_dup_f16(params, src0, dst);
        }
        0 => {
            ggml_compute_forward_dup_f32(params, src0, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    7153 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_add_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !(ggml_are_same_shape(src0, src1) as libc::c_int != 0
        && ggml_are_same_shape(src0, dst) as libc::c_int != 0)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7165 as libc::c_int,
            b"ggml_are_same_shape(src0, src1) && ggml_are_same_shape(src0, dst)\0" as *const u8
                as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nr: libc::c_int = ggml_nrows(src0);
    let ne0: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne2: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let nb00: size_t = (*src0).nb[0 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let nb10: size_t = (*src1).nb[0 as libc::c_int as usize];
    let nb11: size_t = (*src1).nb[1 as libc::c_int as usize];
    let nb12: size_t = (*src1).nb[2 as libc::c_int as usize];
    let nb13: size_t = (*src1).nb[3 as libc::c_int as usize];
    let nb0: size_t = (*dst).nb[0 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    if !(nb0 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7194 as libc::c_int,
            b"nb0 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb00 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7195 as libc::c_int,
            b"nb00 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    if nb10 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong {
        let mut ir: libc::c_int = ir0;
        while ir < ir1 {
            let i3: libc::c_int = (ir as libc::c_long / (ne2 * ne1)) as libc::c_int;
            let i2: libc::c_int =
                ((ir as libc::c_long - i3 as libc::c_long * ne2 * ne1) / ne1) as libc::c_int;
            let i1: libc::c_int = (ir as libc::c_long
                - i3 as libc::c_long * ne2 * ne1
                - i2 as libc::c_long * ne1) as libc::c_int;
            ggml_vec_add_f32(
                ne0 as libc::c_int,
                ((*dst).data as *mut libc::c_char)
                    .offset((i3 as libc::c_ulong).wrapping_mul(nb3) as isize)
                    .offset((i2 as libc::c_ulong).wrapping_mul(nb2) as isize)
                    .offset((i1 as libc::c_ulong).wrapping_mul(nb1) as isize)
                    as *mut libc::c_float,
                ((*src0).data as *mut libc::c_char)
                    .offset((i3 as libc::c_ulong).wrapping_mul(nb03) as isize)
                    .offset((i2 as libc::c_ulong).wrapping_mul(nb02) as isize)
                    .offset((i1 as libc::c_ulong).wrapping_mul(nb01) as isize)
                    as *mut libc::c_float,
                ((*src1).data as *mut libc::c_char)
                    .offset((i3 as libc::c_ulong).wrapping_mul(nb13) as isize)
                    .offset((i2 as libc::c_ulong).wrapping_mul(nb12) as isize)
                    .offset((i1 as libc::c_ulong).wrapping_mul(nb11) as isize)
                    as *mut libc::c_float,
            );
            ir += 1;
            ir;
        }
    } else {
        let mut ir_0: libc::c_int = ir0;
        while ir_0 < ir1 {
            let i3_0: libc::c_int = (ir_0 as libc::c_long / (ne2 * ne1)) as libc::c_int;
            let i2_0: libc::c_int =
                ((ir_0 as libc::c_long - i3_0 as libc::c_long * ne2 * ne1) / ne1) as libc::c_int;
            let i1_0: libc::c_int = (ir_0 as libc::c_long
                - i3_0 as libc::c_long * ne2 * ne1
                - i2_0 as libc::c_long * ne1) as libc::c_int;
            let mut dst_ptr: *mut libc::c_float = ((*dst).data as *mut libc::c_char)
                .offset((i3_0 as libc::c_ulong).wrapping_mul(nb3) as isize)
                .offset((i2_0 as libc::c_ulong).wrapping_mul(nb2) as isize)
                .offset((i1_0 as libc::c_ulong).wrapping_mul(nb1) as isize)
                as *mut libc::c_float;
            let mut src0_ptr: *mut libc::c_float = ((*src0).data as *mut libc::c_char)
                .offset((i3_0 as libc::c_ulong).wrapping_mul(nb03) as isize)
                .offset((i2_0 as libc::c_ulong).wrapping_mul(nb02) as isize)
                .offset((i1_0 as libc::c_ulong).wrapping_mul(nb01) as isize)
                as *mut libc::c_float;
            let mut i0: libc::c_int = 0 as libc::c_int;
            while (i0 as libc::c_long) < ne0 {
                let mut src1_ptr: *mut libc::c_float = ((*src1).data as *mut libc::c_char)
                    .offset((i3_0 as libc::c_ulong).wrapping_mul(nb13) as isize)
                    .offset((i2_0 as libc::c_ulong).wrapping_mul(nb12) as isize)
                    .offset((i1_0 as libc::c_ulong).wrapping_mul(nb11) as isize)
                    .offset((i0 as libc::c_ulong).wrapping_mul(nb10) as isize)
                    as *mut libc::c_float;
                *dst_ptr.offset(i0 as isize) = *src0_ptr.offset(i0 as isize) + *src1_ptr;
                i0 += 1;
                i0;
            }
            ir_0 += 1;
            ir_0;
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_add_f16_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !(ggml_are_same_shape(src0, src1) as libc::c_int != 0
        && ggml_are_same_shape(src0, dst) as libc::c_int != 0)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7251 as libc::c_int,
            b"ggml_are_same_shape(src0, src1) && ggml_are_same_shape(src0, dst)\0" as *const u8
                as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nr: libc::c_int = ggml_nrows(src0);
    let ne0: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne2: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let nb00: size_t = (*src0).nb[0 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let nb10: size_t = (*src1).nb[0 as libc::c_int as usize];
    let nb11: size_t = (*src1).nb[1 as libc::c_int as usize];
    let nb12: size_t = (*src1).nb[2 as libc::c_int as usize];
    let nb13: size_t = (*src1).nb[3 as libc::c_int as usize];
    let nb0: size_t = (*dst).nb[0 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    if !((*src0).type_0 as libc::c_uint == GGML_TYPE_F16 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7280 as libc::c_int,
            b"src0->type == GGML_TYPE_F16\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*src1).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7281 as libc::c_int,
            b"src1->type == GGML_TYPE_F32\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*dst).type_0 as libc::c_uint == GGML_TYPE_F16 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7282 as libc::c_int,
            b"dst->type == GGML_TYPE_F16\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb0 == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7284 as libc::c_int,
            b"nb0 == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb00 == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7285 as libc::c_int,
            b"nb00 == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    if nb10 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong {
        let mut ir: libc::c_int = ir0;
        while ir < ir1 {
            let i3: libc::c_int = (ir as libc::c_long / (ne2 * ne1)) as libc::c_int;
            let i2: libc::c_int =
                ((ir as libc::c_long - i3 as libc::c_long * ne2 * ne1) / ne1) as libc::c_int;
            let i1: libc::c_int = (ir as libc::c_long
                - i3 as libc::c_long * ne2 * ne1
                - i2 as libc::c_long * ne1) as libc::c_int;
            let mut dst_ptr: *mut ggml_fp16_t = ((*dst).data as *mut libc::c_char)
                .offset((i3 as libc::c_ulong).wrapping_mul(nb3) as isize)
                .offset((i2 as libc::c_ulong).wrapping_mul(nb2) as isize)
                .offset((i1 as libc::c_ulong).wrapping_mul(nb1) as isize)
                as *mut ggml_fp16_t;
            let mut src0_ptr: *mut ggml_fp16_t = ((*src0).data as *mut libc::c_char)
                .offset((i3 as libc::c_ulong).wrapping_mul(nb03) as isize)
                .offset((i2 as libc::c_ulong).wrapping_mul(nb02) as isize)
                .offset((i1 as libc::c_ulong).wrapping_mul(nb01) as isize)
                as *mut ggml_fp16_t;
            let mut src1_ptr: *mut libc::c_float = ((*src1).data as *mut libc::c_char)
                .offset((i3 as libc::c_ulong).wrapping_mul(nb13) as isize)
                .offset((i2 as libc::c_ulong).wrapping_mul(nb12) as isize)
                .offset((i1 as libc::c_ulong).wrapping_mul(nb11) as isize)
                as *mut libc::c_float;
            let mut i: libc::c_int = 0 as libc::c_int;
            while (i as libc::c_long) < ne0 {
                *dst_ptr.offset(i as isize) = ({
                    ::core::mem::transmute::<
                        _,
                        [libc::c_short;
                            ::core::mem::size_of::<__v8hi>()
                                / ::core::mem::size_of::<libc::c_short>()],
                    >(_mm_cvtps_ph(
                        _mm_setr_ps(
                            ggml_lookup_fp16_to_fp32(*src0_ptr.offset(i as isize))
                                + *src1_ptr.offset(i as isize),
                            0 as libc::c_int as libc::c_float,
                            0 as libc::c_int as libc::c_float,
                            0 as libc::c_int as libc::c_float,
                        ),
                        0 as libc::c_int,
                    ))[0 as libc::c_int as usize] as libc::c_ushort
                });
                i += 1;
                i;
            }
            ir += 1;
            ir;
        }
    } else if 0 as libc::c_int == 0 {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7312 as libc::c_int,
            b"false\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
}
unsafe extern "C" fn ggml_compute_forward_add_f16_f16(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !(ggml_are_same_shape(src0, src1) as libc::c_int != 0
        && ggml_are_same_shape(src0, dst) as libc::c_int != 0)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7321 as libc::c_int,
            b"ggml_are_same_shape(src0, src1) && ggml_are_same_shape(src0, dst)\0" as *const u8
                as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nr: libc::c_int = ggml_nrows(src0);
    let ne0: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne2: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let nb00: size_t = (*src0).nb[0 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let nb10: size_t = (*src1).nb[0 as libc::c_int as usize];
    let nb11: size_t = (*src1).nb[1 as libc::c_int as usize];
    let nb12: size_t = (*src1).nb[2 as libc::c_int as usize];
    let nb13: size_t = (*src1).nb[3 as libc::c_int as usize];
    let nb0: size_t = (*dst).nb[0 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    if !((*src0).type_0 as libc::c_uint == GGML_TYPE_F16 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7350 as libc::c_int,
            b"src0->type == GGML_TYPE_F16\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*src1).type_0 as libc::c_uint == GGML_TYPE_F16 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7351 as libc::c_int,
            b"src1->type == GGML_TYPE_F16\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*dst).type_0 as libc::c_uint == GGML_TYPE_F16 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7352 as libc::c_int,
            b"dst->type == GGML_TYPE_F16\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb0 == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7354 as libc::c_int,
            b"nb0 == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb00 == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7355 as libc::c_int,
            b"nb00 == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    if nb10 == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong {
        let mut ir: libc::c_int = ir0;
        while ir < ir1 {
            let i3: libc::c_int = (ir as libc::c_long / (ne2 * ne1)) as libc::c_int;
            let i2: libc::c_int =
                ((ir as libc::c_long - i3 as libc::c_long * ne2 * ne1) / ne1) as libc::c_int;
            let i1: libc::c_int = (ir as libc::c_long
                - i3 as libc::c_long * ne2 * ne1
                - i2 as libc::c_long * ne1) as libc::c_int;
            let mut dst_ptr: *mut ggml_fp16_t = ((*dst).data as *mut libc::c_char)
                .offset((i3 as libc::c_ulong).wrapping_mul(nb3) as isize)
                .offset((i2 as libc::c_ulong).wrapping_mul(nb2) as isize)
                .offset((i1 as libc::c_ulong).wrapping_mul(nb1) as isize)
                as *mut ggml_fp16_t;
            let mut src0_ptr: *mut ggml_fp16_t = ((*src0).data as *mut libc::c_char)
                .offset((i3 as libc::c_ulong).wrapping_mul(nb03) as isize)
                .offset((i2 as libc::c_ulong).wrapping_mul(nb02) as isize)
                .offset((i1 as libc::c_ulong).wrapping_mul(nb01) as isize)
                as *mut ggml_fp16_t;
            let mut src1_ptr: *mut ggml_fp16_t = ((*src1).data as *mut libc::c_char)
                .offset((i3 as libc::c_ulong).wrapping_mul(nb13) as isize)
                .offset((i2 as libc::c_ulong).wrapping_mul(nb12) as isize)
                .offset((i1 as libc::c_ulong).wrapping_mul(nb11) as isize)
                as *mut ggml_fp16_t;
            let mut i: libc::c_int = 0 as libc::c_int;
            while (i as libc::c_long) < ne0 {
                *dst_ptr.offset(i as isize) = ({
                    ::core::mem::transmute::<
                        _,
                        [libc::c_short;
                            ::core::mem::size_of::<__v8hi>()
                                / ::core::mem::size_of::<libc::c_short>()],
                    >(_mm_cvtps_ph(
                        _mm_setr_ps(
                            ggml_lookup_fp16_to_fp32(*src0_ptr.offset(i as isize))
                                + ggml_lookup_fp16_to_fp32(*src1_ptr.offset(i as isize)),
                            0 as libc::c_int as libc::c_float,
                            0 as libc::c_int as libc::c_float,
                            0 as libc::c_int as libc::c_float,
                        ),
                        0 as libc::c_int,
                    ))[0 as libc::c_int as usize] as libc::c_ushort
                });
                i += 1;
                i;
            }
            ir += 1;
            ir;
        }
    } else if 0 as libc::c_int == 0 {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7382 as libc::c_int,
            b"false\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
}
unsafe extern "C" fn ggml_compute_forward_add_q_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !(ggml_are_same_shape(src0, src1) as libc::c_int != 0
        && ggml_are_same_shape(src0, dst) as libc::c_int != 0)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7391 as libc::c_int,
            b"ggml_are_same_shape(src0, src1) && ggml_are_same_shape(src0, dst)\0" as *const u8
                as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let nr: libc::c_int = ggml_nrows(src0);
    let ne00: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne01: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne02: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let nb00: size_t = (*src0).nb[0 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let nb10: size_t = (*src1).nb[0 as libc::c_int as usize];
    let nb11: size_t = (*src1).nb[1 as libc::c_int as usize];
    let nb12: size_t = (*src1).nb[2 as libc::c_int as usize];
    let nb13: size_t = (*src1).nb[3 as libc::c_int as usize];
    let nb0: size_t = (*dst).nb[0 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let type_0: ggml_type = (*src0).type_0;
    let dequantize_row_q: dequantize_row_q_t = quantize_fns[type_0 as usize].dequantize_row_q;
    let quantize_row_q: quantize_row_q_t = quantize_fns[type_0 as usize].quantize_row_q;
    if !(nb00 == GGML_TYPE_SIZE[type_0 as usize]) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7426 as libc::c_int,
            b"nb00 == GGML_TYPE_SIZE[type]\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb10 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7427 as libc::c_int,
            b"nb10 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb0 <= nb1) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7430 as libc::c_int,
            b"nb0 <= nb1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb1 <= nb2) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7431 as libc::c_int,
            b"nb1 <= nb2\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb2 <= nb3) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7432 as libc::c_int,
            b"nb2 <= nb3\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_is_quantized((*src0).type_0) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7434 as libc::c_int,
            b"ggml_is_quantized(src0->type)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*dst).type_0 as libc::c_uint == (*src0).type_0 as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7435 as libc::c_int,
            b"dst->type == src0->type\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*src1).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7436 as libc::c_int,
            b"src1->type == GGML_TYPE_F32\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut wdata: *mut libc::c_float = ((*params).wdata as *mut libc::c_float).offset(
        (ne00 as libc::c_ulong)
            .wrapping_add(CACHE_LINE_SIZE_F32)
            .wrapping_mul(ith as libc::c_ulong) as isize,
    );
    let mut ir: libc::c_int = ir0;
    while ir < ir1 {
        let i03: libc::c_int = (ir as libc::c_long / (ne02 * ne01)) as libc::c_int;
        let i02: libc::c_int =
            ((ir as libc::c_long - i03 as libc::c_long * ne02 * ne01) / ne01) as libc::c_int;
        let i01: libc::c_int = (ir as libc::c_long
            - i03 as libc::c_long * ne02 * ne01
            - i02 as libc::c_long * ne01) as libc::c_int;
        let i13: libc::c_int = i03;
        let i12: libc::c_int = i02;
        let i11: libc::c_int = i01;
        let i3: libc::c_int = i03;
        let i2: libc::c_int = i02;
        let i1: libc::c_int = i01;
        let mut src0_row: *mut libc::c_void = ((*src0).data as *mut libc::c_char).offset(
            (i01 as libc::c_ulong)
                .wrapping_mul(nb01)
                .wrapping_add((i02 as libc::c_ulong).wrapping_mul(nb02))
                .wrapping_add((i03 as libc::c_ulong).wrapping_mul(nb03)) as isize,
        ) as *mut libc::c_void;
        let mut src1_row: *mut libc::c_float = ((*src1).data as *mut libc::c_char).offset(
            (i11 as libc::c_ulong)
                .wrapping_mul(nb11)
                .wrapping_add((i12 as libc::c_ulong).wrapping_mul(nb12))
                .wrapping_add((i13 as libc::c_ulong).wrapping_mul(nb13)) as isize,
        ) as *mut libc::c_float;
        let mut dst_row: *mut libc::c_void = ((*dst).data as *mut libc::c_char).offset(
            (i1 as libc::c_ulong)
                .wrapping_mul(nb1)
                .wrapping_add((i2 as libc::c_ulong).wrapping_mul(nb2))
                .wrapping_add((i3 as libc::c_ulong).wrapping_mul(nb0)) as isize,
        ) as *mut libc::c_void;
        dequantize_row_q.expect("non-null function pointer")(src0_row, wdata, ne00 as libc::c_int);
        ggml_vec_acc_f32(ne00 as libc::c_int, wdata, src1_row);
        quantize_row_q.expect("non-null function pointer")(wdata, dst_row, ne00 as libc::c_int);
        ir += 1;
        ir;
    }
}
unsafe extern "C" fn ggml_compute_forward_add(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_add_f32(params, src0, src1, dst);
        }
        1 => {
            if (*src1).type_0 as libc::c_uint == GGML_TYPE_F16 as libc::c_int as libc::c_uint {
                ggml_compute_forward_add_f16_f16(params, src0, src1, dst);
            } else if (*src1).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint
            {
                ggml_compute_forward_add_f16_f32(params, src0, src1, dst);
            } else if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    7496 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
        2 | 3 | 6 | 7 | 8 => {
            ggml_compute_forward_add_q_f32(params, src0, src1, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    7509 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_add1_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !ggml_are_same_shape(src0, dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7521 as libc::c_int,
            b"ggml_are_same_shape(src0, dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_is_scalar(src1) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7522 as libc::c_int,
            b"ggml_is_scalar(src1)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nr: libc::c_int = ggml_nrows(src0);
    let ne0: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne2: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let nb00: size_t = (*src0).nb[0 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let nb0: size_t = (*dst).nb[0 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    if !(nb0 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7546 as libc::c_int,
            b"nb0 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb00 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7547 as libc::c_int,
            b"nb00 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut ir: libc::c_int = ir0;
    while ir < ir1 {
        let i3: libc::c_int = (ir as libc::c_long / (ne2 * ne1)) as libc::c_int;
        let i2: libc::c_int =
            ((ir as libc::c_long - i3 as libc::c_long * ne2 * ne1) / ne1) as libc::c_int;
        let i1: libc::c_int = (ir as libc::c_long
            - i3 as libc::c_long * ne2 * ne1
            - i2 as libc::c_long * ne1) as libc::c_int;
        ggml_vec_add1_f32(
            ne0 as libc::c_int,
            ((*dst).data as *mut libc::c_char)
                .offset((i3 as libc::c_ulong).wrapping_mul(nb3) as isize)
                .offset((i2 as libc::c_ulong).wrapping_mul(nb2) as isize)
                .offset((i1 as libc::c_ulong).wrapping_mul(nb1) as isize)
                as *mut libc::c_float,
            ((*src0).data as *mut libc::c_char)
                .offset((i3 as libc::c_ulong).wrapping_mul(nb03) as isize)
                .offset((i2 as libc::c_ulong).wrapping_mul(nb02) as isize)
                .offset((i1 as libc::c_ulong).wrapping_mul(nb01) as isize)
                as *mut libc::c_float,
            *((*src1).data as *mut libc::c_float),
        );
        ir += 1;
        ir;
    }
}
unsafe extern "C" fn ggml_compute_forward_add1_f16_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !ggml_are_same_shape(src0, dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7584 as libc::c_int,
            b"ggml_are_same_shape(src0, dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_is_scalar(src1) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7585 as libc::c_int,
            b"ggml_is_scalar(src1)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let v: libc::c_float = *((*src1).data as *mut libc::c_float);
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nr: libc::c_int = ggml_nrows(src0);
    let ne0: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne2: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let nb00: size_t = (*src0).nb[0 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let nb0: size_t = (*dst).nb[0 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    if !((*src0).type_0 as libc::c_uint == GGML_TYPE_F16 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7612 as libc::c_int,
            b"src0->type == GGML_TYPE_F16\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*src1).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7613 as libc::c_int,
            b"src1->type == GGML_TYPE_F32\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*dst).type_0 as libc::c_uint == GGML_TYPE_F16 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7614 as libc::c_int,
            b"dst->type == GGML_TYPE_F16\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb0 == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7616 as libc::c_int,
            b"nb0 == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb00 == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7617 as libc::c_int,
            b"nb00 == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut ir: libc::c_int = ir0;
    while ir < ir1 {
        let i3: libc::c_int = (ir as libc::c_long / (ne2 * ne1)) as libc::c_int;
        let i2: libc::c_int =
            ((ir as libc::c_long - i3 as libc::c_long * ne2 * ne1) / ne1) as libc::c_int;
        let i1: libc::c_int = (ir as libc::c_long
            - i3 as libc::c_long * ne2 * ne1
            - i2 as libc::c_long * ne1) as libc::c_int;
        let mut dst_ptr: *mut ggml_fp16_t = ((*dst).data as *mut libc::c_char)
            .offset((i3 as libc::c_ulong).wrapping_mul(nb3) as isize)
            .offset((i2 as libc::c_ulong).wrapping_mul(nb2) as isize)
            .offset((i1 as libc::c_ulong).wrapping_mul(nb1) as isize)
            as *mut ggml_fp16_t;
        let mut src0_ptr: *mut ggml_fp16_t = ((*src0).data as *mut libc::c_char)
            .offset((i3 as libc::c_ulong).wrapping_mul(nb03) as isize)
            .offset((i2 as libc::c_ulong).wrapping_mul(nb02) as isize)
            .offset((i1 as libc::c_ulong).wrapping_mul(nb01) as isize)
            as *mut ggml_fp16_t;
        let mut i: libc::c_int = 0 as libc::c_int;
        while (i as libc::c_long) < ne0 {
            *dst_ptr.offset(i as isize) = ({
                ::core::mem::transmute::<
                    _,
                    [libc::c_short;
                        ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
                >(_mm_cvtps_ph(
                    _mm_setr_ps(
                        ggml_lookup_fp16_to_fp32(*src0_ptr.offset(i as isize)) + v,
                        0 as libc::c_int as libc::c_float,
                        0 as libc::c_int as libc::c_float,
                        0 as libc::c_int as libc::c_float,
                    ),
                    0 as libc::c_int,
                ))[0 as libc::c_int as usize] as libc::c_ushort
            });
            i += 1;
            i;
        }
        ir += 1;
        ir;
    }
}
unsafe extern "C" fn ggml_compute_forward_add1_f16_f16(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !ggml_are_same_shape(src0, dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7645 as libc::c_int,
            b"ggml_are_same_shape(src0, dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_is_scalar(src1) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7646 as libc::c_int,
            b"ggml_is_scalar(src1)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let v: libc::c_float = ggml_lookup_fp16_to_fp32(*((*src1).data as *mut ggml_fp16_t));
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nr: libc::c_int = ggml_nrows(src0);
    let ne0: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne2: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let nb00: size_t = (*src0).nb[0 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let nb0: size_t = (*dst).nb[0 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    if !((*src0).type_0 as libc::c_uint == GGML_TYPE_F16 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7673 as libc::c_int,
            b"src0->type == GGML_TYPE_F16\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*src1).type_0 as libc::c_uint == GGML_TYPE_F16 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7674 as libc::c_int,
            b"src1->type == GGML_TYPE_F16\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*dst).type_0 as libc::c_uint == GGML_TYPE_F16 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7675 as libc::c_int,
            b"dst->type == GGML_TYPE_F16\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb0 == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7677 as libc::c_int,
            b"nb0 == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb00 == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7678 as libc::c_int,
            b"nb00 == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut ir: libc::c_int = ir0;
    while ir < ir1 {
        let i3: libc::c_int = (ir as libc::c_long / (ne2 * ne1)) as libc::c_int;
        let i2: libc::c_int =
            ((ir as libc::c_long - i3 as libc::c_long * ne2 * ne1) / ne1) as libc::c_int;
        let i1: libc::c_int = (ir as libc::c_long
            - i3 as libc::c_long * ne2 * ne1
            - i2 as libc::c_long * ne1) as libc::c_int;
        let mut dst_ptr: *mut ggml_fp16_t = ((*dst).data as *mut libc::c_char)
            .offset((i3 as libc::c_ulong).wrapping_mul(nb3) as isize)
            .offset((i2 as libc::c_ulong).wrapping_mul(nb2) as isize)
            .offset((i1 as libc::c_ulong).wrapping_mul(nb1) as isize)
            as *mut ggml_fp16_t;
        let mut src0_ptr: *mut ggml_fp16_t = ((*src0).data as *mut libc::c_char)
            .offset((i3 as libc::c_ulong).wrapping_mul(nb03) as isize)
            .offset((i2 as libc::c_ulong).wrapping_mul(nb02) as isize)
            .offset((i1 as libc::c_ulong).wrapping_mul(nb01) as isize)
            as *mut ggml_fp16_t;
        let mut i: libc::c_int = 0 as libc::c_int;
        while (i as libc::c_long) < ne0 {
            *dst_ptr.offset(i as isize) = ({
                ::core::mem::transmute::<
                    _,
                    [libc::c_short;
                        ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
                >(_mm_cvtps_ph(
                    _mm_setr_ps(
                        ggml_lookup_fp16_to_fp32(*src0_ptr.offset(i as isize)) + v,
                        0 as libc::c_int as libc::c_float,
                        0 as libc::c_int as libc::c_float,
                        0 as libc::c_int as libc::c_float,
                    ),
                    0 as libc::c_int,
                ))[0 as libc::c_int as usize] as libc::c_ushort
            });
            i += 1;
            i;
        }
        ir += 1;
        ir;
    }
}
unsafe extern "C" fn ggml_compute_forward_add1_q_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !ggml_are_same_shape(src0, dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7706 as libc::c_int,
            b"ggml_are_same_shape(src0, dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_is_scalar(src1) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7707 as libc::c_int,
            b"ggml_is_scalar(src1)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let v: libc::c_float = *((*src1).data as *mut libc::c_float);
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nr: libc::c_int = ggml_nrows(src0);
    let ne0: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne2: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let nb00: size_t = (*src0).nb[0 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let nb0: size_t = (*dst).nb[0 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    let type_0: ggml_type = (*src0).type_0;
    let dequantize_row_q: dequantize_row_q_t = quantize_fns[type_0 as usize].dequantize_row_q;
    let quantize_row_q: quantize_row_q_t = quantize_fns[type_0 as usize].quantize_row_q;
    if !(nb00 == GGML_TYPE_SIZE[type_0 as usize]) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7739 as libc::c_int,
            b"nb00 == GGML_TYPE_SIZE[type]\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb0 <= nb1) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7742 as libc::c_int,
            b"nb0 <= nb1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb1 <= nb2) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7743 as libc::c_int,
            b"nb1 <= nb2\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb2 <= nb3) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7744 as libc::c_int,
            b"nb2 <= nb3\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_is_quantized((*src0).type_0) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7746 as libc::c_int,
            b"ggml_is_quantized(src0->type)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*dst).type_0 as libc::c_uint == (*src0).type_0 as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7747 as libc::c_int,
            b"dst->type == src0->type\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*src1).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7748 as libc::c_int,
            b"src1->type == GGML_TYPE_F32\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut wdata: *mut libc::c_float = ((*params).wdata as *mut libc::c_float).offset(
        (ne0 as libc::c_ulong)
            .wrapping_add(CACHE_LINE_SIZE_F32)
            .wrapping_mul(ith as libc::c_ulong) as isize,
    );
    let mut ir: libc::c_int = ir0;
    while ir < ir1 {
        let i3: libc::c_int = (ir as libc::c_long / (ne2 * ne1)) as libc::c_int;
        let i2: libc::c_int =
            ((ir as libc::c_long - i3 as libc::c_long * ne2 * ne1) / ne1) as libc::c_int;
        let i1: libc::c_int = (ir as libc::c_long
            - i3 as libc::c_long * ne2 * ne1
            - i2 as libc::c_long * ne1) as libc::c_int;
        let mut src0_row: *mut libc::c_void = ((*src0).data as *mut libc::c_char).offset(
            (i1 as libc::c_ulong)
                .wrapping_mul(nb01)
                .wrapping_add((i2 as libc::c_ulong).wrapping_mul(nb02))
                .wrapping_add((i3 as libc::c_ulong).wrapping_mul(nb03)) as isize,
        ) as *mut libc::c_void;
        let mut dst_row: *mut libc::c_void = ((*dst).data as *mut libc::c_char).offset(
            (i1 as libc::c_ulong)
                .wrapping_mul(nb1)
                .wrapping_add((i2 as libc::c_ulong).wrapping_mul(nb2))
                .wrapping_add((i3 as libc::c_ulong).wrapping_mul(nb0)) as isize,
        ) as *mut libc::c_void;
        dequantize_row_q.expect("non-null function pointer")(src0_row, wdata, ne0 as libc::c_int);
        ggml_vec_acc1_f32(ne0 as libc::c_int, wdata, v);
        quantize_row_q.expect("non-null function pointer")(wdata, dst_row, ne0 as libc::c_int);
        ir += 1;
        ir;
    }
}
unsafe extern "C" fn ggml_compute_forward_add1(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_add1_f32(params, src0, src1, dst);
        }
        1 => {
            if (*src1).type_0 as libc::c_uint == GGML_TYPE_F16 as libc::c_int as libc::c_uint {
                ggml_compute_forward_add1_f16_f16(params, src0, src1, dst);
            } else if (*src1).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint
            {
                ggml_compute_forward_add1_f16_f32(params, src0, src1, dst);
            } else if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    7798 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
        2 | 3 | 6 | 7 | 8 | 9 => {
            ggml_compute_forward_add1_q_f32(params, src0, src1, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    7812 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_acc_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut opt0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !ggml_are_same_shape(src0, dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7826 as libc::c_int,
            b"ggml_are_same_shape(src0, dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ggml_is_contiguous(dst) as libc::c_int != 0
        && ggml_is_contiguous(src0) as libc::c_int != 0)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7827 as libc::c_int,
            b"ggml_is_contiguous(dst) && ggml_is_contiguous(src0)\0" as *const u8
                as *const libc::c_char,
        );
        abort();
    }
    if !((*opt0).type_0 as libc::c_uint == GGML_TYPE_I32 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7829 as libc::c_int,
            b"opt0->type == GGML_TYPE_I32\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ggml_nelements(opt0) == 5 as libc::c_int as libc::c_long) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7830 as libc::c_int,
            b"ggml_nelements(opt0) == 5\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut nb1: size_t =
        *((*opt0).data as *mut int32_t).offset(0 as libc::c_int as isize) as size_t;
    let mut nb2: size_t =
        *((*opt0).data as *mut int32_t).offset(1 as libc::c_int as isize) as size_t;
    let mut nb3: size_t =
        *((*opt0).data as *mut int32_t).offset(2 as libc::c_int as isize) as size_t;
    let mut offset: size_t =
        *((*opt0).data as *mut int32_t).offset(3 as libc::c_int as isize) as size_t;
    let mut inplace: bool = *((*opt0).data as *mut int32_t).offset(4 as libc::c_int as isize) != 0;
    if !inplace && (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
    {
        memcpy(
            (*dst).data as *mut libc::c_char as *mut libc::c_void,
            (*src0).data as *mut libc::c_char as *const libc::c_void,
            ggml_nbytes(dst),
        );
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nr: libc::c_int = ggml_nrows(src1);
    let nc: libc::c_int = (*src1).ne[0 as libc::c_int as usize] as libc::c_int;
    let ne10: int64_t = (*src1).ne[0 as libc::c_int as usize];
    let ne11: int64_t = (*src1).ne[1 as libc::c_int as usize];
    let ne12: int64_t = (*src1).ne[2 as libc::c_int as usize];
    let ne13: int64_t = (*src1).ne[3 as libc::c_int as usize];
    let nb10: size_t = (*src1).nb[0 as libc::c_int as usize];
    let nb11: size_t = (*src1).nb[1 as libc::c_int as usize];
    let nb12: size_t = (*src1).nb[2 as libc::c_int as usize];
    let nb13: size_t = (*src1).nb[3 as libc::c_int as usize];
    let nb0: size_t = ggml_element_size(src0);
    let nb00: size_t = nb0;
    let nb01: size_t = nb1;
    let nb02: size_t = nb2;
    let nb03: size_t = nb3;
    if !(offset
        .wrapping_add(
            ((if ne10 == 0 as libc::c_int as libc::c_long {
                0 as libc::c_int as libc::c_long
            } else {
                ne10 - 1 as libc::c_int as libc::c_long
            }) as libc::c_ulong)
                .wrapping_mul(nb0),
        )
        .wrapping_add(
            ((if ne11 == 0 as libc::c_int as libc::c_long {
                0 as libc::c_int as libc::c_long
            } else {
                ne11 - 1 as libc::c_int as libc::c_long
            }) as libc::c_ulong)
                .wrapping_mul(nb1),
        )
        .wrapping_add(
            ((if ne12 == 0 as libc::c_int as libc::c_long {
                0 as libc::c_int as libc::c_long
            } else {
                ne12 - 1 as libc::c_int as libc::c_long
            }) as libc::c_ulong)
                .wrapping_mul(nb2),
        )
        .wrapping_add(
            ((if ne13 == 0 as libc::c_int as libc::c_long {
                0 as libc::c_int as libc::c_long
            } else {
                ne13 - 1 as libc::c_int as libc::c_long
            }) as libc::c_ulong)
                .wrapping_mul(nb3),
        )
        < ggml_nbytes(dst))
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7877 as libc::c_int,
            b"offset + (ne10 == 0 ? 0 : ne10-1)*nb0 + (ne11 == 0 ? 0 : ne11-1)*nb1 + (ne12 == 0 ? 0 : ne12-1)*nb2 + (ne13 == 0 ? 0 : ne13-1)*nb3 < ggml_nbytes(dst)\0"
                as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(offset
        .wrapping_add(
            ((if ne10 == 0 as libc::c_int as libc::c_long {
                0 as libc::c_int as libc::c_long
            } else {
                ne10 - 1 as libc::c_int as libc::c_long
            }) as libc::c_ulong)
                .wrapping_mul(nb00),
        )
        .wrapping_add(
            ((if ne11 == 0 as libc::c_int as libc::c_long {
                0 as libc::c_int as libc::c_long
            } else {
                ne11 - 1 as libc::c_int as libc::c_long
            }) as libc::c_ulong)
                .wrapping_mul(nb01),
        )
        .wrapping_add(
            ((if ne12 == 0 as libc::c_int as libc::c_long {
                0 as libc::c_int as libc::c_long
            } else {
                ne12 - 1 as libc::c_int as libc::c_long
            }) as libc::c_ulong)
                .wrapping_mul(nb02),
        )
        .wrapping_add(
            ((if ne13 == 0 as libc::c_int as libc::c_long {
                0 as libc::c_int as libc::c_long
            } else {
                ne13 - 1 as libc::c_int as libc::c_long
            }) as libc::c_ulong)
                .wrapping_mul(nb03),
        )
        < ggml_nbytes(src0))
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7878 as libc::c_int,
            b"offset + (ne10 == 0 ? 0 : ne10-1)*nb00 + (ne11 == 0 ? 0 : ne11-1)*nb01 + (ne12 == 0 ? 0 : ne12-1)*nb02 + (ne13 == 0 ? 0 : ne13-1)*nb03 < ggml_nbytes(src0)\0"
                as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb10 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7880 as libc::c_int,
            b"nb10 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut ir: libc::c_int = ir0;
    while ir < ir1 {
        let i3: libc::c_int = (ir as libc::c_long / (ne12 * ne11)) as libc::c_int;
        let i2: libc::c_int =
            ((ir as libc::c_long - i3 as libc::c_long * ne12 * ne11) / ne11) as libc::c_int;
        let i1: libc::c_int = (ir as libc::c_long
            - i3 as libc::c_long * ne12 * ne11
            - i2 as libc::c_long * ne11) as libc::c_int;
        ggml_vec_add_f32(
            nc,
            ((*dst).data as *mut libc::c_char)
                .offset((i3 as libc::c_ulong).wrapping_mul(nb3) as isize)
                .offset((i2 as libc::c_ulong).wrapping_mul(nb2) as isize)
                .offset((i1 as libc::c_ulong).wrapping_mul(nb1) as isize)
                .offset(offset as isize) as *mut libc::c_float,
            ((*src0).data as *mut libc::c_char)
                .offset((i3 as libc::c_ulong).wrapping_mul(nb03) as isize)
                .offset((i2 as libc::c_ulong).wrapping_mul(nb02) as isize)
                .offset((i1 as libc::c_ulong).wrapping_mul(nb01) as isize)
                .offset(offset as isize) as *mut libc::c_float,
            ((*src1).data as *mut libc::c_char)
                .offset((i3 as libc::c_ulong).wrapping_mul(nb13) as isize)
                .offset((i2 as libc::c_ulong).wrapping_mul(nb12) as isize)
                .offset((i1 as libc::c_ulong).wrapping_mul(nb11) as isize)
                as *mut libc::c_float,
        );
        ir += 1;
        ir;
    }
}
unsafe extern "C" fn ggml_compute_forward_acc(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut opt0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_acc_f32(params, src0, src1, opt0, dst);
        }
        1 | 2 | 3 | 6 | 7 | 8 | 9 | _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    7931 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_sub_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let nr: libc::c_int = ggml_nrows(src0);
    let ne0: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne2: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let nb00: size_t = (*src0).nb[0 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let nb10: size_t = (*src1).nb[0 as libc::c_int as usize];
    let nb11: size_t = (*src1).nb[1 as libc::c_int as usize];
    let nb12: size_t = (*src1).nb[2 as libc::c_int as usize];
    let nb13: size_t = (*src1).nb[3 as libc::c_int as usize];
    let nb0: size_t = (*dst).nb[0 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    if !(nb0 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7970 as libc::c_int,
            b"nb0 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb00 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            7971 as libc::c_int,
            b"nb00 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if nb10 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong {
        let mut ir: libc::c_int = 0 as libc::c_int;
        while ir < nr {
            let i3: libc::c_int = (ir as libc::c_long / (ne2 * ne1)) as libc::c_int;
            let i2: libc::c_int =
                ((ir as libc::c_long - i3 as libc::c_long * ne2 * ne1) / ne1) as libc::c_int;
            let i1: libc::c_int = (ir as libc::c_long
                - i3 as libc::c_long * ne2 * ne1
                - i2 as libc::c_long * ne1) as libc::c_int;
            ggml_vec_sub_f32(
                ne0 as libc::c_int,
                ((*dst).data as *mut libc::c_char)
                    .offset((i3 as libc::c_ulong).wrapping_mul(nb3) as isize)
                    .offset((i2 as libc::c_ulong).wrapping_mul(nb2) as isize)
                    .offset((i1 as libc::c_ulong).wrapping_mul(nb1) as isize)
                    as *mut libc::c_float,
                ((*src0).data as *mut libc::c_char)
                    .offset((i3 as libc::c_ulong).wrapping_mul(nb03) as isize)
                    .offset((i2 as libc::c_ulong).wrapping_mul(nb02) as isize)
                    .offset((i1 as libc::c_ulong).wrapping_mul(nb01) as isize)
                    as *mut libc::c_float,
                ((*src1).data as *mut libc::c_char)
                    .offset((i3 as libc::c_ulong).wrapping_mul(nb13) as isize)
                    .offset((i2 as libc::c_ulong).wrapping_mul(nb12) as isize)
                    .offset((i1 as libc::c_ulong).wrapping_mul(nb11) as isize)
                    as *mut libc::c_float,
            );
            ir += 1;
            ir;
        }
    } else {
        let mut ir_0: libc::c_int = 0 as libc::c_int;
        while ir_0 < nr {
            let i3_0: libc::c_int = (ir_0 as libc::c_long / (ne2 * ne1)) as libc::c_int;
            let i2_0: libc::c_int =
                ((ir_0 as libc::c_long - i3_0 as libc::c_long * ne2 * ne1) / ne1) as libc::c_int;
            let i1_0: libc::c_int = (ir_0 as libc::c_long
                - i3_0 as libc::c_long * ne2 * ne1
                - i2_0 as libc::c_long * ne1) as libc::c_int;
            let mut dst_ptr: *mut libc::c_float = ((*dst).data as *mut libc::c_char)
                .offset((i3_0 as libc::c_ulong).wrapping_mul(nb3) as isize)
                .offset((i2_0 as libc::c_ulong).wrapping_mul(nb2) as isize)
                .offset((i1_0 as libc::c_ulong).wrapping_mul(nb1) as isize)
                as *mut libc::c_float;
            let mut src0_ptr: *mut libc::c_float = ((*src0).data as *mut libc::c_char)
                .offset((i3_0 as libc::c_ulong).wrapping_mul(nb03) as isize)
                .offset((i2_0 as libc::c_ulong).wrapping_mul(nb02) as isize)
                .offset((i1_0 as libc::c_ulong).wrapping_mul(nb01) as isize)
                as *mut libc::c_float;
            let mut i0: libc::c_int = 0 as libc::c_int;
            while (i0 as libc::c_long) < ne0 {
                let mut src1_ptr: *mut libc::c_float = ((*src1).data as *mut libc::c_char)
                    .offset((i3_0 as libc::c_ulong).wrapping_mul(nb13) as isize)
                    .offset((i2_0 as libc::c_ulong).wrapping_mul(nb12) as isize)
                    .offset((i1_0 as libc::c_ulong).wrapping_mul(nb11) as isize)
                    .offset((i0 as libc::c_ulong).wrapping_mul(nb10) as isize)
                    as *mut libc::c_float;
                *dst_ptr.offset(i0 as isize) = *src0_ptr.offset(i0 as isize) - *src1_ptr;
                i0 += 1;
                i0;
            }
            ir_0 += 1;
            ir_0;
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_sub(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_sub_f32(params, src0, src1, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    8027 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_mul_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !(ggml_can_repeat_rows(src1, src0) as libc::c_int != 0
        && ggml_are_same_shape(src0, dst) as libc::c_int != 0)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8039 as libc::c_int,
            b"ggml_can_repeat_rows(src1, src0) && ggml_are_same_shape(src0, dst)\0" as *const u8
                as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nr: int64_t = ggml_nrows(src0) as int64_t;
    let ne00: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne01: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne02: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let ne10: int64_t = (*src1).ne[0 as libc::c_int as usize];
    let ne11: int64_t = (*src1).ne[1 as libc::c_int as usize];
    let ne12: int64_t = (*src1).ne[2 as libc::c_int as usize];
    let ne13: int64_t = (*src1).ne[3 as libc::c_int as usize];
    let nb00: size_t = (*src0).nb[0 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let nb10: size_t = (*src1).nb[0 as libc::c_int as usize];
    let nb11: size_t = (*src1).nb[1 as libc::c_int as usize];
    let nb12: size_t = (*src1).nb[2 as libc::c_int as usize];
    let nb13: size_t = (*src1).nb[3 as libc::c_int as usize];
    let nb0: size_t = (*dst).nb[0 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    if !(nb0 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8082 as libc::c_int,
            b"nb0 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb00 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8083 as libc::c_int,
            b"nb00 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne00 == ne10) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8084 as libc::c_int,
            b"ne00 == ne10\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if nb10 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong {
        let mut ir: int64_t = ith as int64_t;
        while ir < nr {
            let i03: int64_t = ir / (ne02 * ne01);
            let i02: int64_t = (ir - i03 * ne02 * ne01) / ne01;
            let i01: int64_t = ir - i03 * ne02 * ne01 - i02 * ne01;
            let i13: int64_t = i03 % ne13;
            let i12: int64_t = i02 % ne12;
            let i11: int64_t = i01 % ne11;
            let mut dst_ptr: *mut libc::c_float = ((*dst).data as *mut libc::c_char)
                .offset((i03 as libc::c_ulong).wrapping_mul(nb3) as isize)
                .offset((i02 as libc::c_ulong).wrapping_mul(nb2) as isize)
                .offset((i01 as libc::c_ulong).wrapping_mul(nb1) as isize)
                as *mut libc::c_float;
            let mut src0_ptr: *mut libc::c_float = ((*src0).data as *mut libc::c_char)
                .offset((i03 as libc::c_ulong).wrapping_mul(nb03) as isize)
                .offset((i02 as libc::c_ulong).wrapping_mul(nb02) as isize)
                .offset((i01 as libc::c_ulong).wrapping_mul(nb01) as isize)
                as *mut libc::c_float;
            let mut src1_ptr: *mut libc::c_float = ((*src1).data as *mut libc::c_char)
                .offset((i13 as libc::c_ulong).wrapping_mul(nb13) as isize)
                .offset((i12 as libc::c_ulong).wrapping_mul(nb12) as isize)
                .offset((i11 as libc::c_ulong).wrapping_mul(nb11) as isize)
                as *mut libc::c_float;
            ggml_vec_mul_f32(ne00 as libc::c_int, dst_ptr, src0_ptr, src1_ptr);
            ir += nth as libc::c_long;
        }
    } else {
        let mut ir_0: int64_t = ith as int64_t;
        while ir_0 < nr {
            let i03_0: int64_t = ir_0 / (ne02 * ne01);
            let i02_0: int64_t = (ir_0 - i03_0 * ne02 * ne01) / ne01;
            let i01_0: int64_t = ir_0 - i03_0 * ne02 * ne01 - i02_0 * ne01;
            let i13_0: int64_t = i03_0 % ne13;
            let i12_0: int64_t = i02_0 % ne12;
            let i11_0: int64_t = i01_0 % ne11;
            let mut dst_ptr_0: *mut libc::c_float = ((*dst).data as *mut libc::c_char)
                .offset((i03_0 as libc::c_ulong).wrapping_mul(nb3) as isize)
                .offset((i02_0 as libc::c_ulong).wrapping_mul(nb2) as isize)
                .offset((i01_0 as libc::c_ulong).wrapping_mul(nb1) as isize)
                as *mut libc::c_float;
            let mut src0_ptr_0: *mut libc::c_float = ((*src0).data as *mut libc::c_char)
                .offset((i03_0 as libc::c_ulong).wrapping_mul(nb03) as isize)
                .offset((i02_0 as libc::c_ulong).wrapping_mul(nb02) as isize)
                .offset((i01_0 as libc::c_ulong).wrapping_mul(nb01) as isize)
                as *mut libc::c_float;
            let mut i0: int64_t = 0 as libc::c_int as int64_t;
            while i0 < ne00 {
                let mut src1_ptr_0: *mut libc::c_float = ((*src1).data as *mut libc::c_char)
                    .offset((i13_0 as libc::c_ulong).wrapping_mul(nb13) as isize)
                    .offset((i12_0 as libc::c_ulong).wrapping_mul(nb12) as isize)
                    .offset((i11_0 as libc::c_ulong).wrapping_mul(nb11) as isize)
                    .offset((i0 as libc::c_ulong).wrapping_mul(nb10) as isize)
                    as *mut libc::c_float;
                *dst_ptr_0.offset(i0 as isize) = *src0_ptr_0.offset(i0 as isize) * *src1_ptr_0;
                i0 += 1;
                i0;
            }
            ir_0 += nth as libc::c_long;
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_mul(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_mul_f32(params, src0, src1, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    8148 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_div_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let nr: libc::c_int = ggml_nrows(src0);
    let ne0: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne2: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let nb00: size_t = (*src0).nb[0 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let nb10: size_t = (*src1).nb[0 as libc::c_int as usize];
    let nb11: size_t = (*src1).nb[1 as libc::c_int as usize];
    let nb12: size_t = (*src1).nb[2 as libc::c_int as usize];
    let nb13: size_t = (*src1).nb[3 as libc::c_int as usize];
    let nb0: size_t = (*dst).nb[0 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    if !(nb0 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8187 as libc::c_int,
            b"nb0 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb00 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8188 as libc::c_int,
            b"nb00 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if nb10 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong {
        let mut ir: libc::c_int = 0 as libc::c_int;
        while ir < nr {
            let i3: libc::c_int = (ir as libc::c_long / (ne2 * ne1)) as libc::c_int;
            let i2: libc::c_int =
                ((ir as libc::c_long - i3 as libc::c_long * ne2 * ne1) / ne1) as libc::c_int;
            let i1: libc::c_int = (ir as libc::c_long
                - i3 as libc::c_long * ne2 * ne1
                - i2 as libc::c_long * ne1) as libc::c_int;
            ggml_vec_div_f32(
                ne0 as libc::c_int,
                ((*dst).data as *mut libc::c_char)
                    .offset((i3 as libc::c_ulong).wrapping_mul(nb3) as isize)
                    .offset((i2 as libc::c_ulong).wrapping_mul(nb2) as isize)
                    .offset((i1 as libc::c_ulong).wrapping_mul(nb1) as isize)
                    as *mut libc::c_float,
                ((*src0).data as *mut libc::c_char)
                    .offset((i3 as libc::c_ulong).wrapping_mul(nb03) as isize)
                    .offset((i2 as libc::c_ulong).wrapping_mul(nb02) as isize)
                    .offset((i1 as libc::c_ulong).wrapping_mul(nb01) as isize)
                    as *mut libc::c_float,
                ((*src1).data as *mut libc::c_char)
                    .offset((i3 as libc::c_ulong).wrapping_mul(nb13) as isize)
                    .offset((i2 as libc::c_ulong).wrapping_mul(nb12) as isize)
                    .offset((i1 as libc::c_ulong).wrapping_mul(nb11) as isize)
                    as *mut libc::c_float,
            );
            ir += 1;
            ir;
        }
    } else {
        let mut ir_0: libc::c_int = 0 as libc::c_int;
        while ir_0 < nr {
            let i3_0: libc::c_int = (ir_0 as libc::c_long / (ne2 * ne1)) as libc::c_int;
            let i2_0: libc::c_int =
                ((ir_0 as libc::c_long - i3_0 as libc::c_long * ne2 * ne1) / ne1) as libc::c_int;
            let i1_0: libc::c_int = (ir_0 as libc::c_long
                - i3_0 as libc::c_long * ne2 * ne1
                - i2_0 as libc::c_long * ne1) as libc::c_int;
            let mut dst_ptr: *mut libc::c_float = ((*dst).data as *mut libc::c_char)
                .offset((i3_0 as libc::c_ulong).wrapping_mul(nb3) as isize)
                .offset((i2_0 as libc::c_ulong).wrapping_mul(nb2) as isize)
                .offset((i1_0 as libc::c_ulong).wrapping_mul(nb1) as isize)
                as *mut libc::c_float;
            let mut src0_ptr: *mut libc::c_float = ((*src0).data as *mut libc::c_char)
                .offset((i3_0 as libc::c_ulong).wrapping_mul(nb03) as isize)
                .offset((i2_0 as libc::c_ulong).wrapping_mul(nb02) as isize)
                .offset((i1_0 as libc::c_ulong).wrapping_mul(nb01) as isize)
                as *mut libc::c_float;
            let mut i0: libc::c_int = 0 as libc::c_int;
            while (i0 as libc::c_long) < ne0 {
                let mut src1_ptr: *mut libc::c_float = ((*src1).data as *mut libc::c_char)
                    .offset((i3_0 as libc::c_ulong).wrapping_mul(nb13) as isize)
                    .offset((i2_0 as libc::c_ulong).wrapping_mul(nb12) as isize)
                    .offset((i1_0 as libc::c_ulong).wrapping_mul(nb11) as isize)
                    .offset((i0 as libc::c_ulong).wrapping_mul(nb10) as isize)
                    as *mut libc::c_float;
                *dst_ptr.offset(i0 as isize) = *src0_ptr.offset(i0 as isize) / *src1_ptr;
                i0 += 1;
                i0;
            }
            ir_0 += 1;
            ir_0;
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_div(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_div_f32(params, src0, src1, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    8244 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_sqr_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let n: libc::c_int = ggml_nrows(src0);
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        ggml_vec_sqr_f32(
            nc,
            ((*dst).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*dst).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
            ((*src0).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*src0).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
        );
        i += 1;
        i;
    }
}
unsafe extern "C" fn ggml_compute_forward_sqr(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_sqr_f32(params, src0, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    8286 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_sqrt_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let n: libc::c_int = ggml_nrows(src0);
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        ggml_vec_sqrt_f32(
            nc,
            ((*dst).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*dst).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
            ((*src0).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*src0).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
        );
        i += 1;
        i;
    }
}
unsafe extern "C" fn ggml_compute_forward_sqrt(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_sqrt_f32(params, src0, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    8328 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_log_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !((*params).ith == 0 as libc::c_int) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8340 as libc::c_int,
            b"params->ith == 0\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_are_same_shape(src0, dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8341 as libc::c_int,
            b"ggml_are_same_shape(src0, dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let n: libc::c_int = ggml_nrows(src0);
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    if !((*dst).nb[0 as libc::c_int as usize]
        == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8350 as libc::c_int,
            b"dst->nb[0] == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*src0).nb[0 as libc::c_int as usize]
        == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8351 as libc::c_int,
            b"src0->nb[0] == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        ggml_vec_log_f32(
            nc,
            ((*dst).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*dst).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
            ((*src0).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*src0).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
        );
        i += 1;
        i;
    }
}
unsafe extern "C" fn ggml_compute_forward_log(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_log_f32(params, src0, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    8371 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_sum_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let ne00: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne01: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne02: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let ne03: int64_t = (*src0).ne[3 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let mut sum: ggml_float = 0 as libc::c_int as ggml_float;
    let mut row_sum: ggml_float = 0 as libc::c_int as ggml_float;
    let mut i03: int64_t = 0 as libc::c_int as int64_t;
    while i03 < ne03 {
        let mut i02: int64_t = 0 as libc::c_int as int64_t;
        while i02 < ne02 {
            let mut i01: int64_t = 0 as libc::c_int as int64_t;
            while i01 < ne01 {
                ggml_vec_sum_ggf(
                    ne00 as libc::c_int,
                    &mut row_sum,
                    ((*src0).data as *mut libc::c_char)
                        .offset((i01 as libc::c_ulong).wrapping_mul(nb01) as isize)
                        .offset((i02 as libc::c_ulong).wrapping_mul(nb02) as isize)
                        .offset((i03 as libc::c_ulong).wrapping_mul(nb03) as isize)
                        as *mut libc::c_float,
                );
                sum += row_sum;
                i01 += 1;
                i01;
            }
            i02 += 1;
            i02;
        }
        i03 += 1;
        i03;
    }
    *((*dst).data as *mut libc::c_float).offset(0 as libc::c_int as isize) = sum as libc::c_float;
}
unsafe extern "C" fn ggml_compute_forward_sum(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_sum_f32(params, src0, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    8428 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_sum_rows_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !((*params).ith == 0 as libc::c_int) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8439 as libc::c_int,
            b"params->ith == 0\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    if !((*src0).nb[0 as libc::c_int as usize]
        == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8445 as libc::c_int,
            b"src0->nb[0] == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*dst).nb[0 as libc::c_int as usize]
        == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8446 as libc::c_int,
            b"dst->nb[0] == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let ne00: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne01: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne02: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let ne03: int64_t = (*src0).ne[3 as libc::c_int as usize];
    let ne0: int64_t = (*dst).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*dst).ne[1 as libc::c_int as usize];
    let ne2: int64_t = (*dst).ne[2 as libc::c_int as usize];
    let ne3: int64_t = (*dst).ne[3 as libc::c_int as usize];
    if !(ne0 == 1 as libc::c_int as libc::c_long) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8458 as libc::c_int,
            b"ne0 == 1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne1 == ne01) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8459 as libc::c_int,
            b"ne1 == ne01\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne2 == ne02) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8460 as libc::c_int,
            b"ne2 == ne02\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne3 == ne03) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8461 as libc::c_int,
            b"ne3 == ne03\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    let mut i3: int64_t = 0 as libc::c_int as int64_t;
    while i3 < ne03 {
        let mut i2: int64_t = 0 as libc::c_int as int64_t;
        while i2 < ne02 {
            let mut i1: int64_t = 0 as libc::c_int as int64_t;
            while i1 < ne01 {
                let mut src_row: *mut libc::c_float = ((*src0).data as *mut libc::c_char)
                    .offset((i1 as libc::c_ulong).wrapping_mul(nb01) as isize)
                    .offset((i2 as libc::c_ulong).wrapping_mul(nb02) as isize)
                    .offset((i3 as libc::c_ulong).wrapping_mul(nb03) as isize)
                    as *mut libc::c_float;
                let mut dst_row: *mut libc::c_float = ((*dst).data as *mut libc::c_char)
                    .offset((i1 as libc::c_ulong).wrapping_mul(nb1) as isize)
                    .offset((i2 as libc::c_ulong).wrapping_mul(nb2) as isize)
                    .offset((i3 as libc::c_ulong).wrapping_mul(nb3) as isize)
                    as *mut libc::c_float;
                let mut row_sum: libc::c_float = 0 as libc::c_int as libc::c_float;
                ggml_vec_sum_f32(ne00 as libc::c_int, &mut row_sum, src_row);
                *dst_row.offset(0 as libc::c_int as isize) = row_sum;
                i1 += 1;
                i1;
            }
            i2 += 1;
            i2;
        }
        i3 += 1;
        i3;
    }
}
unsafe extern "C" fn ggml_compute_forward_sum_rows(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_sum_rows_f32(params, src0, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    8495 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_mean_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let ne00: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne01: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne02: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let ne03: int64_t = (*src0).ne[3 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let ne0: int64_t = (*dst).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*dst).ne[1 as libc::c_int as usize];
    let ne2: int64_t = (*dst).ne[2 as libc::c_int as usize];
    let ne3: int64_t = (*dst).ne[3 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    let mut i03: int64_t = 0 as libc::c_int as int64_t;
    while i03 < ne03 {
        let mut i02: int64_t = 0 as libc::c_int as int64_t;
        while i02 < ne02 {
            let mut i01: int64_t = 0 as libc::c_int as int64_t;
            while i01 < ne01 {
                ggml_vec_sum_f32(
                    ne00 as libc::c_int,
                    ((*dst).data as *mut libc::c_char)
                        .offset((i01 as libc::c_ulong).wrapping_mul(nb1) as isize)
                        .offset((i02 as libc::c_ulong).wrapping_mul(nb2) as isize)
                        .offset((i03 as libc::c_ulong).wrapping_mul(nb3) as isize)
                        as *mut libc::c_float,
                    ((*src0).data as *mut libc::c_char)
                        .offset((i01 as libc::c_ulong).wrapping_mul(nb01) as isize)
                        .offset((i02 as libc::c_ulong).wrapping_mul(nb02) as isize)
                        .offset((i03 as libc::c_ulong).wrapping_mul(nb03) as isize)
                        as *mut libc::c_float,
                );
                *(((*dst).data as *mut libc::c_char)
                    .offset((i01 as libc::c_ulong).wrapping_mul(nb1) as isize)
                    .offset((i02 as libc::c_ulong).wrapping_mul(nb2) as isize)
                    .offset((i03 as libc::c_ulong).wrapping_mul(nb3) as isize)
                    as *mut libc::c_float) /= ne00 as libc::c_float;
                i01 += 1;
                i01;
            }
            i02 += 1;
            i02;
        }
        i03 += 1;
        i03;
    }
}
unsafe extern "C" fn ggml_compute_forward_mean(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_mean_f32(params, src0, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    8566 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_repeat_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !((*params).ith == 0 as libc::c_int) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8577 as libc::c_int,
            b"params->ith == 0\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_can_repeat(src0, dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8578 as libc::c_int,
            b"ggml_can_repeat(src0, dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let ne0: int64_t = (*dst).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*dst).ne[1 as libc::c_int as usize];
    let ne2: int64_t = (*dst).ne[2 as libc::c_int as usize];
    let ne3: int64_t = (*dst).ne[3 as libc::c_int as usize];
    let ne00: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne01: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne02: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let ne03: int64_t = (*src0).ne[3 as libc::c_int as usize];
    let nb0: size_t = (*dst).nb[0 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    let nb00: size_t = (*src0).nb[0 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let nr0: libc::c_int = (ne0 / ne00) as libc::c_int;
    let nr1: libc::c_int = (ne1 / ne01) as libc::c_int;
    let nr2: libc::c_int = (ne2 / ne02) as libc::c_int;
    let nr3: libc::c_int = (ne3 / ne03) as libc::c_int;
    if !(nb0 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8611 as libc::c_int,
            b"nb0 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb00 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8612 as libc::c_int,
            b"nb00 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut i3: libc::c_int = 0 as libc::c_int;
    while i3 < nr3 {
        let mut k3: libc::c_int = 0 as libc::c_int;
        while (k3 as libc::c_long) < ne03 {
            let mut i2: libc::c_int = 0 as libc::c_int;
            while i2 < nr2 {
                let mut k2: libc::c_int = 0 as libc::c_int;
                while (k2 as libc::c_long) < ne02 {
                    let mut i1: libc::c_int = 0 as libc::c_int;
                    while i1 < nr1 {
                        let mut k1: libc::c_int = 0 as libc::c_int;
                        while (k1 as libc::c_long) < ne01 {
                            let mut i0: libc::c_int = 0 as libc::c_int;
                            while i0 < nr0 {
                                ggml_vec_cpy_f32(
                                    ne00 as libc::c_int,
                                    ((*dst).data as *mut libc::c_char)
                                        .offset(
                                            ((i3 as libc::c_long * ne03 + k3 as libc::c_long)
                                                as libc::c_ulong)
                                                .wrapping_mul(nb3)
                                                as isize,
                                        )
                                        .offset(
                                            ((i2 as libc::c_long * ne02 + k2 as libc::c_long)
                                                as libc::c_ulong)
                                                .wrapping_mul(nb2)
                                                as isize,
                                        )
                                        .offset(
                                            ((i1 as libc::c_long * ne01 + k1 as libc::c_long)
                                                as libc::c_ulong)
                                                .wrapping_mul(nb1)
                                                as isize,
                                        )
                                        .offset(
                                            ((i0 as libc::c_long * ne00) as libc::c_ulong)
                                                .wrapping_mul(nb0)
                                                as isize,
                                        ) as *mut libc::c_float,
                                    ((*src0).data as *mut libc::c_char)
                                        .offset((k3 as libc::c_ulong).wrapping_mul(nb03) as isize)
                                        .offset((k2 as libc::c_ulong).wrapping_mul(nb02) as isize)
                                        .offset((k1 as libc::c_ulong).wrapping_mul(nb01) as isize)
                                        as *mut libc::c_float,
                                );
                                i0 += 1;
                                i0;
                            }
                            k1 += 1;
                            k1;
                        }
                        i1 += 1;
                        i1;
                    }
                    k2 += 1;
                    k2;
                }
                i2 += 1;
                i2;
            }
            k3 += 1;
            k3;
        }
        i3 += 1;
        i3;
    }
}
unsafe extern "C" fn ggml_compute_forward_repeat(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_repeat_f32(params, src0, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    8645 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_abs_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let n: libc::c_int = ggml_nrows(src0);
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        ggml_vec_abs_f32(
            nc,
            ((*dst).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*dst).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
            ((*src0).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*src0).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
        );
        i += 1;
        i;
    }
}
unsafe extern "C" fn ggml_compute_forward_abs(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_abs_f32(params, src0, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    8687 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_sgn_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let n: libc::c_int = ggml_nrows(src0);
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        ggml_vec_sgn_f32(
            nc,
            ((*dst).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*dst).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
            ((*src0).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*src0).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
        );
        i += 1;
        i;
    }
}
unsafe extern "C" fn ggml_compute_forward_sgn(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_sgn_f32(params, src0, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    8729 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_neg_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let n: libc::c_int = ggml_nrows(src0);
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        ggml_vec_neg_f32(
            nc,
            ((*dst).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*dst).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
            ((*src0).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*src0).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
        );
        i += 1;
        i;
    }
}
unsafe extern "C" fn ggml_compute_forward_neg(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_neg_f32(params, src0, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    8771 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_step_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let n: libc::c_int = ggml_nrows(src0);
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        ggml_vec_step_f32(
            nc,
            ((*dst).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*dst).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
            ((*src0).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*src0).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
        );
        i += 1;
        i;
    }
}
unsafe extern "C" fn ggml_compute_forward_step(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_step_f32(params, src0, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    8813 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_relu_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let n: libc::c_int = ggml_nrows(src0);
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        ggml_vec_relu_f32(
            nc,
            ((*dst).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*dst).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
            ((*src0).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*src0).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
        );
        i += 1;
        i;
    }
}
unsafe extern "C" fn ggml_compute_forward_relu(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_relu_f32(params, src0, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    8855 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_gelu_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !ggml_is_contiguous(src0) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8866 as libc::c_int,
            b"ggml_is_contiguous(src0)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_is_contiguous(dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8867 as libc::c_int,
            b"ggml_is_contiguous(dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_are_same_shape(src0, dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8868 as libc::c_int,
            b"ggml_are_same_shape(src0, dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let nr: libc::c_int = ggml_nrows(src0);
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut i1: libc::c_int = ir0;
    while i1 < ir1 {
        ggml_vec_gelu_f32(
            nc,
            ((*dst).data as *mut libc::c_char).offset(
                (i1 as libc::c_ulong).wrapping_mul((*dst).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
            ((*src0).data as *mut libc::c_char).offset(
                (i1 as libc::c_ulong).wrapping_mul((*src0).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
        );
        i1 += 1;
        i1;
    }
}
unsafe extern "C" fn ggml_compute_forward_gelu(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_gelu_f32(params, src0, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    8914 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_silu_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !ggml_is_contiguous(src0) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8927 as libc::c_int,
            b"ggml_is_contiguous(src0)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_is_contiguous(dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8928 as libc::c_int,
            b"ggml_is_contiguous(dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_are_same_shape(src0, dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8929 as libc::c_int,
            b"ggml_are_same_shape(src0, dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let nr: libc::c_int = ggml_nrows(src0);
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut i1: libc::c_int = ir0;
    while i1 < ir1 {
        ggml_vec_silu_f32(
            nc,
            ((*dst).data as *mut libc::c_char).offset(
                (i1 as libc::c_ulong).wrapping_mul((*dst).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
            ((*src0).data as *mut libc::c_char).offset(
                (i1 as libc::c_ulong).wrapping_mul((*src0).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
        );
        i1 += 1;
        i1;
    }
}
unsafe extern "C" fn ggml_compute_forward_silu(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_silu_f32(params, src0, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    8975 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_silu_back_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut grad: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !ggml_is_contiguous(grad) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8988 as libc::c_int,
            b"ggml_is_contiguous(grad)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_is_contiguous(src0) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8989 as libc::c_int,
            b"ggml_is_contiguous(src0)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_is_contiguous(dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8990 as libc::c_int,
            b"ggml_is_contiguous(dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_are_same_shape(src0, dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8991 as libc::c_int,
            b"ggml_are_same_shape(src0, dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_are_same_shape(src0, grad) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            8992 as libc::c_int,
            b"ggml_are_same_shape(src0, grad)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let nr: libc::c_int = ggml_nrows(src0);
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut i1: libc::c_int = ir0;
    while i1 < ir1 {
        ggml_vec_silu_backward_f32(
            nc,
            ((*dst).data as *mut libc::c_char).offset(
                (i1 as libc::c_ulong).wrapping_mul((*dst).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
            ((*src0).data as *mut libc::c_char).offset(
                (i1 as libc::c_ulong).wrapping_mul((*src0).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
            ((*grad).data as *mut libc::c_char).offset(
                (i1 as libc::c_ulong).wrapping_mul((*grad).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
        );
        i1 += 1;
        i1;
    }
}
unsafe extern "C" fn ggml_compute_forward_silu_back(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut grad: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_silu_back_f32(params, src0, grad, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    9040 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_norm_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !ggml_are_same_shape(src0, dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9051 as libc::c_int,
            b"ggml_are_same_shape(src0, dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    if !((*src0).nb[0 as libc::c_int as usize]
        == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9057 as libc::c_int,
            b"src0->nb[0] == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let ne00: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne01: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne02: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let ne03: int64_t = (*src0).ne[3 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    let eps: libc::c_float = 1e-5f32;
    let mut i03: int64_t = 0 as libc::c_int as int64_t;
    while i03 < ne03 {
        let mut i02: int64_t = 0 as libc::c_int as int64_t;
        while i02 < ne02 {
            let mut i01: int64_t = ith as int64_t;
            while i01 < ne01 {
                let mut x: *const libc::c_float = ((*src0).data as *mut libc::c_char)
                    .offset((i01 as libc::c_ulong).wrapping_mul(nb01) as isize)
                    .offset((i02 as libc::c_ulong).wrapping_mul(nb02) as isize)
                    .offset((i03 as libc::c_ulong).wrapping_mul(nb03) as isize)
                    as *mut libc::c_float;
                let mut sum: ggml_float = 0.0f64;
                let mut i00: int64_t = 0 as libc::c_int as int64_t;
                while i00 < ne00 {
                    sum += *x.offset(i00 as isize) as ggml_float;
                    i00 += 1;
                    i00;
                }
                let mut mean: libc::c_float = (sum / ne00 as libc::c_double) as libc::c_float;
                let mut y: *mut libc::c_float = ((*dst).data as *mut libc::c_char)
                    .offset((i01 as libc::c_ulong).wrapping_mul(nb1) as isize)
                    .offset((i02 as libc::c_ulong).wrapping_mul(nb2) as isize)
                    .offset((i03 as libc::c_ulong).wrapping_mul(nb3) as isize)
                    as *mut libc::c_float;
                let mut sum2: ggml_float = 0.0f64;
                let mut i00_0: int64_t = 0 as libc::c_int as int64_t;
                while i00_0 < ne00 {
                    let mut v: libc::c_float = *x.offset(i00_0 as isize) - mean;
                    *y.offset(i00_0 as isize) = v;
                    sum2 += (v * v) as ggml_float;
                    i00_0 += 1;
                    i00_0;
                }
                let mut variance: libc::c_float = (sum2 / ne00 as libc::c_double) as libc::c_float;
                let scale: libc::c_float = 1.0f32 / sqrtf(variance + eps);
                ggml_vec_scale_f32(ne00 as libc::c_int, y, scale);
                i01 += nth as libc::c_long;
            }
            i02 += 1;
            i02;
        }
        i03 += 1;
        i03;
    }
}
unsafe extern "C" fn ggml_compute_forward_norm(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_norm_f32(params, src0, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    9119 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_rms_norm_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !ggml_are_same_shape(src0, dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9128 as libc::c_int,
            b"ggml_are_same_shape(src0, dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    if !((*src0).nb[0 as libc::c_int as usize]
        == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9134 as libc::c_int,
            b"src0->nb[0] == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let ne00: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne01: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne02: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let ne03: int64_t = (*src0).ne[3 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    let eps: libc::c_float = 1e-6f32;
    let mut i03: int64_t = 0 as libc::c_int as int64_t;
    while i03 < ne03 {
        let mut i02: int64_t = 0 as libc::c_int as int64_t;
        while i02 < ne02 {
            let mut i01: int64_t = ith as int64_t;
            while i01 < ne01 {
                let mut x: *const libc::c_float = ((*src0).data as *mut libc::c_char)
                    .offset((i01 as libc::c_ulong).wrapping_mul(nb01) as isize)
                    .offset((i02 as libc::c_ulong).wrapping_mul(nb02) as isize)
                    .offset((i03 as libc::c_ulong).wrapping_mul(nb03) as isize)
                    as *mut libc::c_float;
                let mut sum: ggml_float = 0.0f64;
                let mut i00: int64_t = 0 as libc::c_int as int64_t;
                while i00 < ne00 {
                    sum += (*x.offset(i00 as isize) * *x.offset(i00 as isize)) as ggml_float;
                    i00 += 1;
                    i00;
                }
                let mut mean: libc::c_float = (sum / ne00 as libc::c_double) as libc::c_float;
                let mut y: *mut libc::c_float = ((*dst).data as *mut libc::c_char)
                    .offset((i01 as libc::c_ulong).wrapping_mul(nb1) as isize)
                    .offset((i02 as libc::c_ulong).wrapping_mul(nb2) as isize)
                    .offset((i03 as libc::c_ulong).wrapping_mul(nb3) as isize)
                    as *mut libc::c_float;
                memcpy(
                    y as *mut libc::c_void,
                    x as *const libc::c_void,
                    (ne00 as libc::c_ulong)
                        .wrapping_mul(::core::mem::size_of::<libc::c_float>() as libc::c_ulong),
                );
                let scale: libc::c_float = 1.0f32 / sqrtf(mean + eps);
                ggml_vec_scale_f32(ne00 as libc::c_int, y, scale);
                i01 += nth as libc::c_long;
            }
            i02 += 1;
            i02;
        }
        i03 += 1;
        i03;
    }
}
unsafe extern "C" fn ggml_compute_forward_rms_norm(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_rms_norm_f32(params, src0, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    9193 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_rms_norm_back_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !(ggml_are_same_shape(src0, dst) as libc::c_int != 0
        && ggml_are_same_shape(src0, src1) as libc::c_int != 0)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9204 as libc::c_int,
            b"ggml_are_same_shape(src0, dst) && ggml_are_same_shape(src0, src1)\0" as *const u8
                as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    if !((*src0).nb[0 as libc::c_int as usize]
        == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9210 as libc::c_int,
            b"src0->nb[0] == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let ne00: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne01: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne02: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let ne03: int64_t = (*src0).ne[3 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let nb11: size_t = (*src1).nb[1 as libc::c_int as usize];
    let nb12: size_t = (*src1).nb[2 as libc::c_int as usize];
    let nb13: size_t = (*src1).nb[3 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    let eps: libc::c_float = 1e-6f32;
    let mut i03: int64_t = 0 as libc::c_int as int64_t;
    while i03 < ne03 {
        let mut i02: int64_t = 0 as libc::c_int as int64_t;
        while i02 < ne02 {
            let mut i01: int64_t = ith as int64_t;
            while i01 < ne01 {
                let i11: int64_t = i01;
                let i12: int64_t = i02;
                let i13: int64_t = i03;
                let mut x: *const libc::c_float = ((*src0).data as *mut libc::c_char)
                    .offset((i01 as libc::c_ulong).wrapping_mul(nb01) as isize)
                    .offset((i02 as libc::c_ulong).wrapping_mul(nb02) as isize)
                    .offset((i03 as libc::c_ulong).wrapping_mul(nb03) as isize)
                    as *mut libc::c_float;
                let mut dz: *const libc::c_float = ((*src1).data as *mut libc::c_char)
                    .offset((i11 as libc::c_ulong).wrapping_mul(nb11) as isize)
                    .offset((i12 as libc::c_ulong).wrapping_mul(nb12) as isize)
                    .offset((i13 as libc::c_ulong).wrapping_mul(nb13) as isize)
                    as *mut libc::c_float;
                let mut sum_xx: ggml_float = 0.0f64;
                let mut sum_xdz: ggml_float = 0.0f64;
                let mut i00: int64_t = 0 as libc::c_int as int64_t;
                while i00 < ne00 {
                    sum_xx += (*x.offset(i00 as isize) * *x.offset(i00 as isize)) as ggml_float;
                    sum_xdz += (*x.offset(i00 as isize) * *dz.offset(i00 as isize)) as ggml_float;
                    i00 += 1;
                    i00;
                }
                let mean_eps: libc::c_float = sum_xx as libc::c_float / ne00 as libc::c_float + eps;
                let sum_eps: libc::c_float = sum_xx as libc::c_float + eps * ne00 as libc::c_float;
                let rrms: libc::c_float = 1.0f32 / sqrtf(mean_eps);
                let mut dx: *mut libc::c_float = ((*dst).data as *mut libc::c_char)
                    .offset((i01 as libc::c_ulong).wrapping_mul(nb1) as isize)
                    .offset((i02 as libc::c_ulong).wrapping_mul(nb2) as isize)
                    .offset((i03 as libc::c_ulong).wrapping_mul(nb3) as isize)
                    as *mut libc::c_float;
                ggml_vec_cpy_f32(ne00 as libc::c_int, dx, x);
                ggml_vec_scale_f32(ne00 as libc::c_int, dx, -sum_xdz as libc::c_float / sum_eps);
                ggml_vec_acc_f32(ne00 as libc::c_int, dx, dz);
                ggml_vec_scale_f32(ne00 as libc::c_int, dx, rrms);
                i01 += nth as libc::c_long;
            }
            i02 += 1;
            i02;
        }
        i03 += 1;
        i03;
    }
}
unsafe extern "C" fn ggml_compute_forward_rms_norm_back(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_rms_norm_back_f32(params, src0, src1, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    9382 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_mul_mat_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    let mut t0: int64_t = 0 as libc::c_int as int64_t;
    let ne00: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne01: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne02: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let ne03: int64_t = (*src0).ne[3 as libc::c_int as usize];
    let ne11: int64_t = (*src1).ne[1 as libc::c_int as usize];
    let nb01: libc::c_int = (*src0).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb02: libc::c_int = (*src0).nb[2 as libc::c_int as usize] as libc::c_int;
    let nb03: libc::c_int = (*src0).nb[3 as libc::c_int as usize] as libc::c_int;
    let nb11: libc::c_int = (*src1).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb12: libc::c_int = (*src1).nb[2 as libc::c_int as usize] as libc::c_int;
    let nb13: libc::c_int = (*src1).nb[3 as libc::c_int as usize] as libc::c_int;
    let nb0: libc::c_int = (*dst).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb1: libc::c_int = (*dst).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb2: libc::c_int = (*dst).nb[2 as libc::c_int as usize] as libc::c_int;
    let nb3: libc::c_int = (*dst).nb[3 as libc::c_int as usize] as libc::c_int;
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint {
        return;
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint {
        return;
    }
    let nr: libc::c_int = (ne01 * ne02 * ne03) as libc::c_int;
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut ir: libc::c_int = ir0;
    while ir < ir1 {
        let i03: libc::c_int = (ir as libc::c_long / (ne02 * ne01)) as libc::c_int;
        let i02: libc::c_int =
            ((ir as libc::c_long - i03 as libc::c_long * ne02 * ne01) / ne01) as libc::c_int;
        let i01: libc::c_int = (ir as libc::c_long
            - i03 as libc::c_long * ne02 * ne01
            - i02 as libc::c_long * ne01) as libc::c_int;
        let mut ic: int64_t = 0 as libc::c_int as int64_t;
        while ic < ne11 {
            let i13: libc::c_int = i03;
            let i12: libc::c_int = i02;
            let i11: libc::c_int = ic as libc::c_int;
            let i0: libc::c_int = i01;
            let i1: libc::c_int = i11;
            let i2: libc::c_int = i02;
            let i3: libc::c_int = i03;
            ggml_vec_dot_f32(
                ne00 as libc::c_int,
                ((*dst).data as *mut libc::c_char)
                    .offset((i0 * nb0 + i1 * nb1 + i2 * nb2 + i3 * nb3) as isize)
                    as *mut libc::c_float,
                ((*src0).data as *mut libc::c_char)
                    .offset((i01 * nb01 + i02 * nb02 + i03 * nb03) as isize)
                    as *mut libc::c_float,
                ((*src1).data as *mut libc::c_char)
                    .offset((i11 * nb11 + i12 * nb12 + i13 * nb13) as isize)
                    as *mut libc::c_float,
            );
            ic += 1;
            ic;
        }
        ir += 1;
        ir;
    }
}
unsafe extern "C" fn ggml_compute_forward_mul_mat_f16_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    let mut t0: int64_t = 0 as libc::c_int as int64_t;
    let ne00: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne01: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne02: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let ne03: int64_t = (*src0).ne[3 as libc::c_int as usize];
    let ne10: int64_t = (*src1).ne[0 as libc::c_int as usize];
    let ne11: int64_t = (*src1).ne[1 as libc::c_int as usize];
    let ne12: int64_t = (*src1).ne[2 as libc::c_int as usize];
    let ne13: int64_t = (*src1).ne[3 as libc::c_int as usize];
    let ne0: int64_t = (*dst).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*dst).ne[1 as libc::c_int as usize];
    let ne2: int64_t = (*dst).ne[2 as libc::c_int as usize];
    let ne3: int64_t = (*dst).ne[3 as libc::c_int as usize];
    let nb00: libc::c_int = (*src0).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb01: libc::c_int = (*src0).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb02: libc::c_int = (*src0).nb[2 as libc::c_int as usize] as libc::c_int;
    let nb03: libc::c_int = (*src0).nb[3 as libc::c_int as usize] as libc::c_int;
    let nb10: libc::c_int = (*src1).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb11: libc::c_int = (*src1).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb12: libc::c_int = (*src1).nb[2 as libc::c_int as usize] as libc::c_int;
    let nb13: libc::c_int = (*src1).nb[3 as libc::c_int as usize] as libc::c_int;
    let nb0: libc::c_int = (*dst).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb1: libc::c_int = (*dst).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb2: libc::c_int = (*dst).nb[2 as libc::c_int as usize] as libc::c_int;
    let nb3: libc::c_int = (*dst).nb[3 as libc::c_int as usize] as libc::c_int;
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    if !(ne02 == ne12) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9638 as libc::c_int,
            b"ne02 == ne12\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne03 == ne13) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9639 as libc::c_int,
            b"ne03 == ne13\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne2 == ne12) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9640 as libc::c_int,
            b"ne2 == ne12\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne3 == ne13) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9641 as libc::c_int,
            b"ne3 == ne13\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb00 as libc::c_ulong == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9644 as libc::c_int,
            b"nb00 == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb0 as libc::c_ulong == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9647 as libc::c_int,
            b"nb0 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb0 <= nb1) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9648 as libc::c_int,
            b"nb0 <= nb1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb1 <= nb2) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9649 as libc::c_int,
            b"nb1 <= nb2\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb2 <= nb3) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9650 as libc::c_int,
            b"nb2 <= nb3\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne0 == ne01) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9652 as libc::c_int,
            b"ne0 == ne01\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne1 == ne11) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9653 as libc::c_int,
            b"ne1 == ne11\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne2 == ne02) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9654 as libc::c_int,
            b"ne2 == ne02\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne3 == ne03) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9655 as libc::c_int,
            b"ne3 == ne03\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint {
        let wdata: *mut ggml_fp16_t = (*params).wdata as *mut ggml_fp16_t;
        let mut id: size_t = 0 as libc::c_int as size_t;
        let mut i13: int64_t = 0 as libc::c_int as int64_t;
        while i13 < ne13 {
            let mut i12: int64_t = 0 as libc::c_int as int64_t;
            while i12 < ne12 {
                let mut i11: int64_t = 0 as libc::c_int as int64_t;
                while i11 < ne11 {
                    let mut i10: int64_t = 0 as libc::c_int as int64_t;
                    while i10 < ne10 {
                        let fresh4 = id;
                        id = id.wrapping_add(1);
                        *wdata.offset(fresh4 as isize) = ({
                            ::core::mem::transmute::<
                                _,
                                [libc::c_short;
                                    ::core::mem::size_of::<__v8hi>()
                                        / ::core::mem::size_of::<libc::c_short>()],
                            >(_mm_cvtps_ph(
                                _mm_setr_ps(
                                    *(((*src1).data as *mut libc::c_char)
                                        .offset((i13 * nb13 as libc::c_long) as isize)
                                        .offset((i12 * nb12 as libc::c_long) as isize)
                                        .offset((i11 * nb11 as libc::c_long) as isize)
                                        .offset((i10 * nb10 as libc::c_long) as isize)
                                        as *mut libc::c_float),
                                    0 as libc::c_int as libc::c_float,
                                    0 as libc::c_int as libc::c_float,
                                    0 as libc::c_int as libc::c_float,
                                ),
                                0 as libc::c_int,
                            ))[0 as libc::c_int as usize]
                                as libc::c_ushort
                        });
                        i10 += 1;
                        i10;
                    }
                    i11 += 1;
                    i11;
                }
                i12 += 1;
                i12;
            }
            i13 += 1;
            i13;
        }
        if !(id.wrapping_mul(::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong)
            <= (*params).wsize)
        {
            fprintf(
                stderr,
                b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                b"ggml.c\0" as *const u8 as *const libc::c_char,
                9740 as libc::c_int,
                b"id*sizeof(ggml_fp16_t) <= params->wsize\0" as *const u8 as *const libc::c_char,
            );
            abort();
        }
        return;
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint {
        return;
    }
    let nr: libc::c_int = (ne01 * ne02 * ne03) as libc::c_int;
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut wdata_0: *mut ggml_fp16_t = (*params).wdata as *mut ggml_fp16_t;
    let mut ir: libc::c_int = ir0;
    while ir < ir1 {
        let i03: libc::c_int = (ir as libc::c_long / (ne02 * ne01)) as libc::c_int;
        let i02: libc::c_int =
            ((ir as libc::c_long - i03 as libc::c_long * ne02 * ne01) / ne01) as libc::c_int;
        let i01: libc::c_int = (ir as libc::c_long
            - i03 as libc::c_long * ne02 * ne01
            - i02 as libc::c_long * ne01) as libc::c_int;
        let i13_0: libc::c_int = i03;
        let i12_0: libc::c_int = i02;
        let i0: libc::c_int = i01;
        let i2: libc::c_int = i02;
        let i3: libc::c_int = i03;
        let mut src0_row: *mut ggml_fp16_t = ((*src0).data as *mut libc::c_char)
            .offset((i01 * nb01 + i02 * nb02 + i03 * nb03) as isize)
            as *mut ggml_fp16_t;
        let mut src1_col: *mut ggml_fp16_t = wdata_0.offset(
            ((0 as libc::c_int as libc::c_long
                + i12_0 as libc::c_long * ne11
                + i13_0 as libc::c_long * ne12 * ne11)
                * ne00) as isize,
        );
        let mut dst_col: *mut libc::c_float = ((*dst).data as *mut libc::c_char)
            .offset((i0 * nb0 + 0 as libc::c_int * nb1 + i2 * nb2 + i3 * nb3) as isize)
            as *mut libc::c_float;
        let mut ic: int64_t = 0 as libc::c_int as int64_t;
        while ic < ne11 {
            ggml_vec_dot_f16(
                ne00 as libc::c_int,
                &mut *dst_col.offset((ic * ne0) as isize),
                src0_row,
                src1_col.offset((ic * ne00) as isize),
            );
            ic += 1;
            ic;
        }
        ir += 1;
        ir;
    }
}
unsafe extern "C" fn ggml_compute_forward_mul_mat_q_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    let mut t0: int64_t = 0 as libc::c_int as int64_t;
    let ne00: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne01: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne02: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let ne03: int64_t = (*src0).ne[3 as libc::c_int as usize];
    let ne10: int64_t = (*src1).ne[0 as libc::c_int as usize];
    let ne11: int64_t = (*src1).ne[1 as libc::c_int as usize];
    let ne12: int64_t = (*src1).ne[2 as libc::c_int as usize];
    let ne13: int64_t = (*src1).ne[3 as libc::c_int as usize];
    let ne0: int64_t = (*dst).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*dst).ne[1 as libc::c_int as usize];
    let ne2: int64_t = (*dst).ne[2 as libc::c_int as usize];
    let ne3: int64_t = (*dst).ne[3 as libc::c_int as usize];
    let nb00: libc::c_int = (*src0).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb01: libc::c_int = (*src0).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb02: libc::c_int = (*src0).nb[2 as libc::c_int as usize] as libc::c_int;
    let nb03: libc::c_int = (*src0).nb[3 as libc::c_int as usize] as libc::c_int;
    let nb10: libc::c_int = (*src1).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb11: libc::c_int = (*src1).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb12: libc::c_int = (*src1).nb[2 as libc::c_int as usize] as libc::c_int;
    let nb13: libc::c_int = (*src1).nb[3 as libc::c_int as usize] as libc::c_int;
    let nb0: libc::c_int = (*dst).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb1: libc::c_int = (*dst).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb2: libc::c_int = (*dst).nb[2 as libc::c_int as usize] as libc::c_int;
    let nb3: libc::c_int = (*dst).nb[3 as libc::c_int as usize] as libc::c_int;
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    if !(ne02 == ne12) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9844 as libc::c_int,
            b"ne02 == ne12\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne03 == ne13) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9845 as libc::c_int,
            b"ne03 == ne13\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne2 == ne12) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9846 as libc::c_int,
            b"ne2 == ne12\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne3 == ne13) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9847 as libc::c_int,
            b"ne3 == ne13\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let type_0: ggml_type = (*src0).type_0;
    let quantize_row_q_dot: quantize_row_q_t = quantize_fns[type_0 as usize].quantize_row_q_dot;
    let vec_dot_q: vec_dot_q_t = quantize_fns[type_0 as usize].vec_dot_q;
    let vec_dot_type: ggml_type = quantize_fns[type_0 as usize].vec_dot_type;
    if !(nb00 == GGML_TYPE_SIZE[type_0 as usize] as libc::c_int) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9855 as libc::c_int,
            b"nb00 == (int) GGML_TYPE_SIZE[type]\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb10 as libc::c_ulong == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9856 as libc::c_int,
            b"nb10 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb0 as libc::c_ulong == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9859 as libc::c_int,
            b"nb0 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb0 <= nb1) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9860 as libc::c_int,
            b"nb0 <= nb1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb1 <= nb2) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9861 as libc::c_int,
            b"nb1 <= nb2\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb2 <= nb3) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9862 as libc::c_int,
            b"nb2 <= nb3\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne0 == ne01) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9864 as libc::c_int,
            b"ne0 == ne01\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne1 == ne11) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9865 as libc::c_int,
            b"ne1 == ne11\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne2 == ne02) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9866 as libc::c_int,
            b"ne2 == ne02\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne3 == ne03) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            9867 as libc::c_int,
            b"ne3 == ne03\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint {
        let mut wdata: *mut libc::c_char = (*params).wdata as *mut libc::c_char;
        let row_size: size_t = (ne10 as libc::c_ulong)
            .wrapping_mul(GGML_TYPE_SIZE[vec_dot_type as usize])
            .wrapping_div(GGML_BLCK_SIZE[vec_dot_type as usize] as libc::c_ulong);
        let mut i13: int64_t = 0 as libc::c_int as int64_t;
        while i13 < ne13 {
            let mut i12: int64_t = 0 as libc::c_int as int64_t;
            while i12 < ne12 {
                let mut i11: int64_t = 0 as libc::c_int as int64_t;
                while i11 < ne11 {
                    quantize_row_q_dot.expect("non-null function pointer")(
                        ((*src1).data as *mut libc::c_char)
                            .offset((i13 * nb13 as libc::c_long) as isize)
                            .offset((i12 * nb12 as libc::c_long) as isize)
                            .offset((i11 * nb11 as libc::c_long) as isize)
                            as *mut libc::c_float,
                        wdata as *mut libc::c_void,
                        ne10 as libc::c_int,
                    );
                    wdata = wdata.offset(row_size as isize);
                    i11 += 1;
                    i11;
                }
                i12 += 1;
                i12;
            }
            i13 += 1;
            i13;
        }
        return;
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint {
        return;
    }
    let nr: libc::c_int = (ne01 * ne02 * ne03) as libc::c_int;
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut wdata_0: *mut libc::c_void = (*params).wdata;
    let row_size_0: size_t = (ne00 as libc::c_ulong)
        .wrapping_mul(GGML_TYPE_SIZE[vec_dot_type as usize])
        .wrapping_div(GGML_BLCK_SIZE[vec_dot_type as usize] as libc::c_ulong);
    let mut ir: libc::c_int = ir0;
    while ir < ir1 {
        let i03: libc::c_int = (ir as libc::c_long / (ne02 * ne01)) as libc::c_int;
        let i02: libc::c_int =
            ((ir as libc::c_long - i03 as libc::c_long * ne02 * ne01) / ne01) as libc::c_int;
        let i01: libc::c_int = (ir as libc::c_long
            - i03 as libc::c_long * ne02 * ne01
            - i02 as libc::c_long * ne01) as libc::c_int;
        let i13_0: libc::c_int = i03;
        let i12_0: libc::c_int = i02;
        let i0: libc::c_int = i01;
        let i2: libc::c_int = i02;
        let i3: libc::c_int = i03;
        let mut src0_row: *mut libc::c_void = ((*src0).data as *mut libc::c_char)
            .offset((i01 * nb01 + i02 * nb02 + i03 * nb03) as isize)
            as *mut libc::c_void;
        let mut src1_col: *mut libc::c_char = (wdata_0 as *mut libc::c_char).offset(
            ((0 as libc::c_int as libc::c_long
                + i12_0 as libc::c_long * ne11
                + i13_0 as libc::c_long * ne12 * ne11) as libc::c_ulong)
                .wrapping_mul(row_size_0) as isize,
        );
        let mut dst_col: *mut libc::c_float = ((*dst).data as *mut libc::c_char)
            .offset((i0 * nb0 + 0 as libc::c_int * nb1 + i2 * nb2 + i3 * nb3) as isize)
            as *mut libc::c_float;
        let mut ic: int64_t = 0 as libc::c_int as int64_t;
        while ic < ne11 {
            vec_dot_q.expect("non-null function pointer")(
                ne00 as libc::c_int,
                &mut *dst_col.offset((ic * ne0) as isize),
                src0_row,
                src1_col.offset((ic as libc::c_ulong).wrapping_mul(row_size_0) as isize)
                    as *mut libc::c_void,
            );
            ic += 1;
            ic;
        }
        ir += 1;
        ir;
    }
}
unsafe extern "C" fn ggml_compute_forward_mul_mat(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        2 | 3 | 6 | 7 | 8 | 9 => {
            ggml_compute_forward_mul_mat_q_f32(params, src0, src1, dst);
        }
        1 => {
            ggml_compute_forward_mul_mat_f16_f32(params, src0, src1, dst);
        }
        0 => {
            ggml_compute_forward_mul_mat_f32(params, src0, src1, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    10035 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_scale_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !ggml_is_contiguous(src0) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10047 as libc::c_int,
            b"ggml_is_contiguous(src0)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_is_contiguous(dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10048 as libc::c_int,
            b"ggml_is_contiguous(dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_are_same_shape(src0, dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10049 as libc::c_int,
            b"ggml_are_same_shape(src0, dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_is_scalar(src1) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10050 as libc::c_int,
            b"ggml_is_scalar(src1)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let v: libc::c_float = *((*src1).data as *mut libc::c_float);
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let nr: libc::c_int = ggml_nrows(src0);
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let mut i1: libc::c_int = ir0;
    while i1 < ir1 {
        if (*dst).data != (*src0).data {
            memcpy(
                ((*dst).data as *mut libc::c_char)
                    .offset((i1 as libc::c_ulong).wrapping_mul(nb1) as isize)
                    as *mut libc::c_void,
                ((*src0).data as *mut libc::c_char)
                    .offset((i1 as libc::c_ulong).wrapping_mul(nb01) as isize)
                    as *const libc::c_void,
                (nc as libc::c_ulong)
                    .wrapping_mul(::core::mem::size_of::<libc::c_float>() as libc::c_ulong),
            );
        }
        ggml_vec_scale_f32(
            nc,
            ((*dst).data as *mut libc::c_char)
                .offset((i1 as libc::c_ulong).wrapping_mul(nb1) as isize)
                as *mut libc::c_float,
            v,
        );
        i1 += 1;
        i1;
    }
}
unsafe extern "C" fn ggml_compute_forward_scale(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_scale_f32(params, src0, src1, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    10098 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_set_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut opt0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !ggml_are_same_shape(src0, dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10111 as libc::c_int,
            b"ggml_are_same_shape(src0, dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ggml_is_contiguous(dst) as libc::c_int != 0
        && ggml_is_contiguous(src0) as libc::c_int != 0)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10112 as libc::c_int,
            b"ggml_is_contiguous(dst) && ggml_is_contiguous(src0)\0" as *const u8
                as *const libc::c_char,
        );
        abort();
    }
    if !((*opt0).type_0 as libc::c_uint == GGML_TYPE_I32 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10114 as libc::c_int,
            b"opt0->type == GGML_TYPE_I32\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ggml_nelements(opt0) == 5 as libc::c_int as libc::c_long) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10115 as libc::c_int,
            b"ggml_nelements(opt0) == 5\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut nb1: size_t =
        *((*opt0).data as *mut int32_t).offset(0 as libc::c_int as isize) as size_t;
    let mut nb2: size_t =
        *((*opt0).data as *mut int32_t).offset(1 as libc::c_int as isize) as size_t;
    let mut nb3: size_t =
        *((*opt0).data as *mut int32_t).offset(2 as libc::c_int as isize) as size_t;
    let mut offset: size_t =
        *((*opt0).data as *mut int32_t).offset(3 as libc::c_int as isize) as size_t;
    let mut inplace: bool = *((*opt0).data as *mut int32_t).offset(4 as libc::c_int as isize) != 0;
    if !inplace && (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
    {
        memcpy(
            (*dst).data as *mut libc::c_char as *mut libc::c_void,
            (*src0).data as *mut libc::c_char as *const libc::c_void,
            ggml_nbytes(dst),
        );
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nr: libc::c_int = ggml_nrows(src1);
    let nc: libc::c_int = (*src1).ne[0 as libc::c_int as usize] as libc::c_int;
    let ne10: int64_t = (*src1).ne[0 as libc::c_int as usize];
    let ne11: int64_t = (*src1).ne[1 as libc::c_int as usize];
    let ne12: int64_t = (*src1).ne[2 as libc::c_int as usize];
    let ne13: int64_t = (*src1).ne[3 as libc::c_int as usize];
    let nb10: size_t = (*src1).nb[0 as libc::c_int as usize];
    let nb11: size_t = (*src1).nb[1 as libc::c_int as usize];
    let nb12: size_t = (*src1).nb[2 as libc::c_int as usize];
    let nb13: size_t = (*src1).nb[3 as libc::c_int as usize];
    let nb0: size_t = ggml_element_size(src0);
    let im0: libc::c_int = (if ne10 == 0 as libc::c_int as libc::c_long {
        0 as libc::c_int as libc::c_long
    } else {
        ne10 - 1 as libc::c_int as libc::c_long
    }) as libc::c_int;
    let im1: libc::c_int = (if ne11 == 0 as libc::c_int as libc::c_long {
        0 as libc::c_int as libc::c_long
    } else {
        ne11 - 1 as libc::c_int as libc::c_long
    }) as libc::c_int;
    let im2: libc::c_int = (if ne12 == 0 as libc::c_int as libc::c_long {
        0 as libc::c_int as libc::c_long
    } else {
        ne12 - 1 as libc::c_int as libc::c_long
    }) as libc::c_int;
    let im3: libc::c_int = (if ne13 == 0 as libc::c_int as libc::c_long {
        0 as libc::c_int as libc::c_long
    } else {
        ne13 - 1 as libc::c_int as libc::c_long
    }) as libc::c_int;
    if !(offset
        .wrapping_add((im0 as libc::c_ulong).wrapping_mul(nb0))
        .wrapping_add((im1 as libc::c_ulong).wrapping_mul(nb1))
        .wrapping_add((im2 as libc::c_ulong).wrapping_mul(nb2))
        .wrapping_add((im3 as libc::c_ulong).wrapping_mul(nb3))
        < ggml_nbytes(dst))
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10162 as libc::c_int,
            b"offset + im0*nb0 + im1*nb1 + im2*nb2 + im3*nb3 < ggml_nbytes(dst)\0" as *const u8
                as *const libc::c_char,
        );
        abort();
    }
    if !(nb10 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10164 as libc::c_int,
            b"nb10 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut ir: libc::c_int = ir0;
    while ir < ir1 {
        let i3: libc::c_int = (ir as libc::c_long / (ne12 * ne11)) as libc::c_int;
        let i2: libc::c_int =
            ((ir as libc::c_long - i3 as libc::c_long * ne12 * ne11) / ne11) as libc::c_int;
        let i1: libc::c_int = (ir as libc::c_long
            - i3 as libc::c_long * ne12 * ne11
            - i2 as libc::c_long * ne11) as libc::c_int;
        ggml_vec_cpy_f32(
            nc,
            ((*dst).data as *mut libc::c_char)
                .offset((i3 as libc::c_ulong).wrapping_mul(nb3) as isize)
                .offset((i2 as libc::c_ulong).wrapping_mul(nb2) as isize)
                .offset((i1 as libc::c_ulong).wrapping_mul(nb1) as isize)
                .offset(offset as isize) as *mut libc::c_float,
            ((*src1).data as *mut libc::c_char)
                .offset((i3 as libc::c_ulong).wrapping_mul(nb13) as isize)
                .offset((i2 as libc::c_ulong).wrapping_mul(nb12) as isize)
                .offset((i1 as libc::c_ulong).wrapping_mul(nb11) as isize)
                as *mut libc::c_float,
        );
        ir += 1;
        ir;
    }
}
unsafe extern "C" fn ggml_compute_forward_set(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut opt0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_set_f32(params, src0, src1, opt0, dst);
        }
        1 | 2 | 3 | 6 | 7 | 8 | 9 | _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    10207 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_cpy(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    ggml_compute_forward_dup(params, src0, dst);
}
unsafe extern "C" fn ggml_compute_forward_cont(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    ggml_compute_forward_dup(params, src0, dst);
}
unsafe extern "C" fn ggml_compute_forward_reshape(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
}
unsafe extern "C" fn ggml_compute_forward_view(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
) {
}
unsafe extern "C" fn ggml_compute_forward_permute(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
) {
}
unsafe extern "C" fn ggml_compute_forward_transpose(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
) {
}
unsafe extern "C" fn ggml_compute_forward_get_rows_q(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let nr: libc::c_int = ggml_nelements(src1) as libc::c_int;
    let type_0: ggml_type = (*src0).type_0;
    let dequantize_row_q: dequantize_row_q_t = quantize_fns[type_0 as usize].dequantize_row_q;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nr {
        let r: libc::c_int = *((*src1).data as *mut int32_t).offset(i as isize);
        dequantize_row_q.expect("non-null function pointer")(
            ((*src0).data as *mut libc::c_char).offset(
                (r as libc::c_ulong).wrapping_mul((*src0).nb[1 as libc::c_int as usize]) as isize,
            ) as *const libc::c_void,
            ((*dst).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*dst).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
            nc,
        );
        i += 1;
        i;
    }
}
unsafe extern "C" fn ggml_compute_forward_get_rows_f16(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let nr: libc::c_int = ggml_nelements(src1) as libc::c_int;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nr {
        let r: libc::c_int = *((*src1).data as *mut int32_t).offset(i as isize);
        let mut j: libc::c_int = 0 as libc::c_int;
        while j < nc {
            let mut v: ggml_fp16_t = *(((*src0).data as *mut libc::c_char).offset(
                (r as libc::c_ulong).wrapping_mul((*src0).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut ggml_fp16_t)
                .offset(j as isize);
            *(((*dst).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*dst).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float)
                .offset(j as isize) = ggml_lookup_fp16_to_fp32(v);
            j += 1;
            j;
        }
        i += 1;
        i;
    }
}
unsafe extern "C" fn ggml_compute_forward_get_rows_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let nr: libc::c_int = ggml_nelements(src1) as libc::c_int;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nr {
        let r: libc::c_int = *((*src1).data as *mut int32_t).offset(i as isize);
        ggml_vec_cpy_f32(
            nc,
            ((*dst).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*dst).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
            ((*src0).data as *mut libc::c_char).offset(
                (r as libc::c_ulong).wrapping_mul((*src0).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
        );
        i += 1;
        i;
    }
}
unsafe extern "C" fn ggml_compute_forward_get_rows(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        2 | 3 | 6 | 7 | 8 | 9 => {
            ggml_compute_forward_get_rows_q(params, src0, src1, dst);
        }
        1 => {
            ggml_compute_forward_get_rows_f16(params, src0, src1, dst);
        }
        0 => {
            ggml_compute_forward_get_rows_f32(params, src0, src1, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    10383 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_get_rows_back_f32_f16(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut opt0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !((*params).ith == 0 as libc::c_int) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10414 as libc::c_int,
            b"params->ith == 0\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_are_same_shape(opt0, dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10415 as libc::c_int,
            b"ggml_are_same_shape(opt0, dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_is_contiguous(opt0) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10416 as libc::c_int,
            b"ggml_is_contiguous(opt0)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_is_contiguous(dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10417 as libc::c_int,
            b"ggml_is_contiguous(dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    ggml_compute_forward_dup_same_cont(params, opt0, dst);
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let nr: libc::c_int = ggml_nelements(src1) as libc::c_int;
    if !((*dst).ne[0 as libc::c_int as usize] == nc as libc::c_long) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10428 as libc::c_int,
            b"dst->ne[0] == nc\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*src0).nb[0 as libc::c_int as usize]
        == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10429 as libc::c_int,
            b"src0->nb[0] == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nr {
        let r: libc::c_int = *((*src1).data as *mut int32_t).offset(i as isize);
        let mut j: libc::c_int = 0 as libc::c_int;
        while j < nc {
            let mut v: ggml_fp16_t = *(((*src0).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*src0).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut ggml_fp16_t)
                .offset(j as isize);
            *(((*dst).data as *mut libc::c_char).offset(
                (r as libc::c_ulong).wrapping_mul((*dst).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float)
                .offset(j as isize) += ggml_lookup_fp16_to_fp32(v);
            j += 1;
            j;
        }
        i += 1;
        i;
    }
}
unsafe extern "C" fn ggml_compute_forward_get_rows_back_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut opt0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !((*params).ith == 0 as libc::c_int) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10447 as libc::c_int,
            b"params->ith == 0\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_are_same_shape(opt0, dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10448 as libc::c_int,
            b"ggml_are_same_shape(opt0, dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_is_contiguous(opt0) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10449 as libc::c_int,
            b"ggml_is_contiguous(opt0)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_is_contiguous(dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10450 as libc::c_int,
            b"ggml_is_contiguous(dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    ggml_compute_forward_dup_same_cont(params, opt0, dst);
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let nr: libc::c_int = ggml_nelements(src1) as libc::c_int;
    if !((*dst).ne[0 as libc::c_int as usize] == nc as libc::c_long) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10461 as libc::c_int,
            b"dst->ne[0] == nc\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*src0).nb[0 as libc::c_int as usize]
        == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong)
    {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10462 as libc::c_int,
            b"src0->nb[0] == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < nr {
        let r: libc::c_int = *((*src1).data as *mut int32_t).offset(i as isize);
        ggml_vec_add_f32(
            nc,
            ((*dst).data as *mut libc::c_char).offset(
                (r as libc::c_ulong).wrapping_mul((*dst).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
            ((*dst).data as *mut libc::c_char).offset(
                (r as libc::c_ulong).wrapping_mul((*dst).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
            ((*src0).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*src0).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
        );
        i += 1;
        i;
    }
}
unsafe extern "C" fn ggml_compute_forward_get_rows_back(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut opt0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        1 => {
            ggml_compute_forward_get_rows_back_f32_f16(params, src0, src1, opt0, dst);
        }
        0 => {
            ggml_compute_forward_get_rows_back_f32(params, src0, src1, opt0, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    10492 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_diag_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !((*params).ith == 0 as libc::c_int) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10521 as libc::c_int,
            b"params->ith == 0\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let ne00: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let ne01: libc::c_int = (*src0).ne[1 as libc::c_int as usize] as libc::c_int;
    let ne02: libc::c_int = (*src0).ne[2 as libc::c_int as usize] as libc::c_int;
    let ne03: libc::c_int = (*src0).ne[3 as libc::c_int as usize] as libc::c_int;
    let ne0: libc::c_int = (*dst).ne[0 as libc::c_int as usize] as libc::c_int;
    let ne1: libc::c_int = (*dst).ne[1 as libc::c_int as usize] as libc::c_int;
    let ne2: libc::c_int = (*dst).ne[2 as libc::c_int as usize] as libc::c_int;
    let ne3: libc::c_int = (*dst).ne[3 as libc::c_int as usize] as libc::c_int;
    if !(ne00 == ne0) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10537 as libc::c_int,
            b"ne00 == ne0\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne00 == ne1) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10538 as libc::c_int,
            b"ne00 == ne1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne01 == 1 as libc::c_int) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10539 as libc::c_int,
            b"ne01 == 1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne02 == ne2) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10540 as libc::c_int,
            b"ne02 == ne2\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne03 == ne3) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10541 as libc::c_int,
            b"ne03 == ne3\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let nb00: libc::c_int = (*src0).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb02: libc::c_int = (*src0).nb[2 as libc::c_int as usize] as libc::c_int;
    let nb03: libc::c_int = (*src0).nb[3 as libc::c_int as usize] as libc::c_int;
    let nb0: libc::c_int = (*dst).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb1: libc::c_int = (*dst).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb2: libc::c_int = (*dst).nb[2 as libc::c_int as usize] as libc::c_int;
    let nb3: libc::c_int = (*dst).nb[3 as libc::c_int as usize] as libc::c_int;
    if !(nb00 as libc::c_ulong == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10552 as libc::c_int,
            b"nb00 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb0 as libc::c_ulong == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10553 as libc::c_int,
            b"nb0 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut i3: libc::c_int = 0 as libc::c_int;
    while i3 < ne3 {
        let mut i2: libc::c_int = 0 as libc::c_int;
        while i2 < ne2 {
            let mut i1: libc::c_int = 0 as libc::c_int;
            while i1 < ne1 {
                let mut d: *mut libc::c_float = ((*dst).data as *mut libc::c_char)
                    .offset((i3 * nb3) as isize)
                    .offset((i2 * nb2) as isize)
                    .offset((i1 * nb1) as isize)
                    as *mut libc::c_float;
                let mut s: *mut libc::c_float = ((*src0).data as *mut libc::c_char)
                    .offset((i3 * nb03) as isize)
                    .offset((i2 * nb02) as isize)
                    as *mut libc::c_float;
                let mut i0: libc::c_int = 0 as libc::c_int;
                while i0 < i1 {
                    *d.offset(i0 as isize) = 0 as libc::c_int as libc::c_float;
                    i0 += 1;
                    i0;
                }
                *d.offset(i1 as isize) = *s.offset(i1 as isize);
                let mut i0_0: libc::c_int = i1 + 1 as libc::c_int;
                while i0_0 < ne0 {
                    *d.offset(i0_0 as isize) = 0 as libc::c_int as libc::c_float;
                    i0_0 += 1;
                    i0_0;
                }
                i1 += 1;
                i1;
            }
            i2 += 1;
            i2;
        }
        i3 += 1;
        i3;
    }
}
unsafe extern "C" fn ggml_compute_forward_diag(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_diag_f32(params, src0, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    10583 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_diag_mask_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
    value: libc::c_float,
) {
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let n_past: libc::c_int = *((*src1).data as *mut int32_t).offset(0 as libc::c_int as isize);
    let inplace: bool = *((*src1).data as *mut int32_t).offset(1 as libc::c_int as isize) != 0;
    if !inplace && (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
    {
        if !(ggml_nelements(dst) == ggml_nelements(src0)) {
            fprintf(
                stderr,
                b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                b"ggml.c\0" as *const u8 as *const libc::c_char,
                10610 as libc::c_int,
                b"ggml_nelements(dst) == ggml_nelements(src0)\0" as *const u8
                    as *const libc::c_char,
            );
            abort();
        }
        if !(ggml_is_contiguous(dst) as libc::c_int != 0
            && ggml_is_contiguous(src0) as libc::c_int != 0)
        {
            fprintf(
                stderr,
                b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                b"ggml.c\0" as *const u8 as *const libc::c_char,
                10611 as libc::c_int,
                b"ggml_is_contiguous(dst) && ggml_is_contiguous(src0)\0" as *const u8
                    as *const libc::c_char,
            );
            abort();
        }
        memcpy(
            (*dst).data as *mut libc::c_char as *mut libc::c_void,
            (*src0).data as *mut libc::c_char as *const libc::c_void,
            ggml_nbytes(dst),
        );
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let n: libc::c_int = ggml_nrows(src0);
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let nr: libc::c_int = (*src0).ne[1 as libc::c_int as usize] as libc::c_int;
    let nz: libc::c_int = n / nr;
    let mut k: libc::c_int = 0 as libc::c_int;
    while k < nz {
        let mut j: libc::c_int = ith;
        while j < nr {
            let mut i: libc::c_int = n_past;
            while i < nc {
                if i > n_past + j {
                    *(((*dst).data as *mut libc::c_char)
                        .offset(
                            (k as libc::c_ulong).wrapping_mul((*dst).nb[2 as libc::c_int as usize])
                                as isize,
                        )
                        .offset(
                            (j as libc::c_ulong).wrapping_mul((*dst).nb[1 as libc::c_int as usize])
                                as isize,
                        )
                        .offset(
                            (i as libc::c_ulong).wrapping_mul((*dst).nb[0 as libc::c_int as usize])
                                as isize,
                        ) as *mut libc::c_float) = value;
                }
                i += 1;
                i;
            }
            j += nth;
        }
        k += 1;
        k;
    }
}
unsafe extern "C" fn ggml_compute_forward_diag_mask_inf(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_diag_mask_f32(params, src0, src1, dst, -::core::f32::INFINITY);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    10655 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_diag_mask_zero(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_diag_mask_f32(
                params,
                src0,
                src1,
                dst,
                0 as libc::c_int as libc::c_float,
            );
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    10672 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_soft_max_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !ggml_is_contiguous(src0) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10683 as libc::c_int,
            b"ggml_is_contiguous(src0)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_is_contiguous(dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10684 as libc::c_int,
            b"ggml_is_contiguous(dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !ggml_are_same_shape(src0, dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10685 as libc::c_int,
            b"ggml_are_same_shape(src0, dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let nr: libc::c_int = ggml_nrows(src0);
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut i1: libc::c_int = ir0;
    while i1 < ir1 {
        let mut sp: *mut libc::c_float = ((*src0).data as *mut libc::c_char).offset(
            (i1 as libc::c_ulong).wrapping_mul((*src0).nb[1 as libc::c_int as usize]) as isize,
        ) as *mut libc::c_float;
        let mut dp: *mut libc::c_float = ((*dst).data as *mut libc::c_char).offset(
            (i1 as libc::c_ulong).wrapping_mul((*dst).nb[1 as libc::c_int as usize]) as isize,
        ) as *mut libc::c_float;
        let mut max: libc::c_float = -::core::f32::INFINITY;
        ggml_vec_max_f32(nc, &mut max, sp);
        let mut sum: ggml_float = 0.0f64;
        let mut scvt: uint16_t = 0;
        let mut i: libc::c_int = 0 as libc::c_int;
        while i < nc {
            if *sp.offset(i as isize) == -::core::f32::INFINITY {
                *dp.offset(i as isize) = 0.0f32;
            } else {
                let mut s: ggml_fp16_t = ({
                    ::core::mem::transmute::<
                        _,
                        [libc::c_short;
                            ::core::mem::size_of::<__v8hi>()
                                / ::core::mem::size_of::<libc::c_short>()],
                    >(_mm_cvtps_ph(
                        _mm_setr_ps(
                            *sp.offset(i as isize) - max,
                            0 as libc::c_int as libc::c_float,
                            0 as libc::c_int as libc::c_float,
                            0 as libc::c_int as libc::c_float,
                        ),
                        0 as libc::c_int,
                    ))[0 as libc::c_int as usize] as libc::c_ushort
                });
                memcpy(
                    &mut scvt as *mut uint16_t as *mut libc::c_void,
                    &mut s as *mut ggml_fp16_t as *const libc::c_void,
                    ::core::mem::size_of::<uint16_t>() as libc::c_ulong,
                );
                let val: libc::c_float = ggml_lookup_fp16_to_fp32(table_exp_f16[scvt as usize]);
                sum += val as ggml_float;
                *dp.offset(i as isize) = val;
            }
            i += 1;
            i;
        }
        sum = 1.0f64 / sum;
        ggml_vec_scale_f32(nc, dp, sum as libc::c_float);
        i1 += 1;
        i1;
    }
}
unsafe extern "C" fn ggml_compute_forward_soft_max(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_soft_max_f32(params, src0, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    10761 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_alibi_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let n_past: libc::c_int = *((*src1).data as *mut int32_t).offset(0 as libc::c_int as isize);
    let n_head: libc::c_int = *((*src1).data as *mut int32_t).offset(1 as libc::c_int as isize);
    let max_bias: libc::c_float =
        *((*src1).data as *mut libc::c_float).offset(2 as libc::c_int as isize);
    let ne0: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let ne1: libc::c_int = (*src0).ne[1 as libc::c_int as usize] as libc::c_int;
    let n: libc::c_int = ggml_nrows(src0);
    let ne2_ne3: libc::c_int = n / ne1;
    let nb0: libc::c_int = (*src0).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb1: libc::c_int = (*src0).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb2: libc::c_int = (*src0).nb[2 as libc::c_int as usize] as libc::c_int;
    let n_heads_log2_floor: libc::c_int =
        (1 as libc::c_int) << floor(log2(n_head as libc::c_double)) as libc::c_int;
    let m0: libc::c_float = powf(2.0f32, -max_bias / n_heads_log2_floor as libc::c_float);
    let m1: libc::c_float = powf(
        2.0f32,
        -(max_bias / 2.0f32) / n_heads_log2_floor as libc::c_float,
    );
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < ne0 {
        let mut j: libc::c_int = 0 as libc::c_int;
        while j < ne1 {
            let mut k: libc::c_int = 0 as libc::c_int;
            while k < ne2_ne3 {
                let src: *mut libc::c_float = ((*src0).data as *mut libc::c_char)
                    .offset((i * nb0) as isize)
                    .offset((j * nb1) as isize)
                    .offset((k * nb2) as isize)
                    as *mut libc::c_float;
                let mut pdst: *mut libc::c_float = ((*dst).data as *mut libc::c_char)
                    .offset((i * nb0) as isize)
                    .offset((j * nb1) as isize)
                    .offset((k * nb2) as isize)
                    as *mut libc::c_float;
                let mut m_k: libc::c_float = 0.;
                if k < n_heads_log2_floor {
                    m_k = powf(m0, (k + 1 as libc::c_int) as libc::c_float);
                } else {
                    m_k = powf(
                        m1,
                        (2 as libc::c_int * (k - n_heads_log2_floor) + 1 as libc::c_int)
                            as libc::c_float,
                    );
                }
                *pdst.offset(0 as libc::c_int as isize) =
                    (i - ne0 + 1 as libc::c_int) as libc::c_float * m_k
                        + *src.offset(0 as libc::c_int as isize);
                k += 1;
                k;
            }
            j += 1;
            j;
        }
        i += 1;
        i;
    }
}
unsafe extern "C" fn ggml_compute_forward_alibi_f16(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let n_past: libc::c_int = *((*src1).data as *mut int32_t).offset(0 as libc::c_int as isize);
    let n_head: libc::c_int = *((*src1).data as *mut int32_t).offset(1 as libc::c_int as isize);
    let max_bias: libc::c_float =
        *((*src1).data as *mut libc::c_float).offset(2 as libc::c_int as isize);
    let ne0: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let ne1: libc::c_int = (*src0).ne[1 as libc::c_int as usize] as libc::c_int;
    let n: libc::c_int = ggml_nrows(src0);
    let ne2_ne3: libc::c_int = n / ne1;
    let nb0: libc::c_int = (*src0).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb1: libc::c_int = (*src0).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb2: libc::c_int = (*src0).nb[2 as libc::c_int as usize] as libc::c_int;
    let n_heads_log2_floor: libc::c_int =
        (1 as libc::c_int) << floor(log2(n_head as libc::c_double)) as libc::c_int;
    let m0: libc::c_float = powf(2.0f32, -max_bias / n_heads_log2_floor as libc::c_float);
    let m1: libc::c_float = powf(
        2.0f32,
        -(max_bias / 2.0f32) / n_heads_log2_floor as libc::c_float,
    );
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < ne0 {
        let mut j: libc::c_int = 0 as libc::c_int;
        while j < ne1 {
            let mut k: libc::c_int = 0 as libc::c_int;
            while k < ne2_ne3 {
                let src: *mut ggml_fp16_t = ((*src0).data as *mut libc::c_char)
                    .offset((i * nb0) as isize)
                    .offset((j * nb1) as isize)
                    .offset((k * nb2) as isize)
                    as *mut ggml_fp16_t;
                let mut pdst: *mut libc::c_float = ((*dst).data as *mut libc::c_char)
                    .offset((i * nb0) as isize)
                    .offset((j * nb1) as isize)
                    .offset((k * nb2) as isize)
                    as *mut libc::c_float;
                let mut m_k: libc::c_float = 0.;
                if k < n_heads_log2_floor {
                    m_k = powf(m0, (k + 1 as libc::c_int) as libc::c_float);
                } else {
                    m_k = powf(
                        m1,
                        (2 as libc::c_int * (k - n_heads_log2_floor) + 1 as libc::c_int)
                            as libc::c_float,
                    );
                }
                *pdst.offset(0 as libc::c_int as isize) =
                    (i - ne0 + 1 as libc::c_int) as libc::c_float * m_k
                        + ggml_lookup_fp16_to_fp32(*src.offset(0 as libc::c_int as isize));
                k += 1;
                k;
            }
            j += 1;
            j;
        }
        i += 1;
        i;
    }
}
unsafe extern "C" fn ggml_compute_forward_alibi(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        1 => {
            ggml_compute_forward_alibi_f16(params, src0, src1, dst);
        }
        0 => {
            ggml_compute_forward_alibi_f32(params, src0, src1, dst);
        }
        2 | 3 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    10921 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
        _ => {}
    };
}
unsafe extern "C" fn ggml_compute_forward_clamp_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let min: libc::c_int =
        *((*src1).data as *mut libc::c_float).offset(0 as libc::c_int as isize) as libc::c_int;
    let max: libc::c_int =
        *((*src1).data as *mut libc::c_float).offset(1 as libc::c_int as isize) as libc::c_int;
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let n: libc::c_int = ggml_nrows(src0);
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let nb00: size_t = (*src0).nb[0 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb0: size_t = (*dst).nb[0 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    if !(nb0 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10957 as libc::c_int,
            b"nb0 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb00 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            10958 as libc::c_int,
            b"nb00 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut j: libc::c_int = ith;
    while j < n {
        let mut dst_ptr: *mut libc::c_float = ((*dst).data as *mut libc::c_char)
            .offset((j as libc::c_ulong).wrapping_mul(nb1) as isize)
            as *mut libc::c_float;
        let mut src0_ptr: *mut libc::c_float = ((*src0).data as *mut libc::c_char)
            .offset((j as libc::c_ulong).wrapping_mul(nb01) as isize)
            as *mut libc::c_float;
        let mut i: libc::c_int = 0 as libc::c_int;
        while i < nc {
            *dst_ptr.offset(i as isize) = if (if *src0_ptr.offset(i as isize) < max as libc::c_float
            {
                *src0_ptr.offset(i as isize)
            } else {
                max as libc::c_float
            }) > min as libc::c_float
            {
                if *src0_ptr.offset(i as isize) < max as libc::c_float {
                    *src0_ptr.offset(i as isize)
                } else {
                    max as libc::c_float
                }
            } else {
                min as libc::c_float
            };
            i += 1;
            i;
        }
        j += nth;
    }
}
unsafe extern "C" fn ggml_compute_forward_clamp(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_clamp_f32(params, src0, src1, dst);
        }
        1 | 2 | 3 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    10992 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
        _ => {}
    };
}
unsafe extern "C" fn ggml_compute_forward_rope_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !((*src1).type_0 as libc::c_uint == GGML_TYPE_I32 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11004 as libc::c_int,
            b"src1->type == GGML_TYPE_I32\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ggml_nelements(src1) == 3 as libc::c_int as libc::c_long) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11005 as libc::c_int,
            b"ggml_nelements(src1) == 3\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let n_past: libc::c_int = *((*src1).data as *mut int32_t).offset(0 as libc::c_int as isize);
    let n_dims: libc::c_int = *((*src1).data as *mut int32_t).offset(1 as libc::c_int as isize);
    let mode: libc::c_int = *((*src1).data as *mut int32_t).offset(2 as libc::c_int as isize);
    let nb00: size_t = (*src0).nb[0 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let ne0: int64_t = (*dst).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*dst).ne[1 as libc::c_int as usize];
    let ne2: int64_t = (*dst).ne[2 as libc::c_int as usize];
    let ne3: int64_t = (*dst).ne[3 as libc::c_int as usize];
    let nb0: size_t = (*dst).nb[0 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    if !(nb00 == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11035 as libc::c_int,
            b"nb00 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nr: libc::c_int = ggml_nrows(dst);
    if !(n_dims as libc::c_long <= ne0) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11042 as libc::c_int,
            b"n_dims <= ne0\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(n_dims % 2 as libc::c_int == 0 as libc::c_int) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11043 as libc::c_int,
            b"n_dims % 2 == 0\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut ir: libc::c_int = 0 as libc::c_int;
    let theta_scale: libc::c_float = powf(
        10000.0f64 as libc::c_float,
        -2.0f32 / n_dims as libc::c_float,
    );
    let is_neox: bool = mode & 2 as libc::c_int != 0;
    let mut i3: int64_t = 0 as libc::c_int as int64_t;
    while i3 < ne3 {
        let mut i2: int64_t = (if mode & 1 as libc::c_int == 0 as libc::c_int {
            0 as libc::c_int
        } else {
            n_past
        }) as int64_t;
        while i2 < ne2 {
            let p: int64_t = if mode & 1 as libc::c_int == 0 as libc::c_int {
                n_past as libc::c_long + i2
            } else {
                i2
            };
            let mut i1: int64_t = 0 as libc::c_int as int64_t;
            while i1 < ne1 {
                let fresh5 = ir;
                ir = ir + 1;
                if !(fresh5 < ir0) {
                    if ir > ir1 {
                        break;
                    }
                    let mut theta: libc::c_float = p as libc::c_float;
                    if !is_neox {
                        let mut i0: int64_t = 0 as libc::c_int as int64_t;
                        while i0 < ne0 {
                            let cos_theta: libc::c_float = cosf(theta);
                            let sin_theta: libc::c_float = sinf(theta);
                            theta *= theta_scale;
                            let src: *const libc::c_float = ((*src0).data as *mut libc::c_char)
                                .offset((i3 as libc::c_ulong).wrapping_mul(nb03) as isize)
                                .offset((i2 as libc::c_ulong).wrapping_mul(nb02) as isize)
                                .offset((i1 as libc::c_ulong).wrapping_mul(nb01) as isize)
                                .offset((i0 as libc::c_ulong).wrapping_mul(nb00) as isize)
                                as *mut libc::c_float;
                            let mut dst_data: *mut libc::c_float = ((*dst).data
                                as *mut libc::c_char)
                                .offset((i3 as libc::c_ulong).wrapping_mul(nb3) as isize)
                                .offset((i2 as libc::c_ulong).wrapping_mul(nb2) as isize)
                                .offset((i1 as libc::c_ulong).wrapping_mul(nb1) as isize)
                                .offset((i0 as libc::c_ulong).wrapping_mul(nb0) as isize)
                                as *mut libc::c_float;
                            let x0: libc::c_float = *src.offset(0 as libc::c_int as isize);
                            let x1: libc::c_float = *src.offset(1 as libc::c_int as isize);
                            *dst_data.offset(0 as libc::c_int as isize) =
                                x0 * cos_theta - x1 * sin_theta;
                            *dst_data.offset(1 as libc::c_int as isize) =
                                x0 * sin_theta + x1 * cos_theta;
                            i0 += 2 as libc::c_int as libc::c_long;
                        }
                    } else {
                        let mut ib: int64_t = 0 as libc::c_int as int64_t;
                        while ib < ne0 / n_dims as libc::c_long {
                            let mut ic: int64_t = 0 as libc::c_int as int64_t;
                            while ic < n_dims as libc::c_long {
                                let cos_theta_0: libc::c_float = cosf(theta);
                                let sin_theta_0: libc::c_float = sinf(theta);
                                theta *= theta_scale;
                                let i0_0: int64_t = ib * n_dims as libc::c_long
                                    + ic / 2 as libc::c_int as libc::c_long;
                                let src_0: *const libc::c_float = ((*src0).data
                                    as *mut libc::c_char)
                                    .offset((i3 as libc::c_ulong).wrapping_mul(nb03) as isize)
                                    .offset((i2 as libc::c_ulong).wrapping_mul(nb02) as isize)
                                    .offset((i1 as libc::c_ulong).wrapping_mul(nb01) as isize)
                                    .offset((i0_0 as libc::c_ulong).wrapping_mul(nb00) as isize)
                                    as *mut libc::c_float;
                                let mut dst_data_0: *mut libc::c_float = ((*dst).data
                                    as *mut libc::c_char)
                                    .offset((i3 as libc::c_ulong).wrapping_mul(nb3) as isize)
                                    .offset((i2 as libc::c_ulong).wrapping_mul(nb2) as isize)
                                    .offset((i1 as libc::c_ulong).wrapping_mul(nb1) as isize)
                                    .offset((i0_0 as libc::c_ulong).wrapping_mul(nb0) as isize)
                                    as *mut libc::c_float;
                                let x0_0: libc::c_float = *src_0.offset(0 as libc::c_int as isize);
                                let x1_0: libc::c_float =
                                    *src_0.offset((n_dims / 2 as libc::c_int) as isize);
                                *dst_data_0.offset(0 as libc::c_int as isize) =
                                    x0_0 * cos_theta_0 - x1_0 * sin_theta_0;
                                *dst_data_0.offset((n_dims / 2 as libc::c_int) as isize) =
                                    x0_0 * sin_theta_0 + x1_0 * cos_theta_0;
                                ic += 2 as libc::c_int as libc::c_long;
                            }
                            ib += 1;
                            ib;
                        }
                    }
                }
                i1 += 1;
                i1;
            }
            i2 += 1;
            i2;
        }
        i3 += 1;
        i3;
    }
}
unsafe extern "C" fn ggml_compute_forward_rope_f16(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !((*src1).type_0 as libc::c_uint == GGML_TYPE_I32 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11117 as libc::c_int,
            b"src1->type == GGML_TYPE_I32\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ggml_nelements(src1) == 3 as libc::c_int as libc::c_long) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11118 as libc::c_int,
            b"ggml_nelements(src1) == 3\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let n_past: libc::c_int = *((*src1).data as *mut int32_t).offset(0 as libc::c_int as isize);
    let n_dims: libc::c_int = *((*src1).data as *mut int32_t).offset(1 as libc::c_int as isize);
    let mode: libc::c_int = *((*src1).data as *mut int32_t).offset(2 as libc::c_int as isize);
    let nb00: size_t = (*src0).nb[0 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let ne0: int64_t = (*dst).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*dst).ne[1 as libc::c_int as usize];
    let ne2: int64_t = (*dst).ne[2 as libc::c_int as usize];
    let ne3: int64_t = (*dst).ne[3 as libc::c_int as usize];
    let nb0: size_t = (*dst).nb[0 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    if !(nb0 == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11148 as libc::c_int,
            b"nb0 == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nr: libc::c_int = ggml_nrows(dst);
    if !(n_dims as libc::c_long <= ne0) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11155 as libc::c_int,
            b"n_dims <= ne0\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(n_dims % 2 as libc::c_int == 0 as libc::c_int) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11156 as libc::c_int,
            b"n_dims % 2 == 0\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut ir: libc::c_int = 0 as libc::c_int;
    let theta_scale: libc::c_float = powf(
        10000.0f64 as libc::c_float,
        -2.0f32 / n_dims as libc::c_float,
    );
    let is_neox: bool = mode & 2 as libc::c_int != 0;
    let mut i3: int64_t = 0 as libc::c_int as int64_t;
    while i3 < ne3 {
        let mut i2: int64_t = (if mode & 1 as libc::c_int == 0 as libc::c_int {
            0 as libc::c_int
        } else {
            n_past
        }) as int64_t;
        while i2 < ne2 {
            let p: int64_t = if mode & 1 as libc::c_int == 0 as libc::c_int {
                n_past as libc::c_long + i2
            } else {
                i2
            };
            let mut i1: int64_t = 0 as libc::c_int as int64_t;
            while i1 < ne1 {
                let fresh6 = ir;
                ir = ir + 1;
                if !(fresh6 < ir0) {
                    if ir > ir1 {
                        break;
                    }
                    let mut theta: libc::c_float = p as libc::c_float;
                    if !is_neox {
                        let mut i0: int64_t = 0 as libc::c_int as int64_t;
                        while i0 < ne0 {
                            let cos_theta: libc::c_float = cosf(theta);
                            let sin_theta: libc::c_float = sinf(theta);
                            theta *= theta_scale;
                            let src: *const ggml_fp16_t = ((*src0).data as *mut libc::c_char)
                                .offset((i3 as libc::c_ulong).wrapping_mul(nb03) as isize)
                                .offset((i2 as libc::c_ulong).wrapping_mul(nb02) as isize)
                                .offset((i1 as libc::c_ulong).wrapping_mul(nb01) as isize)
                                .offset((i0 as libc::c_ulong).wrapping_mul(nb00) as isize)
                                as *mut ggml_fp16_t;
                            let mut dst_data: *mut ggml_fp16_t = ((*dst).data as *mut libc::c_char)
                                .offset((i3 as libc::c_ulong).wrapping_mul(nb3) as isize)
                                .offset((i2 as libc::c_ulong).wrapping_mul(nb2) as isize)
                                .offset((i1 as libc::c_ulong).wrapping_mul(nb1) as isize)
                                .offset((i0 as libc::c_ulong).wrapping_mul(nb0) as isize)
                                as *mut ggml_fp16_t;
                            let x0: libc::c_float =
                                ggml_lookup_fp16_to_fp32(*src.offset(0 as libc::c_int as isize));
                            let x1: libc::c_float =
                                ggml_lookup_fp16_to_fp32(*src.offset(1 as libc::c_int as isize));
                            *dst_data.offset(0 as libc::c_int as isize) = ({
                                ::core::mem::transmute::<
                                    _,
                                    [libc::c_short;
                                        ::core::mem::size_of::<__v8hi>()
                                            / ::core::mem::size_of::<libc::c_short>()],
                                >(_mm_cvtps_ph(
                                    _mm_setr_ps(
                                        x0 * cos_theta - x1 * sin_theta,
                                        0 as libc::c_int as libc::c_float,
                                        0 as libc::c_int as libc::c_float,
                                        0 as libc::c_int as libc::c_float,
                                    ),
                                    0 as libc::c_int,
                                ))[0 as libc::c_int as usize]
                                    as libc::c_ushort
                            });
                            *dst_data.offset(1 as libc::c_int as isize) = ({
                                ::core::mem::transmute::<
                                    _,
                                    [libc::c_short;
                                        ::core::mem::size_of::<__v8hi>()
                                            / ::core::mem::size_of::<libc::c_short>()],
                                >(_mm_cvtps_ph(
                                    _mm_setr_ps(
                                        x0 * sin_theta + x1 * cos_theta,
                                        0 as libc::c_int as libc::c_float,
                                        0 as libc::c_int as libc::c_float,
                                        0 as libc::c_int as libc::c_float,
                                    ),
                                    0 as libc::c_int,
                                ))[0 as libc::c_int as usize]
                                    as libc::c_ushort
                            });
                            i0 += 2 as libc::c_int as libc::c_long;
                        }
                    } else {
                        let mut ib: int64_t = 0 as libc::c_int as int64_t;
                        while ib < ne0 / n_dims as libc::c_long {
                            let mut ic: int64_t = 0 as libc::c_int as int64_t;
                            while ic < n_dims as libc::c_long {
                                let cos_theta_0: libc::c_float = cosf(theta);
                                let sin_theta_0: libc::c_float = sinf(theta);
                                theta *= theta_scale;
                                let i0_0: int64_t = ib * n_dims as libc::c_long
                                    + ic / 2 as libc::c_int as libc::c_long;
                                let src_0: *const ggml_fp16_t = ((*src0).data as *mut libc::c_char)
                                    .offset((i3 as libc::c_ulong).wrapping_mul(nb03) as isize)
                                    .offset((i2 as libc::c_ulong).wrapping_mul(nb02) as isize)
                                    .offset((i1 as libc::c_ulong).wrapping_mul(nb01) as isize)
                                    .offset((i0_0 as libc::c_ulong).wrapping_mul(nb00) as isize)
                                    as *mut ggml_fp16_t;
                                let mut dst_data_0: *mut ggml_fp16_t = ((*dst).data
                                    as *mut libc::c_char)
                                    .offset((i3 as libc::c_ulong).wrapping_mul(nb3) as isize)
                                    .offset((i2 as libc::c_ulong).wrapping_mul(nb2) as isize)
                                    .offset((i1 as libc::c_ulong).wrapping_mul(nb1) as isize)
                                    .offset((i0_0 as libc::c_ulong).wrapping_mul(nb0) as isize)
                                    as *mut ggml_fp16_t;
                                let x0_0: libc::c_float = ggml_lookup_fp16_to_fp32(
                                    *src_0.offset(0 as libc::c_int as isize),
                                );
                                let x1_0: libc::c_float = ggml_lookup_fp16_to_fp32(
                                    *src_0.offset((n_dims / 2 as libc::c_int) as isize),
                                );
                                *dst_data_0.offset(0 as libc::c_int as isize) = ({
                                    ::core::mem::transmute::<
                                        _,
                                        [libc::c_short;
                                            ::core::mem::size_of::<__v8hi>()
                                                / ::core::mem::size_of::<libc::c_short>()],
                                    >(_mm_cvtps_ph(
                                        _mm_setr_ps(
                                            x0_0 * cos_theta_0 - x1_0 * sin_theta_0,
                                            0 as libc::c_int as libc::c_float,
                                            0 as libc::c_int as libc::c_float,
                                            0 as libc::c_int as libc::c_float,
                                        ),
                                        0 as libc::c_int,
                                    ))[0 as libc::c_int as usize]
                                        as libc::c_ushort
                                });
                                *dst_data_0.offset((n_dims / 2 as libc::c_int) as isize) = ({
                                    ::core::mem::transmute::<
                                        _,
                                        [libc::c_short;
                                            ::core::mem::size_of::<__v8hi>()
                                                / ::core::mem::size_of::<libc::c_short>()],
                                    >(_mm_cvtps_ph(
                                        _mm_setr_ps(
                                            x0_0 * sin_theta_0 + x1_0 * cos_theta_0,
                                            0 as libc::c_int as libc::c_float,
                                            0 as libc::c_int as libc::c_float,
                                            0 as libc::c_int as libc::c_float,
                                        ),
                                        0 as libc::c_int,
                                    ))[0 as libc::c_int as usize]
                                        as libc::c_ushort
                                });
                                ic += 2 as libc::c_int as libc::c_long;
                            }
                            ib += 1;
                            ib;
                        }
                    }
                }
                i1 += 1;
                i1;
            }
            i2 += 1;
            i2;
        }
        i3 += 1;
        i3;
    }
}
unsafe extern "C" fn ggml_compute_forward_rope(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        1 => {
            ggml_compute_forward_rope_f16(params, src0, src1, dst);
        }
        0 => {
            ggml_compute_forward_rope_f32(params, src0, src1, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    11241 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_rope_back_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let n_past: libc::c_int = *((*src1).data as *mut int32_t).offset(0 as libc::c_int as isize);
    let n_dims: libc::c_int = *((*src1).data as *mut int32_t).offset(1 as libc::c_int as isize);
    let mode: libc::c_int = *((*src1).data as *mut int32_t).offset(2 as libc::c_int as isize);
    let nb00: size_t = (*src0).nb[0 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let ne0: int64_t = (*dst).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*dst).ne[1 as libc::c_int as usize];
    let ne2: int64_t = (*dst).ne[2 as libc::c_int as usize];
    let ne3: int64_t = (*dst).ne[3 as libc::c_int as usize];
    let nb0: size_t = (*dst).nb[0 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nr: libc::c_int = ggml_nrows(dst);
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut ir: libc::c_int = 0 as libc::c_int;
    let theta_scale: libc::c_float = powf(
        10000.0f64 as libc::c_float,
        -2.0f32 / n_dims as libc::c_float,
    );
    let is_neox: bool = mode & 2 as libc::c_int != 0;
    let mut i3: int64_t = 0 as libc::c_int as int64_t;
    while i3 < ne3 {
        let mut i2: int64_t = (if mode & 1 as libc::c_int == 0 as libc::c_int {
            0 as libc::c_int
        } else {
            n_past
        }) as int64_t;
        while i2 < ne2 {
            let p: int64_t = if mode & 1 as libc::c_int == 0 as libc::c_int {
                n_past as libc::c_long + i2
            } else {
                i2
            };
            let mut i1: int64_t = 0 as libc::c_int as int64_t;
            while i1 < ne1 {
                let fresh7 = ir;
                ir = ir + 1;
                if !(fresh7 < ir0) {
                    if ir > ir1 {
                        break;
                    }
                    let mut theta: libc::c_float = p as libc::c_float;
                    if !is_neox {
                        let mut i0: int64_t = 0 as libc::c_int as int64_t;
                        while i0 < ne0 {
                            let cos_theta: libc::c_float = cosf(theta);
                            let sin_theta: libc::c_float = sinf(theta);
                            theta *= theta_scale;
                            let dy: *const libc::c_float = ((*src0).data as *mut libc::c_char)
                                .offset((i3 as libc::c_ulong).wrapping_mul(nb03) as isize)
                                .offset((i2 as libc::c_ulong).wrapping_mul(nb02) as isize)
                                .offset((i1 as libc::c_ulong).wrapping_mul(nb01) as isize)
                                .offset((i0 as libc::c_ulong).wrapping_mul(nb00) as isize)
                                as *mut libc::c_float;
                            let mut dx: *mut libc::c_float = ((*dst).data as *mut libc::c_char)
                                .offset((i3 as libc::c_ulong).wrapping_mul(nb3) as isize)
                                .offset((i2 as libc::c_ulong).wrapping_mul(nb2) as isize)
                                .offset((i1 as libc::c_ulong).wrapping_mul(nb1) as isize)
                                .offset((i0 as libc::c_ulong).wrapping_mul(nb0) as isize)
                                as *mut libc::c_float;
                            let dy0: libc::c_float = *dy.offset(0 as libc::c_int as isize);
                            let dy1: libc::c_float = *dy.offset(1 as libc::c_int as isize);
                            *dx.offset(0 as libc::c_int as isize) =
                                dy0 * cos_theta + dy1 * sin_theta;
                            *dx.offset(1 as libc::c_int as isize) =
                                -dy0 * sin_theta + dy1 * cos_theta;
                            i0 += 2 as libc::c_int as libc::c_long;
                        }
                    } else {
                        let mut ib: int64_t = 0 as libc::c_int as int64_t;
                        while ib < ne0 / n_dims as libc::c_long {
                            let mut ic: int64_t = 0 as libc::c_int as int64_t;
                            while ic < n_dims as libc::c_long {
                                let cos_theta_0: libc::c_float = cosf(theta);
                                let sin_theta_0: libc::c_float = sinf(theta);
                                theta *= theta_scale;
                                let i0_0: int64_t = ib * n_dims as libc::c_long
                                    + ic / 2 as libc::c_int as libc::c_long;
                                let dy_0: *const libc::c_float = ((*src0).data as *mut libc::c_char)
                                    .offset((i3 as libc::c_ulong).wrapping_mul(nb03) as isize)
                                    .offset((i2 as libc::c_ulong).wrapping_mul(nb02) as isize)
                                    .offset((i1 as libc::c_ulong).wrapping_mul(nb01) as isize)
                                    .offset((i0_0 as libc::c_ulong).wrapping_mul(nb00) as isize)
                                    as *mut libc::c_float;
                                let mut dx_0: *mut libc::c_float = ((*dst).data
                                    as *mut libc::c_char)
                                    .offset((i3 as libc::c_ulong).wrapping_mul(nb3) as isize)
                                    .offset((i2 as libc::c_ulong).wrapping_mul(nb2) as isize)
                                    .offset((i1 as libc::c_ulong).wrapping_mul(nb1) as isize)
                                    .offset((i0_0 as libc::c_ulong).wrapping_mul(nb0) as isize)
                                    as *mut libc::c_float;
                                let dy0_0: libc::c_float = *dy_0.offset(0 as libc::c_int as isize);
                                let dy1_0: libc::c_float =
                                    *dy_0.offset((n_dims / 2 as libc::c_int) as isize);
                                *dx_0.offset(0 as libc::c_int as isize) =
                                    dy0_0 * cos_theta_0 + dy1_0 * sin_theta_0;
                                *dx_0.offset((n_dims / 2 as libc::c_int) as isize) =
                                    -dy0_0 * sin_theta_0 + dy1_0 * cos_theta_0;
                                ic += 2 as libc::c_int as libc::c_long;
                            }
                            ib += 1;
                            ib;
                        }
                    }
                }
                i1 += 1;
                i1;
            }
            i2 += 1;
            i2;
        }
        i3 += 1;
        i3;
    }
}
unsafe extern "C" fn ggml_compute_forward_rope_back_f16(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let n_past: libc::c_int = *((*src1).data as *mut int32_t).offset(0 as libc::c_int as isize);
    let n_dims: libc::c_int = *((*src1).data as *mut int32_t).offset(1 as libc::c_int as isize);
    let mode: libc::c_int = *((*src1).data as *mut int32_t).offset(2 as libc::c_int as isize);
    let nb00: size_t = (*src0).nb[0 as libc::c_int as usize];
    let nb01: size_t = (*src0).nb[1 as libc::c_int as usize];
    let nb02: size_t = (*src0).nb[2 as libc::c_int as usize];
    let nb03: size_t = (*src0).nb[3 as libc::c_int as usize];
    let ne0: int64_t = (*dst).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*dst).ne[1 as libc::c_int as usize];
    let ne2: int64_t = (*dst).ne[2 as libc::c_int as usize];
    let ne3: int64_t = (*dst).ne[3 as libc::c_int as usize];
    let nb0: size_t = (*dst).nb[0 as libc::c_int as usize];
    let nb1: size_t = (*dst).nb[1 as libc::c_int as usize];
    let nb2: size_t = (*dst).nb[2 as libc::c_int as usize];
    let nb3: size_t = (*dst).nb[3 as libc::c_int as usize];
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nr: libc::c_int = ggml_nrows(dst);
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut ir: libc::c_int = 0 as libc::c_int;
    let theta_scale: libc::c_float = powf(
        10000.0f64 as libc::c_float,
        -2.0f32 / n_dims as libc::c_float,
    );
    let is_neox: bool = mode & 2 as libc::c_int != 0;
    let mut i3: int64_t = 0 as libc::c_int as int64_t;
    while i3 < ne3 {
        let mut i2: int64_t = (if mode & 1 as libc::c_int == 0 as libc::c_int {
            0 as libc::c_int
        } else {
            n_past
        }) as int64_t;
        while i2 < ne2 {
            let p: int64_t = if mode & 1 as libc::c_int == 0 as libc::c_int {
                n_past as libc::c_long + i2
            } else {
                i2
            };
            let mut i1: int64_t = 0 as libc::c_int as int64_t;
            while i1 < ne1 {
                let fresh8 = ir;
                ir = ir + 1;
                if !(fresh8 < ir0) {
                    if ir > ir1 {
                        break;
                    }
                    let mut theta: libc::c_float = p as libc::c_float;
                    if !is_neox {
                        let mut i0: int64_t = 0 as libc::c_int as int64_t;
                        while i0 < ne0 {
                            let cos_theta: libc::c_float = cosf(theta);
                            let sin_theta: libc::c_float = sinf(theta);
                            theta *= theta_scale;
                            let dy: *const ggml_fp16_t = ((*src0).data as *mut libc::c_char)
                                .offset((i3 as libc::c_ulong).wrapping_mul(nb03) as isize)
                                .offset((i2 as libc::c_ulong).wrapping_mul(nb02) as isize)
                                .offset((i1 as libc::c_ulong).wrapping_mul(nb01) as isize)
                                .offset((i0 as libc::c_ulong).wrapping_mul(nb00) as isize)
                                as *mut ggml_fp16_t;
                            let mut dx: *mut ggml_fp16_t = ((*dst).data as *mut libc::c_char)
                                .offset((i3 as libc::c_ulong).wrapping_mul(nb3) as isize)
                                .offset((i2 as libc::c_ulong).wrapping_mul(nb2) as isize)
                                .offset((i1 as libc::c_ulong).wrapping_mul(nb1) as isize)
                                .offset((i0 as libc::c_ulong).wrapping_mul(nb0) as isize)
                                as *mut ggml_fp16_t;
                            let dy0: libc::c_float =
                                ggml_lookup_fp16_to_fp32(*dy.offset(0 as libc::c_int as isize));
                            let dy1: libc::c_float =
                                ggml_lookup_fp16_to_fp32(*dy.offset(1 as libc::c_int as isize));
                            *dx.offset(0 as libc::c_int as isize) = ({
                                ::core::mem::transmute::<
                                    _,
                                    [libc::c_short;
                                        ::core::mem::size_of::<__v8hi>()
                                            / ::core::mem::size_of::<libc::c_short>()],
                                >(_mm_cvtps_ph(
                                    _mm_setr_ps(
                                        dy0 * cos_theta + dy1 * sin_theta,
                                        0 as libc::c_int as libc::c_float,
                                        0 as libc::c_int as libc::c_float,
                                        0 as libc::c_int as libc::c_float,
                                    ),
                                    0 as libc::c_int,
                                ))[0 as libc::c_int as usize]
                                    as libc::c_ushort
                            });
                            *dx.offset(1 as libc::c_int as isize) = ({
                                ::core::mem::transmute::<
                                    _,
                                    [libc::c_short;
                                        ::core::mem::size_of::<__v8hi>()
                                            / ::core::mem::size_of::<libc::c_short>()],
                                >(_mm_cvtps_ph(
                                    _mm_setr_ps(
                                        -dy0 * sin_theta + dy1 * cos_theta,
                                        0 as libc::c_int as libc::c_float,
                                        0 as libc::c_int as libc::c_float,
                                        0 as libc::c_int as libc::c_float,
                                    ),
                                    0 as libc::c_int,
                                ))[0 as libc::c_int as usize]
                                    as libc::c_ushort
                            });
                            i0 += 2 as libc::c_int as libc::c_long;
                        }
                    } else {
                        let mut ib: int64_t = 0 as libc::c_int as int64_t;
                        while ib < ne0 / n_dims as libc::c_long {
                            let mut ic: int64_t = 0 as libc::c_int as int64_t;
                            while ic < n_dims as libc::c_long {
                                let cos_theta_0: libc::c_float = cosf(theta);
                                let sin_theta_0: libc::c_float = sinf(theta);
                                theta *= theta_scale;
                                let i0_0: int64_t = ib * n_dims as libc::c_long
                                    + ic / 2 as libc::c_int as libc::c_long;
                                let dy_0: *const ggml_fp16_t = ((*src0).data as *mut libc::c_char)
                                    .offset((i3 as libc::c_ulong).wrapping_mul(nb03) as isize)
                                    .offset((i2 as libc::c_ulong).wrapping_mul(nb02) as isize)
                                    .offset((i1 as libc::c_ulong).wrapping_mul(nb01) as isize)
                                    .offset((i0_0 as libc::c_ulong).wrapping_mul(nb00) as isize)
                                    as *mut ggml_fp16_t;
                                let mut dx_0: *mut ggml_fp16_t = ((*dst).data as *mut libc::c_char)
                                    .offset((i3 as libc::c_ulong).wrapping_mul(nb3) as isize)
                                    .offset((i2 as libc::c_ulong).wrapping_mul(nb2) as isize)
                                    .offset((i1 as libc::c_ulong).wrapping_mul(nb1) as isize)
                                    .offset((i0_0 as libc::c_ulong).wrapping_mul(nb0) as isize)
                                    as *mut ggml_fp16_t;
                                let dy0_0: libc::c_float = ggml_lookup_fp16_to_fp32(
                                    *dy_0.offset(0 as libc::c_int as isize),
                                );
                                let dy1_0: libc::c_float = ggml_lookup_fp16_to_fp32(
                                    *dy_0.offset((n_dims / 2 as libc::c_int) as isize),
                                );
                                *dx_0.offset(0 as libc::c_int as isize) = ({
                                    ::core::mem::transmute::<
                                        _,
                                        [libc::c_short;
                                            ::core::mem::size_of::<__v8hi>()
                                                / ::core::mem::size_of::<libc::c_short>()],
                                    >(_mm_cvtps_ph(
                                        _mm_setr_ps(
                                            dy0_0 * cos_theta_0 + dy1_0 * sin_theta_0,
                                            0 as libc::c_int as libc::c_float,
                                            0 as libc::c_int as libc::c_float,
                                            0 as libc::c_int as libc::c_float,
                                        ),
                                        0 as libc::c_int,
                                    ))[0 as libc::c_int as usize]
                                        as libc::c_ushort
                                });
                                *dx_0.offset((n_dims / 2 as libc::c_int) as isize) = ({
                                    ::core::mem::transmute::<
                                        _,
                                        [libc::c_short;
                                            ::core::mem::size_of::<__v8hi>()
                                                / ::core::mem::size_of::<libc::c_short>()],
                                    >(_mm_cvtps_ph(
                                        _mm_setr_ps(
                                            -dy0_0 * sin_theta_0 + dy1_0 * cos_theta_0,
                                            0 as libc::c_int as libc::c_float,
                                            0 as libc::c_int as libc::c_float,
                                            0 as libc::c_int as libc::c_float,
                                        ),
                                        0 as libc::c_int,
                                    ))[0 as libc::c_int as usize]
                                        as libc::c_ushort
                                });
                                ic += 2 as libc::c_int as libc::c_long;
                            }
                            ib += 1;
                            ib;
                        }
                    }
                }
                i1 += 1;
                i1;
            }
            i2 += 1;
            i2;
        }
        i3 += 1;
        i3;
    }
}
unsafe extern "C" fn ggml_compute_forward_rope_back(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        1 => {
            ggml_compute_forward_rope_back_f16(params, src0, src1, dst);
        }
        0 => {
            ggml_compute_forward_rope_back_f32(params, src0, src1, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    11490 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_conv_1d_1s_f16_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !((*src0).type_0 as libc::c_uint == GGML_TYPE_F16 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11502 as libc::c_int,
            b"src0->type == GGML_TYPE_F16\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*src1).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11503 as libc::c_int,
            b"src1->type == GGML_TYPE_F32\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*dst).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11504 as libc::c_int,
            b"dst->type == GGML_TYPE_F32\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut t0: int64_t = 0 as libc::c_int as int64_t;
    let ne00: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne01: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne02: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let ne10: int64_t = (*src1).ne[0 as libc::c_int as usize];
    let ne11: int64_t = (*src1).ne[1 as libc::c_int as usize];
    let nb00: libc::c_int = (*src0).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb01: libc::c_int = (*src0).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb02: libc::c_int = (*src0).nb[2 as libc::c_int as usize] as libc::c_int;
    let nb10: libc::c_int = (*src1).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb11: libc::c_int = (*src1).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb1: libc::c_int = (*dst).nb[1 as libc::c_int as usize] as libc::c_int;
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nk: libc::c_int = ne00 as libc::c_int;
    let nh: libc::c_int = nk / 2 as libc::c_int;
    let ew0: libc::c_int = ggml_up32(ne01 as libc::c_int);
    if !(ne00 % 2 as libc::c_int as libc::c_long == 1 as libc::c_int as libc::c_long) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11548 as libc::c_int,
            b"ne00 % 2 == 1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb00 as libc::c_ulong == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11549 as libc::c_int,
            b"nb00 == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb10 as libc::c_ulong == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11550 as libc::c_int,
            b"nb10 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint {
        memset((*params).wdata, 0 as libc::c_int, (*params).wsize);
        let wdata: *mut ggml_fp16_t =
            ((*params).wdata as *mut ggml_fp16_t).offset(0 as libc::c_int as isize);
        let mut i02: int64_t = 0 as libc::c_int as int64_t;
        while i02 < ne02 {
            let mut i01: int64_t = 0 as libc::c_int as int64_t;
            while i01 < ne01 {
                let src: *const ggml_fp16_t = ((*src0).data as *mut libc::c_char)
                    .offset((i02 * nb02 as libc::c_long) as isize)
                    .offset((i01 * nb01 as libc::c_long) as isize)
                    as *mut ggml_fp16_t;
                let mut dst_data: *mut ggml_fp16_t =
                    wdata.offset((i02 * ew0 as libc::c_long * ne00) as isize);
                let mut i00: int64_t = 0 as libc::c_int as int64_t;
                while i00 < ne00 {
                    *dst_data.offset((i00 * ew0 as libc::c_long + i01) as isize) =
                        *src.offset(i00 as isize);
                    i00 += 1;
                    i00;
                }
                i01 += 1;
                i01;
            }
            i02 += 1;
            i02;
        }
        let wdata_0: *mut ggml_fp16_t = ((*params).wdata as *mut ggml_fp16_t)
            .offset((ne02 * ew0 as libc::c_long * ne00) as isize);
        let mut i11: int64_t = 0 as libc::c_int as int64_t;
        while i11 < ne11 {
            let src_0: *const libc::c_float = ((*src1).data as *mut libc::c_char)
                .offset((i11 * nb11 as libc::c_long) as isize)
                as *mut libc::c_float;
            let mut dst_data_0: *mut ggml_fp16_t = wdata_0;
            let mut i10: int64_t = 0 as libc::c_int as int64_t;
            while i10 < ne10 {
                *dst_data_0
                    .offset(((i10 + nh as libc::c_long) * ew0 as libc::c_long + i11) as isize) = ({
                    ::core::mem::transmute::<
                        _,
                        [libc::c_short;
                            ::core::mem::size_of::<__v8hi>()
                                / ::core::mem::size_of::<libc::c_short>()],
                    >(_mm_cvtps_ph(
                        _mm_setr_ps(
                            *src_0.offset(i10 as isize),
                            0 as libc::c_int as libc::c_float,
                            0 as libc::c_int as libc::c_float,
                            0 as libc::c_int as libc::c_float,
                        ),
                        0 as libc::c_int,
                    ))[0 as libc::c_int as usize] as libc::c_ushort
                });
                i10 += 1;
                i10;
            }
            i11 += 1;
            i11;
        }
        return;
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint {
        return;
    }
    let nr: libc::c_int = ne02 as libc::c_int;
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut i1: libc::c_int = ir0;
    while i1 < ir1 {
        let mut dst_data_1: *mut libc::c_float =
            ((*dst).data as *mut libc::c_char).offset((i1 * nb1) as isize) as *mut libc::c_float;
        let mut i0: int64_t = 0 as libc::c_int as int64_t;
        while i0 < ne10 {
            *dst_data_1.offset(i0 as isize) = 0 as libc::c_int as libc::c_float;
            let mut k: libc::c_int = -nh;
            while k <= nh {
                let mut v: libc::c_float = 0.0f32;
                ggml_vec_dot_f16(
                    ew0,
                    &mut v,
                    ((*params).wdata as *mut ggml_fp16_t)
                        .offset(((i1 * ew0) as libc::c_long * ne00) as isize)
                        .offset(((nh + k) * ew0) as isize),
                    ((*params).wdata as *mut ggml_fp16_t)
                        .offset((ne02 * ew0 as libc::c_long * ne00) as isize)
                        .offset(
                            ((i0 + nh as libc::c_long + k as libc::c_long) * ew0 as libc::c_long)
                                as isize,
                        ),
                );
                *dst_data_1.offset(i0 as isize) += v;
                k += 1;
                k;
            }
            i0 += 1;
            i0;
        }
        i1 += 1;
        i1;
    }
}
unsafe extern "C" fn ggml_compute_forward_conv_1d_1s_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !((*src0).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11622 as libc::c_int,
            b"src0->type == GGML_TYPE_F32\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*src1).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11623 as libc::c_int,
            b"src1->type == GGML_TYPE_F32\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*dst).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11624 as libc::c_int,
            b"dst->type == GGML_TYPE_F32\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut t0: int64_t = 0 as libc::c_int as int64_t;
    let ne00: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne01: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne02: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let ne10: int64_t = (*src1).ne[0 as libc::c_int as usize];
    let ne11: int64_t = (*src1).ne[1 as libc::c_int as usize];
    let nb00: libc::c_int = (*src0).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb01: libc::c_int = (*src0).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb02: libc::c_int = (*src0).nb[2 as libc::c_int as usize] as libc::c_int;
    let nb10: libc::c_int = (*src1).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb11: libc::c_int = (*src1).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb1: libc::c_int = (*dst).nb[1 as libc::c_int as usize] as libc::c_int;
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nk: libc::c_int = ne00 as libc::c_int;
    let nh: libc::c_int = nk / 2 as libc::c_int;
    let ew0: libc::c_int = ggml_up32(ne01 as libc::c_int);
    if !(ne00 % 2 as libc::c_int as libc::c_long == 1 as libc::c_int as libc::c_long) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11668 as libc::c_int,
            b"ne00 % 2 == 1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb00 as libc::c_ulong == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11669 as libc::c_int,
            b"nb00 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb10 as libc::c_ulong == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11670 as libc::c_int,
            b"nb10 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint {
        memset((*params).wdata, 0 as libc::c_int, (*params).wsize);
        let wdata: *mut libc::c_float =
            ((*params).wdata as *mut libc::c_float).offset(0 as libc::c_int as isize);
        let mut i02: int64_t = 0 as libc::c_int as int64_t;
        while i02 < ne02 {
            let mut i01: int64_t = 0 as libc::c_int as int64_t;
            while i01 < ne01 {
                let src: *const libc::c_float = ((*src0).data as *mut libc::c_char)
                    .offset((i02 * nb02 as libc::c_long) as isize)
                    .offset((i01 * nb01 as libc::c_long) as isize)
                    as *mut libc::c_float;
                let mut dst_data: *mut libc::c_float =
                    wdata.offset((i02 * ew0 as libc::c_long * ne00) as isize);
                let mut i00: int64_t = 0 as libc::c_int as int64_t;
                while i00 < ne00 {
                    *dst_data.offset((i00 * ew0 as libc::c_long + i01) as isize) =
                        *src.offset(i00 as isize);
                    i00 += 1;
                    i00;
                }
                i01 += 1;
                i01;
            }
            i02 += 1;
            i02;
        }
        let wdata_0: *mut libc::c_float = ((*params).wdata as *mut libc::c_float)
            .offset((ne02 * ew0 as libc::c_long * ne00) as isize);
        let mut i11: int64_t = 0 as libc::c_int as int64_t;
        while i11 < ne11 {
            let src_0: *const libc::c_float = ((*src1).data as *mut libc::c_char)
                .offset((i11 * nb11 as libc::c_long) as isize)
                as *mut libc::c_float;
            let mut dst_data_0: *mut libc::c_float = wdata_0;
            let mut i10: int64_t = 0 as libc::c_int as int64_t;
            while i10 < ne10 {
                *dst_data_0
                    .offset(((i10 + nh as libc::c_long) * ew0 as libc::c_long + i11) as isize) =
                    *src_0.offset(i10 as isize);
                i10 += 1;
                i10;
            }
            i11 += 1;
            i11;
        }
        return;
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint {
        return;
    }
    let nr: libc::c_int = ne02 as libc::c_int;
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut i1: libc::c_int = ir0;
    while i1 < ir1 {
        let mut dst_data_1: *mut libc::c_float =
            ((*dst).data as *mut libc::c_char).offset((i1 * nb1) as isize) as *mut libc::c_float;
        let mut i0: int64_t = 0 as libc::c_int as int64_t;
        while i0 < ne10 {
            *dst_data_1.offset(i0 as isize) = 0 as libc::c_int as libc::c_float;
            let mut k: libc::c_int = -nh;
            while k <= nh {
                let mut v: libc::c_float = 0.0f32;
                ggml_vec_dot_f32(
                    ew0,
                    &mut v,
                    ((*params).wdata as *mut libc::c_float)
                        .offset(((i1 * ew0) as libc::c_long * ne00) as isize)
                        .offset(((nh + k) * ew0) as isize),
                    ((*params).wdata as *mut libc::c_float)
                        .offset((ne02 * ew0 as libc::c_long * ne00) as isize)
                        .offset(
                            ((i0 + nh as libc::c_long + k as libc::c_long) * ew0 as libc::c_long)
                                as isize,
                        ),
                );
                *dst_data_1.offset(i0 as isize) += v;
                k += 1;
                k;
            }
            i0 += 1;
            i0;
        }
        i1 += 1;
        i1;
    }
}
unsafe extern "C" fn ggml_compute_forward_conv_1d_1s(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        1 => {
            ggml_compute_forward_conv_1d_1s_f16_f32(params, src0, src1, dst);
        }
        0 => {
            ggml_compute_forward_conv_1d_1s_f32(params, src0, src1, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    11753 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_conv_1d_2s_f16_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !((*src0).type_0 as libc::c_uint == GGML_TYPE_F16 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11765 as libc::c_int,
            b"src0->type == GGML_TYPE_F16\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*src1).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11766 as libc::c_int,
            b"src1->type == GGML_TYPE_F32\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*dst).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11767 as libc::c_int,
            b"dst->type == GGML_TYPE_F32\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut t0: int64_t = 0 as libc::c_int as int64_t;
    let ne00: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne01: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne02: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let ne10: int64_t = (*src1).ne[0 as libc::c_int as usize];
    let ne11: int64_t = (*src1).ne[1 as libc::c_int as usize];
    let nb00: libc::c_int = (*src0).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb01: libc::c_int = (*src0).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb02: libc::c_int = (*src0).nb[2 as libc::c_int as usize] as libc::c_int;
    let nb10: libc::c_int = (*src1).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb11: libc::c_int = (*src1).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb1: libc::c_int = (*dst).nb[1 as libc::c_int as usize] as libc::c_int;
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nk: libc::c_int = ne00 as libc::c_int;
    let nh: libc::c_int = nk / 2 as libc::c_int;
    let ew0: libc::c_int = ggml_up32(ne01 as libc::c_int);
    if !(ne00 % 2 as libc::c_int as libc::c_long == 1 as libc::c_int as libc::c_long) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11811 as libc::c_int,
            b"ne00 % 2 == 1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb00 as libc::c_ulong == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11812 as libc::c_int,
            b"nb00 == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb10 as libc::c_ulong == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11813 as libc::c_int,
            b"nb10 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint {
        memset((*params).wdata, 0 as libc::c_int, (*params).wsize);
        let wdata: *mut ggml_fp16_t =
            ((*params).wdata as *mut ggml_fp16_t).offset(0 as libc::c_int as isize);
        let mut i02: int64_t = 0 as libc::c_int as int64_t;
        while i02 < ne02 {
            let mut i01: int64_t = 0 as libc::c_int as int64_t;
            while i01 < ne01 {
                let src: *const ggml_fp16_t = ((*src0).data as *mut libc::c_char)
                    .offset((i02 * nb02 as libc::c_long) as isize)
                    .offset((i01 * nb01 as libc::c_long) as isize)
                    as *mut ggml_fp16_t;
                let mut dst_data: *mut ggml_fp16_t =
                    wdata.offset((i02 * ew0 as libc::c_long * ne00) as isize);
                let mut i00: int64_t = 0 as libc::c_int as int64_t;
                while i00 < ne00 {
                    *dst_data.offset((i00 * ew0 as libc::c_long + i01) as isize) =
                        *src.offset(i00 as isize);
                    i00 += 1;
                    i00;
                }
                i01 += 1;
                i01;
            }
            i02 += 1;
            i02;
        }
        let wdata_0: *mut ggml_fp16_t = ((*params).wdata as *mut ggml_fp16_t)
            .offset((ne02 * ew0 as libc::c_long * ne00) as isize);
        let mut i11: int64_t = 0 as libc::c_int as int64_t;
        while i11 < ne11 {
            let src_0: *const libc::c_float = ((*src1).data as *mut libc::c_char)
                .offset((i11 * nb11 as libc::c_long) as isize)
                as *mut libc::c_float;
            let mut dst_data_0: *mut ggml_fp16_t = wdata_0;
            let mut i10: int64_t = 0 as libc::c_int as int64_t;
            while i10 < ne10 {
                *dst_data_0
                    .offset(((i10 + nh as libc::c_long) * ew0 as libc::c_long + i11) as isize) = ({
                    ::core::mem::transmute::<
                        _,
                        [libc::c_short;
                            ::core::mem::size_of::<__v8hi>()
                                / ::core::mem::size_of::<libc::c_short>()],
                    >(_mm_cvtps_ph(
                        _mm_setr_ps(
                            *src_0.offset(i10 as isize),
                            0 as libc::c_int as libc::c_float,
                            0 as libc::c_int as libc::c_float,
                            0 as libc::c_int as libc::c_float,
                        ),
                        0 as libc::c_int,
                    ))[0 as libc::c_int as usize] as libc::c_ushort
                });
                i10 += 1;
                i10;
            }
            i11 += 1;
            i11;
        }
        return;
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint {
        return;
    }
    let nr: libc::c_int = ne02 as libc::c_int;
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut i1: libc::c_int = ir0;
    while i1 < ir1 {
        let mut dst_data_1: *mut libc::c_float =
            ((*dst).data as *mut libc::c_char).offset((i1 * nb1) as isize) as *mut libc::c_float;
        let mut i0: int64_t = 0 as libc::c_int as int64_t;
        while i0 < ne10 {
            *dst_data_1.offset((i0 / 2 as libc::c_int as libc::c_long) as isize) =
                0 as libc::c_int as libc::c_float;
            let mut k: libc::c_int = -nh;
            while k <= nh {
                let mut v: libc::c_float = 0.0f32;
                ggml_vec_dot_f16(
                    ew0,
                    &mut v,
                    ((*params).wdata as *mut ggml_fp16_t)
                        .offset(((i1 * ew0) as libc::c_long * ne00) as isize)
                        .offset(((nh + k) * ew0) as isize),
                    ((*params).wdata as *mut ggml_fp16_t)
                        .offset((ne02 * ew0 as libc::c_long * ne00) as isize)
                        .offset(
                            ((i0 + nh as libc::c_long + k as libc::c_long) * ew0 as libc::c_long)
                                as isize,
                        ),
                );
                *dst_data_1.offset((i0 / 2 as libc::c_int as libc::c_long) as isize) += v;
                k += 1;
                k;
            }
            i0 += 2 as libc::c_int as libc::c_long;
        }
        i1 += 1;
        i1;
    }
}
unsafe extern "C" fn ggml_compute_forward_conv_1d_2s_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    if !((*src0).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11885 as libc::c_int,
            b"src0->type == GGML_TYPE_F32\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*src1).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11886 as libc::c_int,
            b"src1->type == GGML_TYPE_F32\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !((*dst).type_0 as libc::c_uint == GGML_TYPE_F32 as libc::c_int as libc::c_uint) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11887 as libc::c_int,
            b"dst->type == GGML_TYPE_F32\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    let mut t0: int64_t = 0 as libc::c_int as int64_t;
    let ne00: int64_t = (*src0).ne[0 as libc::c_int as usize];
    let ne01: int64_t = (*src0).ne[1 as libc::c_int as usize];
    let ne02: int64_t = (*src0).ne[2 as libc::c_int as usize];
    let ne10: int64_t = (*src1).ne[0 as libc::c_int as usize];
    let ne11: int64_t = (*src1).ne[1 as libc::c_int as usize];
    let nb00: libc::c_int = (*src0).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb01: libc::c_int = (*src0).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb02: libc::c_int = (*src0).nb[2 as libc::c_int as usize] as libc::c_int;
    let nb10: libc::c_int = (*src1).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb11: libc::c_int = (*src1).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb1: libc::c_int = (*dst).nb[1 as libc::c_int as usize] as libc::c_int;
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let nk: libc::c_int = ne00 as libc::c_int;
    let nh: libc::c_int = nk / 2 as libc::c_int;
    let ew0: libc::c_int = ggml_up32(ne01 as libc::c_int);
    if !(ne00 % 2 as libc::c_int as libc::c_long == 1 as libc::c_int as libc::c_long) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11931 as libc::c_int,
            b"ne00 % 2 == 1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb00 as libc::c_ulong == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11932 as libc::c_int,
            b"nb00 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb10 as libc::c_ulong == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            11933 as libc::c_int,
            b"nb10 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint {
        memset((*params).wdata, 0 as libc::c_int, (*params).wsize);
        let wdata: *mut libc::c_float =
            ((*params).wdata as *mut libc::c_float).offset(0 as libc::c_int as isize);
        let mut i02: int64_t = 0 as libc::c_int as int64_t;
        while i02 < ne02 {
            let mut i01: int64_t = 0 as libc::c_int as int64_t;
            while i01 < ne01 {
                let src: *const libc::c_float = ((*src0).data as *mut libc::c_char)
                    .offset((i02 * nb02 as libc::c_long) as isize)
                    .offset((i01 * nb01 as libc::c_long) as isize)
                    as *mut libc::c_float;
                let mut dst_data: *mut libc::c_float =
                    wdata.offset((i02 * ew0 as libc::c_long * ne00) as isize);
                let mut i00: int64_t = 0 as libc::c_int as int64_t;
                while i00 < ne00 {
                    *dst_data.offset((i00 * ew0 as libc::c_long + i01) as isize) =
                        *src.offset(i00 as isize);
                    i00 += 1;
                    i00;
                }
                i01 += 1;
                i01;
            }
            i02 += 1;
            i02;
        }
        let wdata_0: *mut libc::c_float = ((*params).wdata as *mut libc::c_float)
            .offset((ne02 * ew0 as libc::c_long * ne00) as isize);
        let mut i11: int64_t = 0 as libc::c_int as int64_t;
        while i11 < ne11 {
            let src_0: *const libc::c_float = ((*src1).data as *mut libc::c_char)
                .offset((i11 * nb11 as libc::c_long) as isize)
                as *mut libc::c_float;
            let mut dst_data_0: *mut libc::c_float = wdata_0;
            let mut i10: int64_t = 0 as libc::c_int as int64_t;
            while i10 < ne10 {
                *dst_data_0
                    .offset(((i10 + nh as libc::c_long) * ew0 as libc::c_long + i11) as isize) =
                    *src_0.offset(i10 as isize);
                i10 += 1;
                i10;
            }
            i11 += 1;
            i11;
        }
        return;
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint {
        return;
    }
    let nr: libc::c_int = ne02 as libc::c_int;
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut i1: libc::c_int = ir0;
    while i1 < ir1 {
        let mut dst_data_1: *mut libc::c_float =
            ((*dst).data as *mut libc::c_char).offset((i1 * nb1) as isize) as *mut libc::c_float;
        let mut i0: int64_t = 0 as libc::c_int as int64_t;
        while i0 < ne10 {
            *dst_data_1.offset((i0 / 2 as libc::c_int as libc::c_long) as isize) =
                0 as libc::c_int as libc::c_float;
            let mut k: libc::c_int = -nh;
            while k <= nh {
                let mut v: libc::c_float = 0.0f32;
                ggml_vec_dot_f32(
                    ew0,
                    &mut v,
                    ((*params).wdata as *mut libc::c_float)
                        .offset(((i1 * ew0) as libc::c_long * ne00) as isize)
                        .offset(((nh + k) * ew0) as isize),
                    ((*params).wdata as *mut libc::c_float)
                        .offset((ne02 * ew0 as libc::c_long * ne00) as isize)
                        .offset(
                            ((i0 + nh as libc::c_long + k as libc::c_long) * ew0 as libc::c_long)
                                as isize,
                        ),
                );
                *dst_data_1.offset((i0 / 2 as libc::c_int as libc::c_long) as isize) += v;
                k += 1;
                k;
            }
            i0 += 2 as libc::c_int as libc::c_long;
        }
        i1 += 1;
        i1;
    }
}
unsafe extern "C" fn ggml_compute_forward_conv_1d_2s(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*src0).type_0 as libc::c_uint {
        1 => {
            ggml_compute_forward_conv_1d_2s_f16_f32(params, src0, src1, dst);
        }
        0 => {
            ggml_compute_forward_conv_1d_2s_f32(params, src0, src1, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    12016 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_flash_attn_f32(
    mut params: *const ggml_compute_params,
    mut q: *const ggml_tensor,
    mut k: *const ggml_tensor,
    mut v: *const ggml_tensor,
    masked: bool,
    mut dst: *mut ggml_tensor,
) {
    let mut t0: int64_t = 0 as libc::c_int as int64_t;
    let neq0: int64_t = (*q).ne[0 as libc::c_int as usize];
    let neq1: int64_t = (*q).ne[1 as libc::c_int as usize];
    let neq2: int64_t = (*q).ne[2 as libc::c_int as usize];
    let neq3: int64_t = (*q).ne[3 as libc::c_int as usize];
    let nek0: int64_t = (*k).ne[0 as libc::c_int as usize];
    let nek1: int64_t = (*k).ne[1 as libc::c_int as usize];
    let nev1: int64_t = (*v).ne[1 as libc::c_int as usize];
    let ne0: int64_t = (*dst).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*dst).ne[1 as libc::c_int as usize];
    let nbk0: libc::c_int = (*k).nb[0 as libc::c_int as usize] as libc::c_int;
    let nbk1: libc::c_int = (*k).nb[1 as libc::c_int as usize] as libc::c_int;
    let nbk2: libc::c_int = (*k).nb[2 as libc::c_int as usize] as libc::c_int;
    let nbk3: libc::c_int = (*k).nb[3 as libc::c_int as usize] as libc::c_int;
    let nbq0: libc::c_int = (*q).nb[0 as libc::c_int as usize] as libc::c_int;
    let nbq1: libc::c_int = (*q).nb[1 as libc::c_int as usize] as libc::c_int;
    let nbq2: libc::c_int = (*q).nb[2 as libc::c_int as usize] as libc::c_int;
    let nbq3: libc::c_int = (*q).nb[3 as libc::c_int as usize] as libc::c_int;
    let nbv0: libc::c_int = (*v).nb[0 as libc::c_int as usize] as libc::c_int;
    let nbv1: libc::c_int = (*v).nb[1 as libc::c_int as usize] as libc::c_int;
    let nbv2: libc::c_int = (*v).nb[2 as libc::c_int as usize] as libc::c_int;
    let nbv3: libc::c_int = (*v).nb[3 as libc::c_int as usize] as libc::c_int;
    let nb0: libc::c_int = (*dst).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb1: libc::c_int = (*dst).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb2: libc::c_int = (*dst).nb[2 as libc::c_int as usize] as libc::c_int;
    let nb3: libc::c_int = (*dst).nb[3 as libc::c_int as usize] as libc::c_int;
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let D: int64_t = neq0;
    let N: int64_t = neq1;
    let P: int64_t = nek1 - N;
    let M: int64_t = P + N;
    let Mup: libc::c_int = ggml_up(M as libc::c_int, 4 as libc::c_int);
    if !(ne0 == D) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12083 as libc::c_int,
            b"ne0 == D\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne1 == N) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12084 as libc::c_int,
            b"ne1 == N\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(P >= 0 as libc::c_int as libc::c_long) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12085 as libc::c_int,
            b"P >= 0\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nbq0 as libc::c_ulong == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12087 as libc::c_int,
            b"nbq0 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nbk0 as libc::c_ulong == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12088 as libc::c_int,
            b"nbk0 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nbv0 as libc::c_ulong == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12089 as libc::c_int,
            b"nbv0 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(neq0 == D) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12091 as libc::c_int,
            b"neq0 == D\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nek0 == D) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12092 as libc::c_int,
            b"nek0 == D\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nev1 == D) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12093 as libc::c_int,
            b"nev1 == D\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(neq1 == N) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12095 as libc::c_int,
            b"neq1 == N\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nek1 == N + P) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12096 as libc::c_int,
            b"nek1 == N + P\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nev1 == D) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12097 as libc::c_int,
            b"nev1 == D\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb0 as libc::c_ulong == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12100 as libc::c_int,
            b"nb0 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb0 <= nb1) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12101 as libc::c_int,
            b"nb0 <= nb1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb1 <= nb2) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12102 as libc::c_int,
            b"nb1 <= nb2\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb2 <= nb3) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12103 as libc::c_int,
            b"nb2 <= nb3\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint {
        return;
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint {
        return;
    }
    let nr: libc::c_int = (neq1 * neq2 * neq3) as libc::c_int;
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let scale: libc::c_float = 1.0f32 / sqrtf(D as libc::c_float);
    let mut ir: libc::c_int = ir0;
    while ir < ir1 {
        let iq3: libc::c_int = (ir as libc::c_long / (neq2 * neq1)) as libc::c_int;
        let iq2: libc::c_int =
            ((ir as libc::c_long - iq3 as libc::c_long * neq2 * neq1) / neq1) as libc::c_int;
        let iq1: libc::c_int = (ir as libc::c_long
            - iq3 as libc::c_long * neq2 * neq1
            - iq2 as libc::c_long * neq1) as libc::c_int;
        let mut S: *mut libc::c_float = ((*params).wdata as *mut libc::c_float).offset(
            (ith as libc::c_ulong)
                .wrapping_mul((Mup as libc::c_ulong).wrapping_add(CACHE_LINE_SIZE_F32))
                as isize,
        );
        let mut i: libc::c_int = M as libc::c_int;
        while i < Mup {
            *S.offset(i as isize) = -::core::f32::INFINITY;
            i += 1;
            i;
        }
        let mut ic: int64_t = 0 as libc::c_int as int64_t;
        while ic < nek1 {
            let ik3: libc::c_int = iq3;
            let ik2: libc::c_int = iq2;
            let ik1: libc::c_int = ic as libc::c_int;
            let i1: libc::c_int = ik1;
            ggml_vec_dot_f32(
                neq0 as libc::c_int,
                S.offset(i1 as isize),
                ((*k).data as *mut libc::c_char)
                    .offset((ik1 * nbk1 + ik2 * nbk2 + ik3 * nbk3) as isize)
                    as *mut libc::c_float,
                ((*q).data as *mut libc::c_char)
                    .offset((iq1 * nbq1 + iq2 * nbq2 + iq3 * nbq3) as isize)
                    as *mut libc::c_float,
            );
            ic += 1;
            ic;
        }
        ggml_vec_scale_f32(nek1 as libc::c_int, S, scale);
        if masked {
            let mut i_0: int64_t = P;
            while i_0 < M {
                if i_0 > P + iq1 as libc::c_long {
                    *S.offset(i_0 as isize) = -::core::f32::INFINITY;
                }
                i_0 += 1;
                i_0;
            }
        }
        let mut max: libc::c_float = -::core::f32::INFINITY;
        ggml_vec_max_f32(M as libc::c_int, &mut max, S);
        let mut sum: ggml_float = 0.0f64;
        let mut scvt: [uint16_t; 4] = [0; 4];
        let mut sump: [ggml_float; 4] = [0.0f64, 0., 0., 0.];
        let mut i_1: libc::c_int = 0 as libc::c_int;
        while i_1 < Mup {
            let mut SS: *mut libc::c_float = S.offset(i_1 as isize);
            let mut j: libc::c_int = 0 as libc::c_int;
            while j < 4 as libc::c_int {
                if *SS.offset(j as isize) == -::core::f32::INFINITY {
                    *SS.offset(j as isize) = 0.0f32;
                } else {
                    let mut s: ggml_fp16_t = ({
                        ::core::mem::transmute::<
                            _,
                            [libc::c_short;
                                ::core::mem::size_of::<__v8hi>()
                                    / ::core::mem::size_of::<libc::c_short>()],
                        >(_mm_cvtps_ph(
                            _mm_setr_ps(
                                *SS.offset(j as isize) - max,
                                0 as libc::c_int as libc::c_float,
                                0 as libc::c_int as libc::c_float,
                                0 as libc::c_int as libc::c_float,
                            ),
                            0 as libc::c_int,
                        ))[0 as libc::c_int as usize] as libc::c_ushort
                    });
                    memcpy(
                        &mut *scvt.as_mut_ptr().offset(j as isize) as *mut uint16_t
                            as *mut libc::c_void,
                        &mut s as *mut ggml_fp16_t as *const libc::c_void,
                        ::core::mem::size_of::<uint16_t>() as libc::c_ulong,
                    );
                    let val: libc::c_float =
                        ggml_lookup_fp16_to_fp32(table_exp_f16[scvt[j as usize] as usize]);
                    sump[j as usize] += val as ggml_float;
                    *SS.offset(j as isize) = val;
                }
                j += 1;
                j;
            }
            i_1 += 4 as libc::c_int;
        }
        let mut i_2: libc::c_int = 0 as libc::c_int;
        while i_2 < 4 as libc::c_int {
            sum += sump[i_2 as usize];
            i_2 += 1;
            i_2;
        }
        sum = 1.0f64 / sum;
        ggml_vec_scale_f32(M as libc::c_int, S, sum as libc::c_float);
        let mut ic_0: int64_t = 0 as libc::c_int as int64_t;
        while ic_0 < nev1 {
            let i1_0: libc::c_int = iq1;
            let i2: libc::c_int = iq2;
            let i3: libc::c_int = iq3;
            ggml_vec_dot_f32(
                nek1 as libc::c_int,
                ((*dst).data as *mut libc::c_char).offset(
                    (ic_0 * nb0 as libc::c_long
                        + (i1_0 * nb1) as libc::c_long
                        + (i2 * nb2) as libc::c_long
                        + (i3 * nb3) as libc::c_long) as isize,
                ) as *mut libc::c_float,
                ((*v).data as *mut libc::c_char).offset(
                    (ic_0 * nbv1 as libc::c_long
                        + (i2 * nbv2) as libc::c_long
                        + (i3 * nbv3) as libc::c_long) as isize,
                ) as *mut libc::c_float,
                S,
            );
            ic_0 += 1;
            ic_0;
        }
        ir += 1;
        ir;
    }
}
unsafe extern "C" fn ggml_compute_forward_flash_attn_f16(
    mut params: *const ggml_compute_params,
    mut q: *const ggml_tensor,
    mut k: *const ggml_tensor,
    mut v: *const ggml_tensor,
    masked: bool,
    mut dst: *mut ggml_tensor,
) {
    let mut t0: int64_t = 0 as libc::c_int as int64_t;
    let neq0: int64_t = (*q).ne[0 as libc::c_int as usize];
    let neq1: int64_t = (*q).ne[1 as libc::c_int as usize];
    let neq2: int64_t = (*q).ne[2 as libc::c_int as usize];
    let neq3: int64_t = (*q).ne[3 as libc::c_int as usize];
    let nek0: int64_t = (*k).ne[0 as libc::c_int as usize];
    let nek1: int64_t = (*k).ne[1 as libc::c_int as usize];
    let nev1: int64_t = (*v).ne[1 as libc::c_int as usize];
    let ne0: int64_t = (*dst).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*dst).ne[1 as libc::c_int as usize];
    let nbk0: libc::c_int = (*k).nb[0 as libc::c_int as usize] as libc::c_int;
    let nbk1: libc::c_int = (*k).nb[1 as libc::c_int as usize] as libc::c_int;
    let nbk2: libc::c_int = (*k).nb[2 as libc::c_int as usize] as libc::c_int;
    let nbk3: libc::c_int = (*k).nb[3 as libc::c_int as usize] as libc::c_int;
    let nbq0: libc::c_int = (*q).nb[0 as libc::c_int as usize] as libc::c_int;
    let nbq1: libc::c_int = (*q).nb[1 as libc::c_int as usize] as libc::c_int;
    let nbq2: libc::c_int = (*q).nb[2 as libc::c_int as usize] as libc::c_int;
    let nbq3: libc::c_int = (*q).nb[3 as libc::c_int as usize] as libc::c_int;
    let nbv0: libc::c_int = (*v).nb[0 as libc::c_int as usize] as libc::c_int;
    let nbv1: libc::c_int = (*v).nb[1 as libc::c_int as usize] as libc::c_int;
    let nbv2: libc::c_int = (*v).nb[2 as libc::c_int as usize] as libc::c_int;
    let nbv3: libc::c_int = (*v).nb[3 as libc::c_int as usize] as libc::c_int;
    let nb0: libc::c_int = (*dst).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb1: libc::c_int = (*dst).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb2: libc::c_int = (*dst).nb[2 as libc::c_int as usize] as libc::c_int;
    let nb3: libc::c_int = (*dst).nb[3 as libc::c_int as usize] as libc::c_int;
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let D: int64_t = neq0;
    let N: int64_t = neq1;
    let P: int64_t = nek1 - N;
    let M: int64_t = P + N;
    let Mup: libc::c_int = ggml_up(M as libc::c_int, 4 as libc::c_int);
    if !(ne0 == D) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12292 as libc::c_int,
            b"ne0 == D\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne1 == N) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12293 as libc::c_int,
            b"ne1 == N\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(P >= 0 as libc::c_int as libc::c_long) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12294 as libc::c_int,
            b"P >= 0\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nbq0 as libc::c_ulong == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12296 as libc::c_int,
            b"nbq0 == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nbk0 as libc::c_ulong == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12297 as libc::c_int,
            b"nbk0 == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nbv0 as libc::c_ulong == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12298 as libc::c_int,
            b"nbv0 == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(neq0 == D) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12300 as libc::c_int,
            b"neq0 == D\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nek0 == D) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12301 as libc::c_int,
            b"nek0 == D\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nev1 == D) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12302 as libc::c_int,
            b"nev1 == D\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(neq1 == N) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12304 as libc::c_int,
            b"neq1 == N\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nek1 == N + P) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12305 as libc::c_int,
            b"nek1 == N + P\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nev1 == D) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12306 as libc::c_int,
            b"nev1 == D\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb0 as libc::c_ulong == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12309 as libc::c_int,
            b"nb0 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb0 <= nb1) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12310 as libc::c_int,
            b"nb0 <= nb1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb1 <= nb2) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12311 as libc::c_int,
            b"nb1 <= nb2\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb2 <= nb3) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12312 as libc::c_int,
            b"nb2 <= nb3\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint {
        return;
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint {
        return;
    }
    let nr: libc::c_int = (neq1 * neq2 * neq3) as libc::c_int;
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let scale: libc::c_float = 1.0f32 / sqrtf(D as libc::c_float);
    let mut ir: libc::c_int = ir0;
    while ir < ir1 {
        let iq3: libc::c_int = (ir as libc::c_long / (neq2 * neq1)) as libc::c_int;
        let iq2: libc::c_int =
            ((ir as libc::c_long - iq3 as libc::c_long * neq2 * neq1) / neq1) as libc::c_int;
        let iq1: libc::c_int = (ir as libc::c_long
            - iq3 as libc::c_long * neq2 * neq1
            - iq2 as libc::c_long * neq1) as libc::c_int;
        let mut S: *mut libc::c_float =
            ((*params).wdata as *mut libc::c_float).offset((ith as libc::c_ulong).wrapping_mul(
                ((2 as libc::c_int * Mup) as libc::c_ulong).wrapping_add(CACHE_LINE_SIZE_F32),
            ) as isize);
        let mut i: libc::c_int = M as libc::c_int;
        while i < Mup {
            *S.offset(i as isize) = -::core::f32::INFINITY;
            i += 1;
            i;
        }
        if 2 as libc::c_int > 2 as libc::c_int
            || nek1 % 2 as libc::c_int as libc::c_long != 0 as libc::c_int as libc::c_long
        {
            let mut ic: int64_t = 0 as libc::c_int as int64_t;
            while ic < nek1 {
                let ik3: libc::c_int = iq3;
                let ik2: libc::c_int = iq2;
                let ik1: libc::c_int = ic as libc::c_int;
                let i1: libc::c_int = ik1;
                ggml_vec_dot_f16(
                    neq0 as libc::c_int,
                    S.offset(i1 as isize),
                    ((*k).data as *mut libc::c_char)
                        .offset((ik1 * nbk1 + ik2 * nbk2 + ik3 * nbk3) as isize)
                        as *mut ggml_fp16_t,
                    ((*q).data as *mut libc::c_char)
                        .offset((iq1 * nbq1 + iq2 * nbq2 + iq3 * nbq3) as isize)
                        as *mut ggml_fp16_t,
                );
                ic += 1;
                ic;
            }
        } else {
            let mut ic_0: int64_t = 0 as libc::c_int as int64_t;
            while ic_0 < nek1 {
                let ik3_0: libc::c_int = iq3;
                let ik2_0: libc::c_int = iq2;
                let ik1_0: libc::c_int = ic_0 as libc::c_int;
                let i1_0: libc::c_int = ik1_0;
                ggml_vec_dot_f16_unroll(
                    neq0 as libc::c_int,
                    nbk1,
                    S.offset(i1_0 as isize),
                    ((*k).data as *mut libc::c_char)
                        .offset((ik1_0 * nbk1 + ik2_0 * nbk2 + ik3_0 * nbk3) as isize)
                        as *mut libc::c_void,
                    ((*q).data as *mut libc::c_char)
                        .offset((iq1 * nbq1 + iq2 * nbq2 + iq3 * nbq3) as isize)
                        as *mut ggml_fp16_t,
                );
                ic_0 += 2 as libc::c_int as libc::c_long;
            }
        }
        ggml_vec_scale_f32(nek1 as libc::c_int, S, scale);
        if masked {
            let mut i_0: int64_t = P;
            while i_0 < M {
                if i_0 > P + iq1 as libc::c_long {
                    *S.offset(i_0 as isize) = -::core::f32::INFINITY;
                }
                i_0 += 1;
                i_0;
            }
        }
        let mut max: libc::c_float = -::core::f32::INFINITY;
        ggml_vec_max_f32(M as libc::c_int, &mut max, S);
        let mut sum: ggml_float = 0.0f64;
        let mut scvt: [uint16_t; 4] = [0; 4];
        let mut sump: [ggml_float; 4] = [0.0f64, 0., 0., 0.];
        let mut i_1: libc::c_int = 0 as libc::c_int;
        while i_1 < Mup {
            let mut SS: *mut libc::c_float = S.offset(i_1 as isize);
            let mut j: libc::c_int = 0 as libc::c_int;
            while j < 4 as libc::c_int {
                if *SS.offset(j as isize) == -::core::f32::INFINITY {
                    *SS.offset(j as isize) = 0.0f32;
                } else {
                    let mut s: ggml_fp16_t = ({
                        ::core::mem::transmute::<
                            _,
                            [libc::c_short;
                                ::core::mem::size_of::<__v8hi>()
                                    / ::core::mem::size_of::<libc::c_short>()],
                        >(_mm_cvtps_ph(
                            _mm_setr_ps(
                                *SS.offset(j as isize) - max,
                                0 as libc::c_int as libc::c_float,
                                0 as libc::c_int as libc::c_float,
                                0 as libc::c_int as libc::c_float,
                            ),
                            0 as libc::c_int,
                        ))[0 as libc::c_int as usize] as libc::c_ushort
                    });
                    memcpy(
                        &mut *scvt.as_mut_ptr().offset(j as isize) as *mut uint16_t
                            as *mut libc::c_void,
                        &mut s as *mut ggml_fp16_t as *const libc::c_void,
                        ::core::mem::size_of::<uint16_t>() as libc::c_ulong,
                    );
                    let val: libc::c_float =
                        ggml_lookup_fp16_to_fp32(table_exp_f16[scvt[j as usize] as usize]);
                    sump[j as usize] += val as ggml_float;
                    *SS.offset(j as isize) = val;
                }
                j += 1;
                j;
            }
            i_1 += 4 as libc::c_int;
        }
        let mut i_2: libc::c_int = 0 as libc::c_int;
        while i_2 < 4 as libc::c_int {
            sum += sump[i_2 as usize];
            i_2 += 1;
            i_2;
        }
        sum = 1.0f64 / sum;
        ggml_vec_scale_f32(M as libc::c_int, S, sum as libc::c_float);
        let mut S16: *mut ggml_fp16_t = ((*params).wdata as *mut libc::c_float)
            .offset((ith as libc::c_ulong).wrapping_mul(
                ((2 as libc::c_int * Mup) as libc::c_ulong).wrapping_add(CACHE_LINE_SIZE_F32),
            ) as isize)
            .offset(Mup as isize) as *mut ggml_fp16_t;
        let mut i_3: int64_t = 0 as libc::c_int as int64_t;
        while i_3 < M {
            *S16.offset(i_3 as isize) = ({
                ::core::mem::transmute::<
                    _,
                    [libc::c_short;
                        ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
                >(_mm_cvtps_ph(
                    _mm_setr_ps(
                        *S.offset(i_3 as isize),
                        0 as libc::c_int as libc::c_float,
                        0 as libc::c_int as libc::c_float,
                        0 as libc::c_int as libc::c_float,
                    ),
                    0 as libc::c_int,
                ))[0 as libc::c_int as usize] as libc::c_ushort
            });
            i_3 += 1;
            i_3;
        }
        if 2 as libc::c_int == 1 as libc::c_int
            || nev1 % 2 as libc::c_int as libc::c_long != 0 as libc::c_int as libc::c_long
        {
            let mut ic_1: int64_t = 0 as libc::c_int as int64_t;
            while ic_1 < nev1 {
                let i1_1: libc::c_int = iq1;
                let i2: libc::c_int = iq2;
                let i3: libc::c_int = iq3;
                ggml_vec_dot_f16(
                    nek1 as libc::c_int,
                    ((*dst).data as *mut libc::c_char).offset(
                        (ic_1 * nb0 as libc::c_long
                            + (i1_1 * nb1) as libc::c_long
                            + (i2 * nb2) as libc::c_long
                            + (i3 * nb3) as libc::c_long) as isize,
                    ) as *mut libc::c_float,
                    ((*v).data as *mut libc::c_char).offset(
                        (ic_1 * nbv1 as libc::c_long
                            + (i2 * nbv2) as libc::c_long
                            + (i3 * nbv3) as libc::c_long) as isize,
                    ) as *mut ggml_fp16_t,
                    S16,
                );
                ic_1 += 1;
                ic_1;
            }
        } else {
            let mut ic_2: int64_t = 0 as libc::c_int as int64_t;
            while ic_2 < nev1 {
                let i1_2: libc::c_int = iq1;
                let i2_0: libc::c_int = iq2;
                let i3_0: libc::c_int = iq3;
                ggml_vec_dot_f16_unroll(
                    nek1 as libc::c_int,
                    nbv1,
                    ((*dst).data as *mut libc::c_char).offset(
                        (ic_2 * nb0 as libc::c_long
                            + (i1_2 * nb1) as libc::c_long
                            + (i2_0 * nb2) as libc::c_long
                            + (i3_0 * nb3) as libc::c_long) as isize,
                    ) as *mut libc::c_float,
                    ((*v).data as *mut libc::c_char).offset(
                        (ic_2 * nbv1 as libc::c_long
                            + (i2_0 * nbv2) as libc::c_long
                            + (i3_0 * nbv3) as libc::c_long) as isize,
                    ) as *mut libc::c_void,
                    S16,
                );
                ic_2 += 2 as libc::c_int as libc::c_long;
            }
        }
        ir += 1;
        ir;
    }
}
unsafe extern "C" fn ggml_compute_forward_flash_attn(
    mut params: *const ggml_compute_params,
    mut q: *const ggml_tensor,
    mut k: *const ggml_tensor,
    mut v: *const ggml_tensor,
    masked: bool,
    mut dst: *mut ggml_tensor,
) {
    match (*q).type_0 as libc::c_uint {
        1 => {
            ggml_compute_forward_flash_attn_f16(params, q, k, v, masked, dst);
        }
        0 => {
            ggml_compute_forward_flash_attn_f32(params, q, k, v, masked, dst);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    12496 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_flash_ff_f16(
    mut params: *const ggml_compute_params,
    mut a: *const ggml_tensor,
    mut b0: *const ggml_tensor,
    mut b1: *const ggml_tensor,
    mut c0: *const ggml_tensor,
    mut c1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    let mut t0: int64_t = 0 as libc::c_int as int64_t;
    let nea0: int64_t = (*a).ne[0 as libc::c_int as usize];
    let nea1: int64_t = (*a).ne[1 as libc::c_int as usize];
    let nea2: int64_t = (*a).ne[2 as libc::c_int as usize];
    let nea3: int64_t = (*a).ne[3 as libc::c_int as usize];
    let neb00: int64_t = (*b0).ne[0 as libc::c_int as usize];
    let neb01: int64_t = (*b0).ne[1 as libc::c_int as usize];
    let neb10: int64_t = (*b1).ne[0 as libc::c_int as usize];
    let neb11: int64_t = (*b1).ne[1 as libc::c_int as usize];
    let nec00: int64_t = (*c0).ne[0 as libc::c_int as usize];
    let nec01: int64_t = (*c0).ne[1 as libc::c_int as usize];
    let nec10: int64_t = (*c1).ne[0 as libc::c_int as usize];
    let nec11: int64_t = (*c1).ne[1 as libc::c_int as usize];
    let ne0: int64_t = (*dst).ne[0 as libc::c_int as usize];
    let ne1: int64_t = (*dst).ne[1 as libc::c_int as usize];
    let ne2: int64_t = (*dst).ne[2 as libc::c_int as usize];
    let nba0: libc::c_int = (*a).nb[0 as libc::c_int as usize] as libc::c_int;
    let nba1: libc::c_int = (*a).nb[1 as libc::c_int as usize] as libc::c_int;
    let nba2: libc::c_int = (*a).nb[2 as libc::c_int as usize] as libc::c_int;
    let nba3: libc::c_int = (*a).nb[3 as libc::c_int as usize] as libc::c_int;
    let nbb00: libc::c_int = (*b0).nb[0 as libc::c_int as usize] as libc::c_int;
    let nbb01: libc::c_int = (*b0).nb[1 as libc::c_int as usize] as libc::c_int;
    let nbb02: libc::c_int = (*b0).nb[2 as libc::c_int as usize] as libc::c_int;
    let nbb03: libc::c_int = (*b0).nb[3 as libc::c_int as usize] as libc::c_int;
    let nbb10: libc::c_int = (*b1).nb[0 as libc::c_int as usize] as libc::c_int;
    let nbc00: libc::c_int = (*c0).nb[0 as libc::c_int as usize] as libc::c_int;
    let nbc01: libc::c_int = (*c0).nb[1 as libc::c_int as usize] as libc::c_int;
    let nbc02: libc::c_int = (*c0).nb[2 as libc::c_int as usize] as libc::c_int;
    let nbc03: libc::c_int = (*c0).nb[3 as libc::c_int as usize] as libc::c_int;
    let nbc10: libc::c_int = (*c1).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb0: libc::c_int = (*dst).nb[0 as libc::c_int as usize] as libc::c_int;
    let nb1: libc::c_int = (*dst).nb[1 as libc::c_int as usize] as libc::c_int;
    let nb2: libc::c_int = (*dst).nb[2 as libc::c_int as usize] as libc::c_int;
    let nb3: libc::c_int = (*dst).nb[3 as libc::c_int as usize] as libc::c_int;
    let ith: libc::c_int = (*params).ith;
    let nth: libc::c_int = (*params).nth;
    let D: int64_t = nea0;
    let M: int64_t = neb01;
    if !(ne0 == nea0) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12581 as libc::c_int,
            b"ne0 == nea0\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne1 == nea1) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12582 as libc::c_int,
            b"ne1 == nea1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(ne2 == nea2) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12583 as libc::c_int,
            b"ne2 == nea2\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nba0 as libc::c_ulong == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12585 as libc::c_int,
            b"nba0 == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nbb00 as libc::c_ulong == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12586 as libc::c_int,
            b"nbb00 == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nbb10 as libc::c_ulong == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12587 as libc::c_int,
            b"nbb10 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nbc00 as libc::c_ulong == ::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12588 as libc::c_int,
            b"nbc00 == sizeof(ggml_fp16_t)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nbc10 as libc::c_ulong == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12589 as libc::c_int,
            b"nbc10 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(neb00 == D) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12591 as libc::c_int,
            b"neb00 == D\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(neb01 == M) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12592 as libc::c_int,
            b"neb01 == M\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(neb10 == M) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12593 as libc::c_int,
            b"neb10 == M\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(neb11 == 1 as libc::c_int as libc::c_long) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12594 as libc::c_int,
            b"neb11 == 1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nec00 == M) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12596 as libc::c_int,
            b"nec00 == M\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nec01 == D) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12597 as libc::c_int,
            b"nec01 == D\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nec10 == D) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12598 as libc::c_int,
            b"nec10 == D\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nec11 == 1 as libc::c_int as libc::c_long) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12599 as libc::c_int,
            b"nec11 == 1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb0 as libc::c_ulong == ::core::mem::size_of::<libc::c_float>() as libc::c_ulong) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12602 as libc::c_int,
            b"nb0 == sizeof(float)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb0 <= nb1) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12603 as libc::c_int,
            b"nb0 <= nb1\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb1 <= nb2) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12604 as libc::c_int,
            b"nb1 <= nb2\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if !(nb2 <= nb3) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12605 as libc::c_int,
            b"nb2 <= nb3\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint {
        return;
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint {
        return;
    }
    let nr: libc::c_int = (nea1 * nea2 * nea3) as libc::c_int;
    let dr: libc::c_int = (nr + nth - 1 as libc::c_int) / nth;
    let ir0: libc::c_int = dr * ith;
    let ir1: libc::c_int = if ir0 + dr < nr { ir0 + dr } else { nr };
    let mut ir: libc::c_int = ir0;
    while ir < ir1 {
        let ia3: libc::c_int = (ir as libc::c_long / (nea2 * nea1)) as libc::c_int;
        let ia2: libc::c_int =
            ((ir as libc::c_long - ia3 as libc::c_long * nea2 * nea1) / nea1) as libc::c_int;
        let ia1: libc::c_int = (ir as libc::c_long
            - ia3 as libc::c_long * nea2 * nea1
            - ia2 as libc::c_long * nea1) as libc::c_int;
        let mut S: *mut libc::c_float = ((*params).wdata as *mut libc::c_float).offset(
            (ith as libc::c_ulong).wrapping_mul(
                ((2 as libc::c_int as libc::c_long * M) as libc::c_ulong)
                    .wrapping_add(CACHE_LINE_SIZE_F32),
            ) as isize,
        );
        let mut ic: int64_t = 0 as libc::c_int as int64_t;
        while ic < neb01 {
            let ib03: libc::c_int = ia3;
            let ib02: libc::c_int = ia2;
            let ib01: libc::c_int = ic as libc::c_int;
            let i1: libc::c_int = ib01;
            ggml_vec_dot_f16(
                nea0 as libc::c_int,
                S.offset(i1 as isize),
                ((*b0).data as *mut libc::c_char)
                    .offset((ib01 * nbb01 + ib02 * nbb02 + ib03 * nbb03) as isize)
                    as *mut ggml_fp16_t,
                ((*a).data as *mut libc::c_char)
                    .offset((ia1 * nba1 + ia2 * nba2 + ia3 * nba3) as isize)
                    as *mut ggml_fp16_t,
            );
            ic += 1;
            ic;
        }
        ggml_vec_add_f32(neb01 as libc::c_int, S, S, (*b1).data as *mut libc::c_float);
        let mut S16: *mut ggml_fp16_t = ((*params).wdata as *mut libc::c_float)
            .offset(
                (ith as libc::c_ulong).wrapping_mul(
                    ((2 as libc::c_int as libc::c_long * M) as libc::c_ulong)
                        .wrapping_add(CACHE_LINE_SIZE_F32),
                ) as isize,
            )
            .offset(M as isize) as *mut ggml_fp16_t;
        let mut i: int64_t = 0 as libc::c_int as int64_t;
        while i < M {
            *S16.offset(i as isize) = ({
                ::core::mem::transmute::<
                    _,
                    [libc::c_short;
                        ::core::mem::size_of::<__v8hi>() / ::core::mem::size_of::<libc::c_short>()],
                >(_mm_cvtps_ph(
                    _mm_setr_ps(
                        *S.offset(i as isize),
                        0 as libc::c_int as libc::c_float,
                        0 as libc::c_int as libc::c_float,
                        0 as libc::c_int as libc::c_float,
                    ),
                    0 as libc::c_int,
                ))[0 as libc::c_int as usize] as libc::c_ushort
            });
            i += 1;
            i;
        }
        ggml_vec_gelu_f16(neb01 as libc::c_int, S16, S16);
        let i1_0: libc::c_int = ia1;
        let i2: libc::c_int = ia2;
        let i3: libc::c_int = ia3;
        let mut ic_0: int64_t = 0 as libc::c_int as int64_t;
        while ic_0 < nec01 {
            ggml_vec_dot_f16(
                neb01 as libc::c_int,
                ((*dst).data as *mut libc::c_char).offset(
                    (ic_0 * nb0 as libc::c_long
                        + (i1_0 * nb1) as libc::c_long
                        + (i2 * nb2) as libc::c_long
                        + (i3 * nb3) as libc::c_long) as isize,
                ) as *mut libc::c_float,
                ((*c0).data as *mut libc::c_char).offset(
                    (ic_0 * nbc01 as libc::c_long
                        + (i2 * nbc02) as libc::c_long
                        + (i3 * nbc03) as libc::c_long) as isize,
                ) as *mut ggml_fp16_t,
                S16,
            );
            ic_0 += 1;
            ic_0;
        }
        ggml_vec_add_f32(
            nec01 as libc::c_int,
            ((*dst).data as *mut libc::c_char).offset((i1_0 * nb1 + i2 * nb2 + i3 * nb3) as isize)
                as *mut libc::c_float,
            ((*dst).data as *mut libc::c_char).offset((i1_0 * nb1 + i2 * nb2 + i3 * nb3) as isize)
                as *mut libc::c_float,
            (*c1).data as *mut libc::c_float,
        );
        ir += 1;
        ir;
    }
}
unsafe extern "C" fn ggml_compute_forward_flash_ff(
    mut params: *const ggml_compute_params,
    mut a: *const ggml_tensor,
    mut b0: *const ggml_tensor,
    mut b1: *const ggml_tensor,
    mut c0: *const ggml_tensor,
    mut c1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
) {
    match (*b0).type_0 as libc::c_uint {
        1 => {
            ggml_compute_forward_flash_ff_f16(params, a, b0, b1, c0, c1, dst);
        }
        0 => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    12698 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    12702 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_map_unary_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
    fun: ggml_unary_op_f32_t,
) {
    if !ggml_are_same_shape(src0, dst) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12714 as libc::c_int,
            b"ggml_are_same_shape(src0, dst)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let n: libc::c_int = ggml_nrows(src0);
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        fun.expect("non-null function pointer")(
            nc,
            ((*dst).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*dst).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
            ((*src0).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*src0).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
        );
        i += 1;
        i;
    }
}
unsafe extern "C" fn ggml_compute_forward_map_unary(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
    fun: ggml_unary_op_f32_t,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_map_unary_f32(params, src0, dst, fun);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    12746 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward_map_binary_f32(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
    fun: ggml_binary_op_f32_t,
) {
    if (*params).type_0 as libc::c_uint == GGML_TASK_INIT as libc::c_int as libc::c_uint
        || (*params).type_0 as libc::c_uint == GGML_TASK_FINALIZE as libc::c_int as libc::c_uint
    {
        return;
    }
    let n: libc::c_int = ggml_nrows(src0);
    let nc: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < n {
        fun.expect("non-null function pointer")(
            nc,
            ((*dst).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*dst).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
            ((*src0).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*src0).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
            ((*src1).data as *mut libc::c_char).offset(
                (i as libc::c_ulong).wrapping_mul((*src1).nb[1 as libc::c_int as usize]) as isize,
            ) as *mut libc::c_float,
        );
        i += 1;
        i;
    }
}
unsafe extern "C" fn ggml_compute_forward_map_binary(
    mut params: *const ggml_compute_params,
    mut src0: *const ggml_tensor,
    mut src1: *const ggml_tensor,
    mut dst: *mut ggml_tensor,
    fun: ggml_binary_op_f32_t,
) {
    match (*src0).type_0 as libc::c_uint {
        0 => {
            ggml_compute_forward_map_binary_f32(params, src0, src1, dst, fun);
        }
        _ => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    12795 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
    };
}
unsafe extern "C" fn ggml_compute_forward(
    mut params: *mut ggml_compute_params,
    mut tensor: *mut ggml_tensor,
) {
    if params.is_null() {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            12803 as libc::c_int,
            b"params\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    match (*tensor).op as libc::c_uint {
        1 => {
            ggml_compute_forward_dup(params, (*tensor).src0, tensor);
        }
        2 => {
            ggml_compute_forward_add(params, (*tensor).src0, (*tensor).src1, tensor);
        }
        3 => {
            ggml_compute_forward_add1(params, (*tensor).src0, (*tensor).src1, tensor);
        }
        4 => {
            ggml_compute_forward_acc(
                params,
                (*tensor).src0,
                (*tensor).src1,
                (*tensor).opt[0 as libc::c_int as usize],
                tensor,
            );
        }
        5 => {
            ggml_compute_forward_sub(params, (*tensor).src0, (*tensor).src1, tensor);
        }
        6 => {
            ggml_compute_forward_mul(params, (*tensor).src0, (*tensor).src1, tensor);
        }
        7 => {
            ggml_compute_forward_div(params, (*tensor).src0, (*tensor).src1, tensor);
        }
        8 => {
            ggml_compute_forward_sqr(params, (*tensor).src0, tensor);
        }
        9 => {
            ggml_compute_forward_sqrt(params, (*tensor).src0, tensor);
        }
        10 => {
            ggml_compute_forward_log(params, (*tensor).src0, tensor);
        }
        11 => {
            ggml_compute_forward_sum(params, (*tensor).src0, tensor);
        }
        12 => {
            ggml_compute_forward_sum_rows(params, (*tensor).src0, tensor);
        }
        13 => {
            ggml_compute_forward_mean(params, (*tensor).src0, tensor);
        }
        14 => {
            ggml_compute_forward_repeat(params, (*tensor).src0, tensor);
        }
        15 => {
            ggml_compute_forward_abs(params, (*tensor).src0, tensor);
        }
        16 => {
            ggml_compute_forward_sgn(params, (*tensor).src0, tensor);
        }
        17 => {
            ggml_compute_forward_neg(params, (*tensor).src0, tensor);
        }
        18 => {
            ggml_compute_forward_step(params, (*tensor).src0, tensor);
        }
        19 => {
            ggml_compute_forward_relu(params, (*tensor).src0, tensor);
        }
        20 => {
            ggml_compute_forward_gelu(params, (*tensor).src0, tensor);
        }
        21 => {
            ggml_compute_forward_silu(params, (*tensor).src0, tensor);
        }
        22 => {
            ggml_compute_forward_silu_back(params, (*tensor).src0, (*tensor).src1, tensor);
        }
        23 => {
            ggml_compute_forward_norm(params, (*tensor).src0, tensor);
        }
        24 => {
            ggml_compute_forward_rms_norm(params, (*tensor).src0, tensor);
        }
        25 => {
            ggml_compute_forward_rms_norm_back(params, (*tensor).src0, (*tensor).src1, tensor);
        }
        26 => {
            ggml_compute_forward_mul_mat(params, (*tensor).src0, (*tensor).src1, tensor);
        }
        27 => {
            ggml_compute_forward_scale(params, (*tensor).src0, (*tensor).src1, tensor);
        }
        28 => {
            ggml_compute_forward_set(
                params,
                (*tensor).src0,
                (*tensor).src1,
                (*tensor).opt[0 as libc::c_int as usize],
                tensor,
            );
        }
        29 => {
            ggml_compute_forward_cpy(params, (*tensor).src0, tensor);
        }
        30 => {
            ggml_compute_forward_cont(params, (*tensor).src0, tensor);
        }
        31 => {
            ggml_compute_forward_reshape(params, (*tensor).src0, tensor);
        }
        32 => {
            ggml_compute_forward_view(params, (*tensor).src0);
        }
        33 => {
            ggml_compute_forward_permute(params, (*tensor).src0);
        }
        34 => {
            ggml_compute_forward_transpose(params, (*tensor).src0);
        }
        35 => {
            ggml_compute_forward_get_rows(params, (*tensor).src0, (*tensor).src1, tensor);
        }
        36 => {
            ggml_compute_forward_get_rows_back(
                params,
                (*tensor).src0,
                (*tensor).src1,
                (*tensor).opt[0 as libc::c_int as usize],
                tensor,
            );
        }
        37 => {
            ggml_compute_forward_diag(params, (*tensor).src0, tensor);
        }
        38 => {
            ggml_compute_forward_diag_mask_inf(params, (*tensor).src0, (*tensor).src1, tensor);
        }
        39 => {
            ggml_compute_forward_diag_mask_zero(params, (*tensor).src0, (*tensor).src1, tensor);
        }
        40 => {
            ggml_compute_forward_soft_max(params, (*tensor).src0, tensor);
        }
        41 => {
            ggml_compute_forward_rope(params, (*tensor).src0, (*tensor).src1, tensor);
        }
        42 => {
            ggml_compute_forward_rope_back(params, (*tensor).src0, (*tensor).src1, tensor);
        }
        43 => {
            ggml_compute_forward_alibi(params, (*tensor).src0, (*tensor).src1, tensor);
        }
        44 => {
            ggml_compute_forward_clamp(params, (*tensor).src0, (*tensor).src1, tensor);
        }
        45 => {
            ggml_compute_forward_conv_1d_1s(params, (*tensor).src0, (*tensor).src1, tensor);
        }
        46 => {
            ggml_compute_forward_conv_1d_2s(params, (*tensor).src0, (*tensor).src1, tensor);
        }
        47 => {
            let mut t: int32_t =
                ggml_get_i32_1d((*tensor).opt[1 as libc::c_int as usize], 0 as libc::c_int);
            if !(t == 0 as libc::c_int || t == 1 as libc::c_int) {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    12993 as libc::c_int,
                    b"t == 0 || t == 1\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            let mut masked: bool = t != 0 as libc::c_int;
            ggml_compute_forward_flash_attn(
                params,
                (*tensor).src0,
                (*tensor).src1,
                (*tensor).opt[0 as libc::c_int as usize],
                masked,
                tensor,
            );
        }
        48 => {
            ggml_compute_forward_flash_ff(
                params,
                (*tensor).src0,
                (*tensor).src1,
                (*tensor).opt[0 as libc::c_int as usize],
                (*tensor).opt[1 as libc::c_int as usize],
                (*tensor).opt[2 as libc::c_int as usize],
                tensor,
            );
        }
        49 => {
            let fun: ggml_unary_op_f32_t =
                *((*(*tensor).opt[0 as libc::c_int as usize]).data as *mut ggml_unary_op_f32_t);
            ggml_compute_forward_map_unary(params, (*tensor).src0, tensor, fun);
        }
        50 => {
            let fun_0: ggml_binary_op_f32_t =
                *((*(*tensor).opt[0 as libc::c_int as usize]).data as *mut ggml_binary_op_f32_t);
            ggml_compute_forward_map_binary(params, (*tensor).src0, (*tensor).src1, tensor, fun_0);
        }
        51 => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    13019 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
        0 | _ => {}
    };
}
unsafe extern "C" fn ggml_compute_backward(
    mut ctx: *mut ggml_context,
    mut tensor: *mut ggml_tensor,
    mut inplace: bool,
) {
    let mut src0: *mut ggml_tensor = (*tensor).src0;
    let mut src1: *mut ggml_tensor = (*tensor).src1;
    match (*tensor).op as libc::c_uint {
        1 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add_impl(ctx, (*src0).grad, (*tensor).grad, inplace);
            }
        }
        2 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add_impl(ctx, (*src0).grad, (*tensor).grad, inplace);
            }
            if !((*src1).grad).is_null() {
                (*src1).grad = ggml_add_impl(ctx, (*src1).grad, (*tensor).grad, inplace);
            }
        }
        3 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add_impl(ctx, (*src0).grad, (*tensor).grad, inplace);
            }
            if !((*src1).grad).is_null() {
                (*src1).grad =
                    ggml_add_impl(ctx, (*src1).grad, ggml_mean(ctx, (*tensor).grad), inplace);
            }
        }
        4 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add_impl(ctx, (*src0).grad, (*tensor).grad, inplace);
            }
            if !((*src1).grad).is_null() {
                if !(ggml_nelements((*tensor).opt[0 as libc::c_int as usize])
                    == 5 as libc::c_int as libc::c_long)
                {
                    fprintf(
                        stderr,
                        b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                        b"ggml.c\0" as *const u8 as *const libc::c_char,
                        13064 as libc::c_int,
                        b"ggml_nelements(tensor->opt[0]) == 5\0" as *const u8
                            as *const libc::c_char,
                    );
                    abort();
                }
                if !((*(*tensor).opt[0 as libc::c_int as usize]).type_0 as libc::c_uint
                    == GGML_TYPE_I32 as libc::c_int as libc::c_uint)
                {
                    fprintf(
                        stderr,
                        b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                        b"ggml.c\0" as *const u8 as *const libc::c_char,
                        13065 as libc::c_int,
                        b"tensor->opt[0]->type == GGML_TYPE_I32\0" as *const u8
                            as *const libc::c_char,
                    );
                    abort();
                }
                let nb1: size_t = *((*(*tensor).opt[0 as libc::c_int as usize]).data
                    as *mut int32_t)
                    .offset(0 as libc::c_int as isize) as size_t;
                let nb2: size_t = *((*(*tensor).opt[0 as libc::c_int as usize]).data
                    as *mut int32_t)
                    .offset(1 as libc::c_int as isize) as size_t;
                let nb3: size_t = *((*(*tensor).opt[0 as libc::c_int as usize]).data
                    as *mut int32_t)
                    .offset(2 as libc::c_int as isize) as size_t;
                let offset: size_t =
                    *((*(*tensor).opt[0 as libc::c_int as usize]).data as *mut int32_t)
                        .offset(3 as libc::c_int as isize) as size_t;
                let mut tensor_grad_view: *mut ggml_tensor = ggml_view_4d(
                    ctx,
                    (*tensor).grad,
                    (*(*src1).grad).ne[0 as libc::c_int as usize],
                    (*(*src1).grad).ne[1 as libc::c_int as usize],
                    (*(*src1).grad).ne[2 as libc::c_int as usize],
                    (*(*src1).grad).ne[3 as libc::c_int as usize],
                    nb1,
                    nb2,
                    nb3,
                    offset,
                );
                (*src1).grad = ggml_add_impl(
                    ctx,
                    (*src1).grad,
                    ggml_reshape(ctx, ggml_cont(ctx, tensor_grad_view), (*src1).grad),
                    inplace,
                );
            }
        }
        5 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add_impl(ctx, (*src0).grad, (*tensor).grad, inplace);
            }
            if !((*src1).grad).is_null() {
                (*src1).grad = ggml_sub_impl(ctx, (*src1).grad, (*tensor).grad, inplace);
            }
        }
        6 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add_impl(
                    ctx,
                    (*src0).grad,
                    ggml_mul(ctx, src1, (*tensor).grad),
                    inplace,
                );
            }
            if !((*src1).grad).is_null() {
                (*src1).grad = ggml_add_impl(
                    ctx,
                    (*src1).grad,
                    ggml_mul(ctx, src0, (*tensor).grad),
                    inplace,
                );
            }
        }
        7 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add_impl(
                    ctx,
                    (*src0).grad,
                    ggml_div(ctx, (*tensor).grad, src1),
                    inplace,
                );
            }
            if !((*src1).grad).is_null() {
                (*src1).grad = ggml_sub_impl(
                    ctx,
                    (*src1).grad,
                    ggml_mul(ctx, (*tensor).grad, ggml_div(ctx, tensor, src1)),
                    inplace,
                );
            }
        }
        8 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add_impl(
                    ctx,
                    (*src0).grad,
                    ggml_scale(
                        ctx,
                        ggml_mul(ctx, src0, (*tensor).grad),
                        ggml_new_f32(ctx, 2.0f32),
                    ),
                    inplace,
                );
            }
        }
        9 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add_impl(
                    ctx,
                    (*src0).grad,
                    ggml_mul(
                        ctx,
                        (*tensor).grad,
                        ggml_div(
                            ctx,
                            ggml_repeat(ctx, ggml_new_f32(ctx, 0.5f32), tensor),
                            tensor,
                        ),
                    ),
                    inplace,
                );
            }
        }
        10 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add_impl(
                    ctx,
                    (*src0).grad,
                    ggml_div(ctx, (*tensor).grad, src0),
                    inplace,
                );
            }
        }
        11 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add1_impl(ctx, (*src0).grad, (*tensor).grad, inplace);
            }
        }
        12 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add_impl(
                    ctx,
                    (*src0).grad,
                    ggml_repeat(ctx, (*tensor).grad, (*src0).grad),
                    inplace,
                );
            }
        }
        13 => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    13195 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
        14 => {
            if !((*src0).grad).is_null() {
                if !((*src0).n_dims == 1 as libc::c_int || (*src0).n_dims == 2 as libc::c_int) {
                    fprintf(
                        stderr,
                        b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                        b"ggml.c\0" as *const u8 as *const libc::c_char,
                        13201 as libc::c_int,
                        b"src0->n_dims == 1 || src0->n_dims == 2\0" as *const u8
                            as *const libc::c_char,
                    );
                    abort();
                }
                let nc: libc::c_int = (*tensor).ne[0 as libc::c_int as usize] as libc::c_int;
                let nr: libc::c_int = (*tensor).ne[1 as libc::c_int as usize] as libc::c_int;
                let nc0: libc::c_int = (*src0).ne[0 as libc::c_int as usize] as libc::c_int;
                let nr0: libc::c_int = (*src0).ne[1 as libc::c_int as usize] as libc::c_int;
                let ncr: libc::c_int = nc / nc0;
                let nrr: libc::c_int = nr / nr0;
                let mut ne: [int64_t; 4] = [
                    nc0 as int64_t,
                    ncr as int64_t,
                    nr0 as int64_t,
                    nrr as int64_t,
                ];
                let mut F00: *mut ggml_tensor = (*tensor).grad;
                let mut F01: *mut ggml_tensor = ggml_reshape(
                    ctx,
                    F00,
                    ggml_new_tensor(
                        ctx,
                        (*(*tensor).grad).type_0,
                        4 as libc::c_int,
                        ne.as_mut_ptr(),
                    ),
                );
                let mut F02: *mut ggml_tensor = ggml_permute(
                    ctx,
                    F01,
                    0 as libc::c_int,
                    2 as libc::c_int,
                    1 as libc::c_int,
                    3 as libc::c_int,
                );
                let mut F03: *mut ggml_tensor = ggml_cont(ctx, F02);
                let mut F04: *mut ggml_tensor =
                    ggml_reshape_2d(ctx, F03, (nc0 * nr0) as int64_t, (ncr * nrr) as int64_t);
                let mut F05: *mut ggml_tensor = ggml_transpose(ctx, F04);
                let mut F06: *mut ggml_tensor = ggml_cont(ctx, F05);
                let mut F07: *mut ggml_tensor = ggml_sum_rows(ctx, F06);
                let mut F08: *mut ggml_tensor = ggml_transpose(ctx, F07);
                let mut F09: *mut ggml_tensor = ggml_cont(ctx, F08);
                let mut F10: *mut ggml_tensor = ggml_reshape(ctx, F09, (*src0).grad);
                (*src0).grad = ggml_add_impl(ctx, (*src0).grad, F10, inplace);
            }
        }
        15 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add_impl(
                    ctx,
                    (*src0).grad,
                    ggml_mul(ctx, ggml_sgn(ctx, src0), (*tensor).grad),
                    inplace,
                );
            }
        }
        16 => {
            !((*src0).grad).is_null();
        }
        17 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_sub_impl(ctx, (*src0).grad, (*tensor).grad, inplace);
            }
        }
        18 => {
            !((*src0).grad).is_null();
        }
        19 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_sub_impl(
                    ctx,
                    (*src0).grad,
                    ggml_mul(ctx, ggml_step(ctx, src0), (*tensor).grad),
                    inplace,
                );
            }
        }
        20 => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    13283 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
        43 => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    13287 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
        44 => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    13291 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
        21 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add_impl(
                    ctx,
                    (*src0).grad,
                    ggml_silu_back(ctx, src0, (*tensor).grad),
                    inplace,
                );
            }
        }
        22 => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    13305 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
        23 => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    13309 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
        24 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add_impl(
                    ctx,
                    (*src0).grad,
                    ggml_rms_norm_back(ctx, src0, (*tensor).grad),
                    inplace,
                );
            }
        }
        25 => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    13323 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
        26 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add_impl(
                    ctx,
                    (*src0).grad,
                    ggml_cont(
                        ctx,
                        ggml_transpose(
                            ctx,
                            ggml_mul_mat(
                                ctx,
                                ggml_cont(ctx, ggml_transpose(ctx, (*tensor).grad)),
                                ggml_cont(ctx, ggml_transpose(ctx, src1)),
                            ),
                        ),
                    ),
                    inplace,
                );
            }
            if !((*src1).grad).is_null() {
                (*src1).grad = ggml_add_impl(
                    ctx,
                    (*src1).grad,
                    ggml_mul_mat(
                        ctx,
                        ggml_cont(ctx, ggml_transpose(ctx, src0)),
                        (*tensor).grad,
                    ),
                    inplace,
                );
            }
        }
        27 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add_impl(
                    ctx,
                    (*src0).grad,
                    ggml_scale_impl(ctx, (*tensor).grad, src1, 0 as libc::c_int != 0),
                    inplace,
                );
            }
            if !((*src1).grad).is_null() {
                (*src1).grad = ggml_add_impl(
                    ctx,
                    (*src1).grad,
                    ggml_sum(
                        ctx,
                        ggml_mul_impl(ctx, (*tensor).grad, src0, 0 as libc::c_int != 0),
                    ),
                    inplace,
                );
            }
        }
        28 => {
            if !(ggml_nelements((*tensor).opt[0 as libc::c_int as usize])
                == 5 as libc::c_int as libc::c_long)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    13396 as libc::c_int,
                    b"ggml_nelements(tensor->opt[0]) == 5\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            if !((*(*tensor).opt[0 as libc::c_int as usize]).type_0 as libc::c_uint
                == GGML_TYPE_I32 as libc::c_int as libc::c_uint)
            {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    13397 as libc::c_int,
                    b"tensor->opt[0]->type == GGML_TYPE_I32\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            let nb1_0: size_t = *((*(*tensor).opt[0 as libc::c_int as usize]).data as *mut int32_t)
                .offset(0 as libc::c_int as isize) as size_t;
            let nb2_0: size_t = *((*(*tensor).opt[0 as libc::c_int as usize]).data as *mut int32_t)
                .offset(1 as libc::c_int as isize) as size_t;
            let nb3_0: size_t = *((*(*tensor).opt[0 as libc::c_int as usize]).data as *mut int32_t)
                .offset(2 as libc::c_int as isize) as size_t;
            let offset_0: size_t = *((*(*tensor).opt[0 as libc::c_int as usize]).data
                as *mut int32_t)
                .offset(3 as libc::c_int as isize) as size_t;
            let mut tensor_grad_view_0: *mut ggml_tensor = 0 as *mut ggml_tensor;
            if !((*src0).grad).is_null() || !((*src1).grad).is_null() {
                if !((*src0).type_0 as libc::c_uint == (*tensor).type_0 as libc::c_uint) {
                    fprintf(
                        stderr,
                        b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                        b"ggml.c\0" as *const u8 as *const libc::c_char,
                        13406 as libc::c_int,
                        b"src0->type == tensor->type\0" as *const u8 as *const libc::c_char,
                    );
                    abort();
                }
                if !((*(*tensor).grad).type_0 as libc::c_uint == (*tensor).type_0 as libc::c_uint) {
                    fprintf(
                        stderr,
                        b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                        b"ggml.c\0" as *const u8 as *const libc::c_char,
                        13407 as libc::c_int,
                        b"tensor->grad->type == tensor->type\0" as *const u8 as *const libc::c_char,
                    );
                    abort();
                }
                if !((*(*tensor).grad).type_0 as libc::c_uint
                    == (*(*src1).grad).type_0 as libc::c_uint)
                {
                    fprintf(
                        stderr,
                        b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                        b"ggml.c\0" as *const u8 as *const libc::c_char,
                        13408 as libc::c_int,
                        b"tensor->grad->type == src1->grad->type\0" as *const u8
                            as *const libc::c_char,
                    );
                    abort();
                }
                tensor_grad_view_0 = ggml_view_4d(
                    ctx,
                    (*tensor).grad,
                    (*(*src1).grad).ne[0 as libc::c_int as usize],
                    (*(*src1).grad).ne[1 as libc::c_int as usize],
                    (*(*src1).grad).ne[2 as libc::c_int as usize],
                    (*(*src1).grad).ne[3 as libc::c_int as usize],
                    nb1_0,
                    nb2_0,
                    nb3_0,
                    offset_0,
                );
            }
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add_impl(
                    ctx,
                    (*src0).grad,
                    ggml_acc_impl(
                        ctx,
                        (*tensor).grad,
                        ggml_neg(ctx, tensor_grad_view_0),
                        nb1_0,
                        nb2_0,
                        nb3_0,
                        offset_0,
                        0 as libc::c_int != 0,
                    ),
                    inplace,
                );
            }
            if !((*src1).grad).is_null() {
                (*src1).grad = ggml_add_impl(
                    ctx,
                    (*src1).grad,
                    ggml_reshape(ctx, ggml_cont(ctx, tensor_grad_view_0), (*src1).grad),
                    inplace,
                );
            }
        }
        29 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add_impl(ctx, (*src0).grad, (*tensor).grad, inplace);
            }
            !((*src1).grad).is_null();
        }
        30 => {
            if !((*src0).grad).is_null() {
                if !ggml_is_contiguous((*src0).grad) {
                    fprintf(
                        stderr,
                        b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                        b"ggml.c\0" as *const u8 as *const libc::c_char,
                        13457 as libc::c_int,
                        b"ggml_is_contiguous(src0->grad)\0" as *const u8 as *const libc::c_char,
                    );
                    abort();
                }
                if !ggml_is_contiguous((*tensor).grad) {
                    fprintf(
                        stderr,
                        b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                        b"ggml.c\0" as *const u8 as *const libc::c_char,
                        13458 as libc::c_int,
                        b"ggml_is_contiguous(tensor->grad)\0" as *const u8 as *const libc::c_char,
                    );
                    abort();
                }
                (*src0).grad = ggml_add_impl(ctx, (*src0).grad, (*tensor).grad, inplace);
            }
        }
        31 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add_impl(
                    ctx,
                    (*src0).grad,
                    ggml_reshape(ctx, (*tensor).grad, (*src0).grad),
                    inplace,
                );
            }
        }
        32 => {
            if !((*src0).grad).is_null() {
                let mut offset_1: size_t = 0;
                memcpy(
                    &mut offset_1 as *mut size_t as *mut libc::c_void,
                    ((*tensor).padding).as_mut_ptr() as *const libc::c_void,
                    ::core::mem::size_of::<size_t>() as libc::c_ulong,
                );
                let mut nb1_1: size_t = (*tensor).nb[1 as libc::c_int as usize];
                let mut nb2_1: size_t = (*tensor).nb[2 as libc::c_int as usize];
                let mut nb3_1: size_t = (*tensor).nb[3 as libc::c_int as usize];
                if (*src0).type_0 as libc::c_uint != (*(*src0).grad).type_0 as libc::c_uint {
                    let mut ng: size_t = ggml_element_size((*src0).grad);
                    let mut n0: size_t = ggml_element_size(src0);
                    if !(offset_1.wrapping_rem(n0) == 0 as libc::c_int as libc::c_ulong) {
                        fprintf(
                            stderr,
                            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                            b"ggml.c\0" as *const u8 as *const libc::c_char,
                            13487 as libc::c_int,
                            b"offset % n0 == 0\0" as *const u8 as *const libc::c_char,
                        );
                        abort();
                    }
                    if !(nb1_1.wrapping_rem(n0) == 0 as libc::c_int as libc::c_ulong) {
                        fprintf(
                            stderr,
                            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                            b"ggml.c\0" as *const u8 as *const libc::c_char,
                            13488 as libc::c_int,
                            b"nb1 % n0 == 0\0" as *const u8 as *const libc::c_char,
                        );
                        abort();
                    }
                    if !(nb2_1.wrapping_rem(n0) == 0 as libc::c_int as libc::c_ulong) {
                        fprintf(
                            stderr,
                            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                            b"ggml.c\0" as *const u8 as *const libc::c_char,
                            13489 as libc::c_int,
                            b"nb2 % n0 == 0\0" as *const u8 as *const libc::c_char,
                        );
                        abort();
                    }
                    if !(nb3_1.wrapping_rem(n0) == 0 as libc::c_int as libc::c_ulong) {
                        fprintf(
                            stderr,
                            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                            b"ggml.c\0" as *const u8 as *const libc::c_char,
                            13490 as libc::c_int,
                            b"nb3 % n0 == 0\0" as *const u8 as *const libc::c_char,
                        );
                        abort();
                    }
                    offset_1 = offset_1.wrapping_div(n0).wrapping_mul(ng);
                    nb1_1 = nb1_1.wrapping_div(n0).wrapping_mul(ng);
                    nb2_1 = nb2_1.wrapping_div(n0).wrapping_mul(ng);
                    nb3_1 = nb3_1.wrapping_div(n0).wrapping_mul(ng);
                }
                (*src0).grad = ggml_acc_impl(
                    ctx,
                    (*src0).grad,
                    (*tensor).grad,
                    nb1_1,
                    nb2_1,
                    nb3_1,
                    offset_1,
                    inplace,
                );
            }
        }
        33 => {
            if !((*src0).grad).is_null() {
                let mut axis0: libc::c_int = (*tensor).padding[0 as libc::c_int as usize]
                    as libc::c_int
                    & 0x3 as libc::c_int;
                let mut axis1: libc::c_int = (*tensor).padding[1 as libc::c_int as usize]
                    as libc::c_int
                    & 0x3 as libc::c_int;
                let mut axis2: libc::c_int = (*tensor).padding[2 as libc::c_int as usize]
                    as libc::c_int
                    & 0x3 as libc::c_int;
                let mut axis3: libc::c_int = (*tensor).padding[3 as libc::c_int as usize]
                    as libc::c_int
                    & 0x3 as libc::c_int;
                let mut axes_backward: [libc::c_int; 4] = [
                    0 as libc::c_int,
                    0 as libc::c_int,
                    0 as libc::c_int,
                    0 as libc::c_int,
                ];
                axes_backward[axis0 as usize] = 0 as libc::c_int;
                axes_backward[axis1 as usize] = 1 as libc::c_int;
                axes_backward[axis2 as usize] = 2 as libc::c_int;
                axes_backward[axis3 as usize] = 3 as libc::c_int;
                (*src0).grad = ggml_add_impl(
                    ctx,
                    (*src0).grad,
                    ggml_permute(
                        ctx,
                        (*tensor).grad,
                        axes_backward[0 as libc::c_int as usize],
                        axes_backward[1 as libc::c_int as usize],
                        axes_backward[2 as libc::c_int as usize],
                        axes_backward[3 as libc::c_int as usize],
                    ),
                    inplace,
                );
            }
        }
        34 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add_impl(
                    ctx,
                    (*src0).grad,
                    ggml_transpose(ctx, (*tensor).grad),
                    inplace,
                );
            }
        }
        35 => {
            if !((*src0).grad).is_null() {
                (*src0).grad = ggml_add_impl(
                    ctx,
                    (*src0).grad,
                    ggml_get_rows_back(ctx, (*tensor).grad, src1, (*src0).grad),
                    inplace,
                );
            }
            !((*src1).grad).is_null();
        }
        36 => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    13549 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
        37 => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    13553 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
        38 => {
            if !((*src0).grad).is_null() {
                let n_past: libc::c_int =
                    *((*src1).data as *mut int32_t).offset(0 as libc::c_int as isize);
                (*src0).grad = ggml_add_impl(
                    ctx,
                    (*src0).grad,
                    ggml_diag_mask_zero_impl(ctx, (*tensor).grad, n_past, 0 as libc::c_int != 0),
                    inplace,
                );
            }
            !((*src1).grad).is_null();
        }
        39 => {
            if !((*src0).grad).is_null() {
                let n_past_0: libc::c_int =
                    *((*src1).data as *mut int32_t).offset(0 as libc::c_int as isize);
                (*src0).grad = ggml_add_impl(
                    ctx,
                    (*src0).grad,
                    ggml_diag_mask_zero_impl(ctx, (*tensor).grad, n_past_0, 0 as libc::c_int != 0),
                    inplace,
                );
            }
            !((*src1).grad).is_null();
        }
        40 => {
            if !((*src0).grad).is_null() {
                let mut ne2: [int64_t; 4] = [
                    (*tensor).ne[0 as libc::c_int as usize],
                    1 as libc::c_int as int64_t,
                    (*tensor).ne[1 as libc::c_int as usize]
                        * (*tensor).ne[2 as libc::c_int as usize],
                    (*tensor).ne[3 as libc::c_int as usize],
                ];
                let mut tensor2: *mut ggml_tensor = ggml_cont(
                    ctx,
                    ggml_reshape_4d(
                        ctx,
                        ggml_cont(ctx, tensor),
                        ne2[0 as libc::c_int as usize],
                        ne2[1 as libc::c_int as usize],
                        ne2[2 as libc::c_int as usize],
                        ne2[3 as libc::c_int as usize],
                    ),
                );
                let mut grad2: *mut ggml_tensor = ggml_cont(
                    ctx,
                    ggml_reshape_4d(
                        ctx,
                        ggml_cont(ctx, (*tensor).grad),
                        ne2[0 as libc::c_int as usize],
                        ne2[1 as libc::c_int as usize],
                        ne2[2 as libc::c_int as usize],
                        ne2[3 as libc::c_int as usize],
                    ),
                );
                let mut tensor2_t: *mut ggml_tensor = ggml_cont(
                    ctx,
                    ggml_permute(
                        ctx,
                        tensor2,
                        1 as libc::c_int,
                        0 as libc::c_int,
                        2 as libc::c_int,
                        3 as libc::c_int,
                    ),
                );
                (*src0).grad = ggml_add_impl(
                    ctx,
                    (*src0).grad,
                    ggml_reshape(
                        ctx,
                        ggml_mul_mat(
                            ctx,
                            ggml_sub(
                                ctx,
                                ggml_diag(ctx, tensor2),
                                ggml_mul_mat(ctx, tensor2_t, tensor2_t),
                            ),
                            grad2,
                        ),
                        (*src0).grad,
                    ),
                    inplace,
                );
            }
        }
        41 => {
            if !((*src0).grad).is_null() {
                let n_past_1: libc::c_int =
                    *((*src1).data as *mut int32_t).offset(0 as libc::c_int as isize);
                let n_dims: libc::c_int =
                    *((*src1).data as *mut int32_t).offset(1 as libc::c_int as isize);
                let mode: libc::c_int =
                    *((*src1).data as *mut int32_t).offset(2 as libc::c_int as isize);
                (*src0).grad = ggml_add_impl(
                    ctx,
                    (*src0).grad,
                    ggml_rope_back(ctx, (*tensor).grad, n_past_1, n_dims, mode),
                    inplace,
                );
            }
            !((*src1).grad).is_null();
        }
        42 => {
            if !((*src0).grad).is_null() {
                let n_past_2: libc::c_int =
                    *((*src1).data as *mut int32_t).offset(0 as libc::c_int as isize);
                let n_dims_0: libc::c_int =
                    *((*src1).data as *mut int32_t).offset(1 as libc::c_int as isize);
                let mode_0: libc::c_int =
                    *((*src1).data as *mut int32_t).offset(2 as libc::c_int as isize);
                (*src0).grad = ggml_add_impl(
                    ctx,
                    (*src0).grad,
                    ggml_rope(ctx, (*tensor).grad, n_past_2, n_dims_0, mode_0),
                    inplace,
                );
            }
            !((*src1).grad).is_null();
        }
        45 => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    13681 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
        46 => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    13685 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
        47 => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    13689 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
        48 => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    13693 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
        49 | 50 => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    13698 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
        51 => {
            if 0 as libc::c_int == 0 {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    13706 as libc::c_int,
                    b"false\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
        }
        0 | _ => {}
    };
}
unsafe extern "C" fn ggml_visit_parents(mut cgraph: *mut ggml_cgraph, mut node: *mut ggml_tensor) {
    if ((*node).grad).is_null() {
        (*node).op as libc::c_uint != GGML_OP_NONE as libc::c_int as libc::c_uint;
    }
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < (*cgraph).n_nodes {
        if (*cgraph).nodes[i as usize] == node {
            return;
        }
        i += 1;
        i;
    }
    let mut i_0: libc::c_int = 0 as libc::c_int;
    while i_0 < (*cgraph).n_leafs {
        if (*cgraph).leafs[i_0 as usize] == node {
            return;
        }
        i_0 += 1;
        i_0;
    }
    if !((*node).src0).is_null() {
        ggml_visit_parents(cgraph, (*node).src0);
    }
    if !((*node).src1).is_null() {
        ggml_visit_parents(cgraph, (*node).src1);
    }
    let mut i_1: libc::c_int = 0 as libc::c_int;
    while i_1 < 4 as libc::c_int {
        if !((*node).opt[i_1 as usize]).is_null() {
            ggml_visit_parents(cgraph, (*node).opt[i_1 as usize]);
        }
        i_1 += 1;
        i_1;
    }
    if (*node).op as libc::c_uint == GGML_OP_NONE as libc::c_int as libc::c_uint
        && ((*node).grad).is_null()
    {
        if !((*cgraph).n_leafs < 4096 as libc::c_int) {
            fprintf(
                stderr,
                b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                b"ggml.c\0" as *const u8 as *const libc::c_char,
                13749 as libc::c_int,
                b"cgraph->n_leafs < GGML_MAX_NODES\0" as *const u8 as *const libc::c_char,
            );
            abort();
        }
        (*cgraph).leafs[(*cgraph).n_leafs as usize] = node;
        (*cgraph).n_leafs += 1;
        (*cgraph).n_leafs;
    } else {
        if !((*cgraph).n_nodes < 4096 as libc::c_int) {
            fprintf(
                stderr,
                b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                b"ggml.c\0" as *const u8 as *const libc::c_char,
                13754 as libc::c_int,
                b"cgraph->n_nodes < GGML_MAX_NODES\0" as *const u8 as *const libc::c_char,
            );
            abort();
        }
        (*cgraph).nodes[(*cgraph).n_nodes as usize] = node;
        (*cgraph).grads[(*cgraph).n_nodes as usize] = (*node).grad;
        (*cgraph).n_nodes += 1;
        (*cgraph).n_nodes;
    };
}
unsafe extern "C" fn ggml_build_forward_impl(
    mut cgraph: *mut ggml_cgraph,
    mut tensor: *mut ggml_tensor,
    mut expand: bool,
) {
    if !expand {
        (*cgraph).n_nodes = 0 as libc::c_int;
        (*cgraph).n_leafs = 0 as libc::c_int;
    }
    let n0: libc::c_int = (*cgraph).n_nodes;
    ggml_visit_parents(cgraph, tensor);
    let n_new: libc::c_int = (*cgraph).n_nodes - n0;
    if n_new > 0 as libc::c_int {
        if !((*cgraph).nodes[((*cgraph).n_nodes - 1 as libc::c_int) as usize] == tensor) {
            fprintf(
                stderr,
                b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                b"ggml.c\0" as *const u8 as *const libc::c_char,
                13778 as libc::c_int,
                b"cgraph->nodes[cgraph->n_nodes - 1] == tensor\0" as *const u8
                    as *const libc::c_char,
            );
            abort();
        }
    }
}
#[no_mangle]
pub unsafe extern "C" fn ggml_build_forward_expand(
    mut cgraph: *mut ggml_cgraph,
    mut tensor: *mut ggml_tensor,
) {
    ggml_build_forward_impl(cgraph, tensor, 1 as libc::c_int != 0);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_build_forward(mut tensor: *mut ggml_tensor) -> ggml_cgraph {
    let mut result: ggml_cgraph = {
        let mut init = ggml_cgraph {
            n_nodes: 0 as libc::c_int,
            n_leafs: 0 as libc::c_int,
            n_threads: 4 as libc::c_int,
            work_size: 0 as libc::c_int as size_t,
            work: 0 as *mut ggml_tensor,
            nodes: [
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
            ],
            grads: [
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
            ],
            leafs: [
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
                0 as *mut ggml_tensor,
            ],
            perf_runs: 0 as libc::c_int,
            perf_cycles: 0 as libc::c_int as int64_t,
            perf_time_us: 0 as libc::c_int as int64_t,
        };
        init
    };
    ggml_build_forward_impl(&mut result, tensor, 0 as libc::c_int != 0);
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_build_backward(
    mut ctx: *mut ggml_context,
    mut gf: *mut ggml_cgraph,
    mut keep: bool,
) -> ggml_cgraph {
    let mut result: ggml_cgraph = *gf;
    if !((*gf).n_nodes > 0 as libc::c_int) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            13809 as libc::c_int,
            b"gf->n_nodes > 0\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    if keep {
        let mut i: libc::c_int = 0 as libc::c_int;
        while i < (*gf).n_nodes {
            let mut node: *mut ggml_tensor = (*gf).nodes[i as usize];
            if !((*node).grad).is_null() {
                (*node).grad = ggml_dup_tensor(ctx, node);
                (*gf).grads[i as usize] = (*node).grad;
            }
            i += 1;
            i;
        }
    }
    let mut i_0: libc::c_int = (*gf).n_nodes - 1 as libc::c_int;
    while i_0 >= 0 as libc::c_int {
        let mut node_0: *mut ggml_tensor = (*gf).nodes[i_0 as usize];
        if !((*node_0).grad).is_null() {
            ggml_compute_backward(ctx, node_0, keep);
        }
        i_0 -= 1;
        i_0;
    }
    let mut i_1: libc::c_int = (*gf).n_nodes - 1 as libc::c_int;
    while i_1 >= 0 as libc::c_int {
        let mut node_1: *mut ggml_tensor = (*gf).nodes[i_1 as usize];
        if (*node_1).is_param {
            ggml_build_forward_impl(&mut result, (*node_1).grad, 1 as libc::c_int != 0);
        }
        i_1 -= 1;
        i_1;
    }
    return result;
}
unsafe extern "C" fn ggml_graph_compute_thread(mut data: *mut libc::c_void) -> thread_ret_t {
    let mut state: *mut ggml_compute_state = data as *mut ggml_compute_state;
    let n_threads: libc::c_int = (*(*state).shared).n_threads;
    loop {
        if atomic_fetch_add(&mut (*(*state).shared).n_ready, 1 as libc::c_int)
            == n_threads - 1 as libc::c_int
        {
            atomic_store(
                &mut (*(*state).shared).has_work as *mut bool as *mut libc::c_int,
                0 as libc::c_int,
            );
        } else {
            while atomic_load(&mut (*(*state).shared).has_work as *mut bool as *mut libc::c_int)
                != 0
            {
                if atomic_load(&mut (*(*state).shared).stop as *mut bool as *mut libc::c_int) != 0 {
                    return 0 as thread_ret_t;
                }
                &mut (*(*state).shared).spin;
                &mut (*(*state).shared).spin;
            }
        }
        atomic_fetch_sub(&mut (*(*state).shared).n_ready, 1 as libc::c_int);
        while atomic_load(&mut (*(*state).shared).has_work as *mut bool as *mut libc::c_int) == 0 {
            if atomic_load(&mut (*(*state).shared).stop as *mut bool as *mut libc::c_int) != 0 {
                return 0 as thread_ret_t;
            }
            &mut (*(*state).shared).spin;
            &mut (*(*state).shared).spin;
        }
        if atomic_load(&mut (*(*state).shared).stop as *mut bool as *mut libc::c_int) != 0 {
            break;
        }
        if ((*state).node).is_null() {
            break;
        }
        if (*state).params.ith < (*state).params.nth {
            ggml_compute_forward(&mut (*state).params, (*state).node);
        }
        (*state).node = 0 as *mut ggml_tensor;
    }
    return 0 as thread_ret_t;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_graph_compute(
    mut ctx: *mut ggml_context,
    mut cgraph: *mut ggml_cgraph,
) {
    let n_threads: libc::c_int = (*cgraph).n_threads;
    let mut state_shared: ggml_compute_state_shared = {
        let mut init = ggml_compute_state_shared {
            spin: 0 as libc::c_int,
            n_threads: n_threads,
            n_ready: 0 as libc::c_int,
            has_work: 0 as libc::c_int != 0,
            stop: 0 as libc::c_int != 0,
        };
        init
    };
    let mut workers: *mut ggml_compute_state = (if n_threads > 1 as libc::c_int {
        let mut fresh9 = ::std::vec::from_elem(
            0,
            (::core::mem::size_of::<ggml_compute_state>() as libc::c_ulong)
                .wrapping_mul((n_threads - 1 as libc::c_int) as libc::c_ulong) as usize,
        );
        fresh9.as_mut_ptr()
    } else {
        0 as *mut libc::c_int
    }) as *mut ggml_compute_state;
    if n_threads > 1 as libc::c_int {
        &mut state_shared.spin;
        atomic_store(
            &mut state_shared.has_work as *mut bool as *mut libc::c_int,
            1 as libc::c_int,
        );
        let mut j: libc::c_int = 0 as libc::c_int;
        while j < n_threads - 1 as libc::c_int {
            *workers.offset(j as isize) = {
                let mut init = ggml_compute_state {
                    thrd: 0 as libc::c_int as ggml_thread_t,
                    params: {
                        let mut init = ggml_compute_params {
                            type_0: GGML_TASK_COMPUTE,
                            ith: j + 1 as libc::c_int,
                            nth: n_threads,
                            wsize: if !((*cgraph).work).is_null() {
                                ggml_nbytes((*cgraph).work)
                            } else {
                                0 as libc::c_int as libc::c_ulong
                            },
                            wdata: if !((*cgraph).work).is_null() {
                                (*(*cgraph).work).data
                            } else {
                                0 as *mut libc::c_void
                            },
                        };
                        init
                    },
                    node: 0 as *mut ggml_tensor,
                    shared: &mut state_shared,
                };
                init
            };
            let mut rc: libc::c_int = pthread_create(
                &mut (*workers.offset(j as isize)).thrd,
                0 as *const pthread_attr_t,
                Some(
                    ggml_graph_compute_thread
                        as unsafe extern "C" fn(*mut libc::c_void) -> thread_ret_t,
                ),
                &mut *workers.offset(j as isize) as *mut ggml_compute_state as *mut libc::c_void,
            );
            if !(rc == 0 as libc::c_int) {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    14008 as libc::c_int,
                    b"rc == 0\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            j += 1;
            j;
        }
    }
    let mut work_size: size_t = 0 as libc::c_int as size_t;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < (*cgraph).n_nodes {
        let mut node: *mut ggml_tensor = (*cgraph).nodes[i as usize];
        match (*node).op as libc::c_uint {
            29 | 1 => {
                (*node).n_tasks = n_threads;
                let mut cur: size_t = 0 as libc::c_int as size_t;
                if ggml_is_quantized((*node).type_0) {
                    cur = (GGML_TYPE_SIZE[GGML_TYPE_F32 as libc::c_int as usize])
                        .wrapping_mul((*node).ne[0 as libc::c_int as usize] as libc::c_ulong)
                        .wrapping_mul(n_threads as libc::c_ulong);
                }
                work_size = if work_size > cur { work_size } else { cur };
            }
            2 | 3 => {
                (*node).n_tasks = n_threads;
                let mut cur_0: size_t = 0 as libc::c_int as size_t;
                if ggml_is_quantized((*(*node).src0).type_0) {
                    cur_0 = (GGML_TYPE_SIZE[GGML_TYPE_F32 as libc::c_int as usize])
                        .wrapping_mul(
                            (*(*node).src0).ne[0 as libc::c_int as usize] as libc::c_ulong,
                        )
                        .wrapping_mul(n_threads as libc::c_ulong);
                }
                work_size = if work_size > cur_0 { work_size } else { cur_0 };
            }
            4 => {
                (*node).n_tasks = n_threads;
                let mut cur_1: size_t = 0 as libc::c_int as size_t;
                if ggml_is_quantized((*(*node).src0).type_0) {
                    cur_1 = (GGML_TYPE_SIZE[GGML_TYPE_F32 as libc::c_int as usize])
                        .wrapping_mul(
                            (*(*node).src1).ne[0 as libc::c_int as usize] as libc::c_ulong,
                        )
                        .wrapping_mul(n_threads as libc::c_ulong);
                }
                work_size = if work_size > cur_1 { work_size } else { cur_1 };
            }
            5 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 => {
                (*node).n_tasks = 1 as libc::c_int;
            }
            6 | 20 | 21 | 22 | 23 | 24 | 25 => {
                (*node).n_tasks = n_threads;
            }
            26 => {
                (*node).n_tasks = n_threads;
                let mut cur_2: size_t = 0 as libc::c_int as size_t;
                if (*(*node).src0).type_0 as libc::c_uint
                    == GGML_TYPE_F16 as libc::c_int as libc::c_uint
                    && (*(*node).src1).type_0 as libc::c_uint
                        == GGML_TYPE_F32 as libc::c_int as libc::c_uint
                {
                    cur_2 = (GGML_TYPE_SIZE[GGML_TYPE_F16 as libc::c_int as usize])
                        .wrapping_mul(ggml_nelements((*node).src1) as libc::c_ulong);
                } else if (*(*node).src0).type_0 as libc::c_uint
                    == GGML_TYPE_F32 as libc::c_int as libc::c_uint
                    && (*(*node).src1).type_0 as libc::c_uint
                        == GGML_TYPE_F32 as libc::c_int as libc::c_uint
                {
                    cur_2 = 0 as libc::c_int as size_t;
                } else if ggml_is_quantized((*(*node).src0).type_0) as libc::c_int != 0
                    && (*(*node).src1).type_0 as libc::c_uint
                        == GGML_TYPE_F32 as libc::c_int as libc::c_uint
                {
                    let type_q: ggml_type =
                        quantize_fns[(*(*node).src0).type_0 as usize].vec_dot_type;
                    cur_2 = (GGML_TYPE_SIZE[type_q as usize])
                        .wrapping_mul(ggml_nelements((*node).src1) as libc::c_ulong)
                        .wrapping_div(GGML_BLCK_SIZE[type_q as usize] as libc::c_ulong);
                } else if 0 as libc::c_int == 0 {
                    fprintf(
                        stderr,
                        b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                        b"ggml.c\0" as *const u8 as *const libc::c_char,
                        14146 as libc::c_int,
                        b"false\0" as *const u8 as *const libc::c_char,
                    );
                    abort();
                }
                work_size = if work_size > cur_2 { work_size } else { cur_2 };
            }
            27 => {
                (*node).n_tasks = n_threads;
            }
            28 | 30 | 31 | 32 | 33 | 34 | 35 | 36 | 37 | 39 => {
                (*node).n_tasks = 1 as libc::c_int;
            }
            38 | 40 | 41 | 42 => {
                (*node).n_tasks = n_threads;
            }
            43 => {
                (*node).n_tasks = 1 as libc::c_int;
            }
            44 => {
                (*node).n_tasks = 1 as libc::c_int;
            }
            45 | 46 => {
                (*node).n_tasks = n_threads;
                if !((*(*node).src0).ne[3 as libc::c_int as usize]
                    == 1 as libc::c_int as libc::c_long)
                {
                    fprintf(
                        stderr,
                        b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                        b"ggml.c\0" as *const u8 as *const libc::c_char,
                        14188 as libc::c_int,
                        b"node->src0->ne[3] == 1\0" as *const u8 as *const libc::c_char,
                    );
                    abort();
                }
                if !((*(*node).src1).ne[2 as libc::c_int as usize]
                    == 1 as libc::c_int as libc::c_long)
                {
                    fprintf(
                        stderr,
                        b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                        b"ggml.c\0" as *const u8 as *const libc::c_char,
                        14189 as libc::c_int,
                        b"node->src1->ne[2] == 1\0" as *const u8 as *const libc::c_char,
                    );
                    abort();
                }
                if !((*(*node).src1).ne[3 as libc::c_int as usize]
                    == 1 as libc::c_int as libc::c_long)
                {
                    fprintf(
                        stderr,
                        b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                        b"ggml.c\0" as *const u8 as *const libc::c_char,
                        14190 as libc::c_int,
                        b"node->src1->ne[3] == 1\0" as *const u8 as *const libc::c_char,
                    );
                    abort();
                }
                let mut cur_3: size_t = 0 as libc::c_int as size_t;
                let nk: libc::c_int = (*(*node).src0).ne[0 as libc::c_int as usize] as libc::c_int;
                if (*(*node).src0).type_0 as libc::c_uint
                    == GGML_TYPE_F16 as libc::c_int as libc::c_uint
                    && (*(*node).src1).type_0 as libc::c_uint
                        == GGML_TYPE_F32 as libc::c_int as libc::c_uint
                {
                    cur_3 = (::core::mem::size_of::<ggml_fp16_t>() as libc::c_ulong).wrapping_mul(
                        ((nk * ggml_up32(
                            (*(*node).src0).ne[1 as libc::c_int as usize] as libc::c_int,
                        )) as libc::c_long
                            * (*(*node).src0).ne[2 as libc::c_int as usize]
                            + ((2 as libc::c_int * (nk / 2 as libc::c_int)) as libc::c_long
                                + (*(*node).src1).ne[0 as libc::c_int as usize])
                                * (*(*node).src1).ne[1 as libc::c_int as usize])
                            as libc::c_ulong,
                    );
                } else if (*(*node).src0).type_0 as libc::c_uint
                    == GGML_TYPE_F32 as libc::c_int as libc::c_uint
                    && (*(*node).src1).type_0 as libc::c_uint
                        == GGML_TYPE_F32 as libc::c_int as libc::c_uint
                {
                    cur_3 = (::core::mem::size_of::<libc::c_float>() as libc::c_ulong)
                        .wrapping_mul(
                            ((nk * ggml_up32(
                                (*(*node).src0).ne[1 as libc::c_int as usize] as libc::c_int,
                            )) as libc::c_long
                                * (*(*node).src0).ne[2 as libc::c_int as usize]
                                + ((2 as libc::c_int * (nk / 2 as libc::c_int)) as libc::c_long
                                    + (*(*node).src1).ne[0 as libc::c_int as usize])
                                    * (*(*node).src1).ne[1 as libc::c_int as usize])
                                as libc::c_ulong,
                        );
                } else if 0 as libc::c_int == 0 {
                    fprintf(
                        stderr,
                        b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                        b"ggml.c\0" as *const u8 as *const libc::c_char,
                        14208 as libc::c_int,
                        b"false\0" as *const u8 as *const libc::c_char,
                    );
                    abort();
                }
                work_size = if work_size > cur_3 { work_size } else { cur_3 };
            }
            47 => {
                (*node).n_tasks = n_threads;
                let mut cur_4: size_t = 0 as libc::c_int as size_t;
                let ne11: int64_t = ggml_up(
                    (*(*node).src1).ne[1 as libc::c_int as usize] as libc::c_int,
                    4 as libc::c_int,
                ) as int64_t;
                if (*(*node).src1).type_0 as libc::c_uint
                    == GGML_TYPE_F32 as libc::c_int as libc::c_uint
                {
                    cur_4 = (::core::mem::size_of::<libc::c_float>() as libc::c_ulong)
                        .wrapping_mul(ne11 as libc::c_ulong)
                        .wrapping_mul((*node).n_tasks as libc::c_ulong);
                    cur_4 = (cur_4 as libc::c_ulong).wrapping_add(
                        (::core::mem::size_of::<libc::c_float>() as libc::c_ulong)
                            .wrapping_mul(ne11 as libc::c_ulong)
                            .wrapping_mul((*node).n_tasks as libc::c_ulong),
                    ) as size_t as size_t;
                }
                if (*(*node).src1).type_0 as libc::c_uint
                    == GGML_TYPE_F16 as libc::c_int as libc::c_uint
                {
                    cur_4 = (::core::mem::size_of::<libc::c_float>() as libc::c_ulong)
                        .wrapping_mul(ne11 as libc::c_ulong)
                        .wrapping_mul((*node).n_tasks as libc::c_ulong);
                    cur_4 = (cur_4 as libc::c_ulong).wrapping_add(
                        (::core::mem::size_of::<libc::c_float>() as libc::c_ulong)
                            .wrapping_mul(ne11 as libc::c_ulong)
                            .wrapping_mul((*node).n_tasks as libc::c_ulong),
                    ) as size_t as size_t;
                }
                work_size = if work_size > cur_4 { work_size } else { cur_4 };
            }
            48 => {
                (*node).n_tasks = n_threads;
                let mut cur_5: size_t = 0 as libc::c_int as size_t;
                if (*(*node).src1).type_0 as libc::c_uint
                    == GGML_TYPE_F32 as libc::c_int as libc::c_uint
                {
                    cur_5 = (::core::mem::size_of::<libc::c_float>() as libc::c_ulong)
                        .wrapping_mul(
                            (*(*node).src1).ne[1 as libc::c_int as usize] as libc::c_ulong,
                        )
                        .wrapping_mul((*node).n_tasks as libc::c_ulong);
                    cur_5 = (cur_5 as libc::c_ulong).wrapping_add(
                        (::core::mem::size_of::<libc::c_float>() as libc::c_ulong)
                            .wrapping_mul(
                                (*(*node).src1).ne[1 as libc::c_int as usize] as libc::c_ulong,
                            )
                            .wrapping_mul((*node).n_tasks as libc::c_ulong),
                    ) as size_t as size_t;
                }
                if (*(*node).src1).type_0 as libc::c_uint
                    == GGML_TYPE_F16 as libc::c_int as libc::c_uint
                {
                    cur_5 = (::core::mem::size_of::<libc::c_float>() as libc::c_ulong)
                        .wrapping_mul(
                            (*(*node).src1).ne[1 as libc::c_int as usize] as libc::c_ulong,
                        )
                        .wrapping_mul((*node).n_tasks as libc::c_ulong);
                    cur_5 = (cur_5 as libc::c_ulong).wrapping_add(
                        (::core::mem::size_of::<libc::c_float>() as libc::c_ulong)
                            .wrapping_mul(
                                (*(*node).src1).ne[1 as libc::c_int as usize] as libc::c_ulong,
                            )
                            .wrapping_mul((*node).n_tasks as libc::c_ulong),
                    ) as size_t as size_t;
                }
                work_size = if work_size > cur_5 { work_size } else { cur_5 };
            }
            49 | 50 => {
                (*node).n_tasks = 1 as libc::c_int;
            }
            0 => {
                (*node).n_tasks = 1 as libc::c_int;
            }
            51 => {
                if 0 as libc::c_int == 0 {
                    fprintf(
                        stderr,
                        b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                        b"ggml.c\0" as *const u8 as *const libc::c_char,
                        14262 as libc::c_int,
                        b"false\0" as *const u8 as *const libc::c_char,
                    );
                    abort();
                }
            }
            _ => {}
        }
        i += 1;
        i;
    }
    if !((*cgraph).work).is_null() && work_size > (*cgraph).work_size {
        if 0 as libc::c_int == 0 {
            fprintf(
                stderr,
                b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                b"ggml.c\0" as *const u8 as *const libc::c_char,
                14268 as libc::c_int,
                b"false\0" as *const u8 as *const libc::c_char,
            );
            abort();
        }
    }
    if work_size > 0 as libc::c_int as libc::c_ulong && ((*cgraph).work).is_null() {
        (*cgraph).work_size = work_size
            .wrapping_add((64 as libc::c_int * (n_threads - 1 as libc::c_int)) as libc::c_ulong);
        (*cgraph).work = ggml_new_tensor_1d(ctx, GGML_TYPE_I8, (*cgraph).work_size as int64_t);
    }
    let perf_start_cycles: int64_t = 0 as libc::c_int as int64_t;
    let perf_start_time_us: int64_t = 0 as libc::c_int as int64_t;
    let mut i_0: libc::c_int = 0 as libc::c_int;
    while i_0 < (*cgraph).n_nodes {
        let mut node_0: *mut ggml_tensor = (*cgraph).nodes[i_0 as usize];
        let perf_node_start_cycles: int64_t = 0 as libc::c_int as int64_t;
        let perf_node_start_time_us: int64_t = 0 as libc::c_int as int64_t;
        let mut params: ggml_compute_params = {
            let mut init = ggml_compute_params {
                type_0: GGML_TASK_INIT,
                ith: 0 as libc::c_int,
                nth: (*node_0).n_tasks,
                wsize: if !((*cgraph).work).is_null() {
                    ggml_nbytes((*cgraph).work)
                } else {
                    0 as libc::c_int as libc::c_ulong
                },
                wdata: if !((*cgraph).work).is_null() {
                    (*(*cgraph).work).data
                } else {
                    0 as *mut libc::c_void
                },
            };
            init
        };
        ggml_compute_forward(&mut params, node_0);
        if (*node_0).n_tasks > 1 as libc::c_int {
            if atomic_fetch_add(&mut state_shared.n_ready, 1 as libc::c_int)
                == n_threads - 1 as libc::c_int
            {
                atomic_store(
                    &mut state_shared.has_work as *mut bool as *mut libc::c_int,
                    0 as libc::c_int,
                );
            }
            while atomic_load(&mut state_shared.has_work as *mut bool as *mut libc::c_int) != 0 {
                &mut state_shared.spin;
                &mut state_shared.spin;
            }
            let mut j_0: libc::c_int = 0 as libc::c_int;
            while j_0 < n_threads - 1 as libc::c_int {
                (*workers.offset(j_0 as isize)).params = {
                    let mut init = ggml_compute_params {
                        type_0: GGML_TASK_COMPUTE,
                        ith: j_0 + 1 as libc::c_int,
                        nth: (*node_0).n_tasks,
                        wsize: if !((*cgraph).work).is_null() {
                            ggml_nbytes((*cgraph).work)
                        } else {
                            0 as libc::c_int as libc::c_ulong
                        },
                        wdata: if !((*cgraph).work).is_null() {
                            (*(*cgraph).work).data
                        } else {
                            0 as *mut libc::c_void
                        },
                    };
                    init
                };
                let ref mut fresh10 = (*workers.offset(j_0 as isize)).node;
                *fresh10 = node_0;
                j_0 += 1;
                j_0;
            }
            atomic_fetch_sub(&mut state_shared.n_ready, 1 as libc::c_int);
            while atomic_load(&mut state_shared.n_ready) > 0 as libc::c_int {
                &mut state_shared.spin;
                &mut state_shared.spin;
            }
            atomic_store(
                &mut state_shared.has_work as *mut bool as *mut libc::c_int,
                1 as libc::c_int,
            );
        }
        params.type_0 = GGML_TASK_COMPUTE;
        ggml_compute_forward(&mut params, node_0);
        if (*node_0).n_tasks > 1 as libc::c_int {
            if atomic_fetch_add(&mut state_shared.n_ready, 1 as libc::c_int)
                == n_threads - 1 as libc::c_int
            {
                atomic_store(
                    &mut state_shared.has_work as *mut bool as *mut libc::c_int,
                    0 as libc::c_int,
                );
            }
            while atomic_load(&mut state_shared.has_work as *mut bool as *mut libc::c_int) != 0 {
                &mut state_shared.spin;
                &mut state_shared.spin;
            }
            atomic_fetch_sub(&mut state_shared.n_ready, 1 as libc::c_int);
            while atomic_load(&mut state_shared.n_ready) != 0 as libc::c_int {
                &mut state_shared.spin;
                &mut state_shared.spin;
            }
        }
        if (*node_0).n_tasks > 1 as libc::c_int {
            if atomic_fetch_add(&mut state_shared.n_ready, 1 as libc::c_int)
                == n_threads - 1 as libc::c_int
            {
                atomic_store(
                    &mut state_shared.has_work as *mut bool as *mut libc::c_int,
                    0 as libc::c_int,
                );
            }
            while atomic_load(&mut state_shared.has_work as *mut bool as *mut libc::c_int) != 0 {
                &mut state_shared.spin;
                &mut state_shared.spin;
            }
            let mut j_1: libc::c_int = 0 as libc::c_int;
            while j_1 < n_threads - 1 as libc::c_int {
                (*workers.offset(j_1 as isize)).params = {
                    let mut init = ggml_compute_params {
                        type_0: GGML_TASK_FINALIZE,
                        ith: j_1 + 1 as libc::c_int,
                        nth: (*node_0).n_tasks,
                        wsize: if !((*cgraph).work).is_null() {
                            ggml_nbytes((*cgraph).work)
                        } else {
                            0 as libc::c_int as libc::c_ulong
                        },
                        wdata: if !((*cgraph).work).is_null() {
                            (*(*cgraph).work).data
                        } else {
                            0 as *mut libc::c_void
                        },
                    };
                    init
                };
                let ref mut fresh11 = (*workers.offset(j_1 as isize)).node;
                *fresh11 = node_0;
                j_1 += 1;
                j_1;
            }
            atomic_fetch_sub(&mut state_shared.n_ready, 1 as libc::c_int);
            while atomic_load(&mut state_shared.n_ready) > 0 as libc::c_int {
                &mut state_shared.spin;
                &mut state_shared.spin;
            }
            atomic_store(
                &mut state_shared.has_work as *mut bool as *mut libc::c_int,
                1 as libc::c_int,
            );
        }
        params.type_0 = GGML_TASK_FINALIZE;
        ggml_compute_forward(&mut params, node_0);
        if (*node_0).n_tasks > 1 as libc::c_int {
            if atomic_fetch_add(&mut state_shared.n_ready, 1 as libc::c_int)
                == n_threads - 1 as libc::c_int
            {
                atomic_store(
                    &mut state_shared.has_work as *mut bool as *mut libc::c_int,
                    0 as libc::c_int,
                );
            }
            while atomic_load(&mut state_shared.has_work as *mut bool as *mut libc::c_int) != 0 {
                &mut state_shared.spin;
                &mut state_shared.spin;
            }
            atomic_fetch_sub(&mut state_shared.n_ready, 1 as libc::c_int);
            while atomic_load(&mut state_shared.n_ready) != 0 as libc::c_int {
                &mut state_shared.spin;
                &mut state_shared.spin;
            }
        }
        let mut perf_cycles_cur: int64_t =
            0 as libc::c_int as libc::c_long - perf_node_start_cycles;
        let mut perf_time_us_cur: int64_t =
            0 as libc::c_int as libc::c_long - perf_node_start_time_us;
        (*node_0).perf_runs += 1;
        (*node_0).perf_runs;
        (*node_0).perf_cycles += perf_cycles_cur;
        (*node_0).perf_time_us += perf_time_us_cur;
        i_0 += 1;
        i_0;
    }
    if n_threads > 1 as libc::c_int {
        atomic_store(
            &mut state_shared.stop as *mut bool as *mut libc::c_int,
            1 as libc::c_int,
        );
        atomic_store(
            &mut state_shared.has_work as *mut bool as *mut libc::c_int,
            1 as libc::c_int,
        );
        let mut j_2: libc::c_int = 0 as libc::c_int;
        while j_2 < n_threads - 1 as libc::c_int {
            let mut rc_0: libc::c_int = pthread_join(
                (*workers.offset(j_2 as isize)).thrd,
                0 as *mut *mut libc::c_void,
            );
            if !(rc_0 == 0 as libc::c_int) {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    14434 as libc::c_int,
                    b"rc == 0\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            j_2 += 1;
            j_2;
        }
        &mut state_shared.spin;
    }
    let mut perf_cycles_cur_0: int64_t = 0 as libc::c_int as libc::c_long - perf_start_cycles;
    let mut perf_time_us_cur_0: int64_t = 0 as libc::c_int as libc::c_long - perf_start_time_us;
    (*cgraph).perf_runs += 1;
    (*cgraph).perf_runs;
    (*cgraph).perf_cycles += perf_cycles_cur_0;
    (*cgraph).perf_time_us += perf_time_us_cur_0;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_graph_reset(mut cgraph: *mut ggml_cgraph) {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < (*cgraph).n_nodes {
        let mut grad: *mut ggml_tensor = (*cgraph).grads[i as usize];
        if !grad.is_null() {
            ggml_set_zero(grad);
        }
        i += 1;
        i;
    }
}
#[no_mangle]
pub unsafe extern "C" fn ggml_graph_print(mut cgraph: *const ggml_cgraph) {
    let mut perf_total_per_op_us: [int64_t; 51] = [
        0 as libc::c_int as int64_t,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
    ];
    printf(b"=== GRAPH ===\n\0" as *const u8 as *const libc::c_char);
    printf(
        b"n_nodes = %d\n\0" as *const u8 as *const libc::c_char,
        (*cgraph).n_nodes,
    );
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < (*cgraph).n_nodes {
        let mut node: *mut ggml_tensor = (*cgraph).nodes[i as usize];
        perf_total_per_op_us[(*node).op as usize] +=
            if 1 as libc::c_int as libc::c_long > (*node).perf_time_us {
                1 as libc::c_int as libc::c_long
            } else {
                (*node).perf_time_us
            };
        printf(
            b" - %3d: [ %5ld, %5ld, %5ld] %16s %s (%3d) cpu = %7.3f / %7.3f ms, wall = %7.3f / %7.3f ms\n\0"
                as *const u8 as *const libc::c_char,
            i,
            (*node).ne[0 as libc::c_int as usize],
            (*node).ne[1 as libc::c_int as usize],
            (*node).ne[2 as libc::c_int as usize],
            GGML_OP_LABEL[(*node).op as usize],
            if (*node).is_param as libc::c_int != 0 {
                b"x\0" as *const u8 as *const libc::c_char
            } else if !((*node).grad).is_null() {
                b"g\0" as *const u8 as *const libc::c_char
            } else {
                b" \0" as *const u8 as *const libc::c_char
            },
            (*node).perf_runs,
            (*node).perf_cycles as libc::c_double
                / ggml_cycles_per_ms() as libc::c_double,
            (*node).perf_cycles as libc::c_double
                / ggml_cycles_per_ms() as libc::c_double
                / (*node).perf_runs as libc::c_double,
            (*node).perf_time_us as libc::c_double / 1000.0f64,
            (*node).perf_time_us as libc::c_double / 1000.0f64
                / (*node).perf_runs as libc::c_double,
        );
        i += 1;
        i;
    }
    printf(
        b"n_leafs = %d\n\0" as *const u8 as *const libc::c_char,
        (*cgraph).n_leafs,
    );
    let mut i_0: libc::c_int = 0 as libc::c_int;
    while i_0 < (*cgraph).n_leafs {
        let mut node_0: *mut ggml_tensor = (*cgraph).leafs[i_0 as usize];
        printf(
            b" - %3d: [ %5ld, %5ld] %8s\n\0" as *const u8 as *const libc::c_char,
            i_0,
            (*node_0).ne[0 as libc::c_int as usize],
            (*node_0).ne[1 as libc::c_int as usize],
            GGML_OP_LABEL[(*node_0).op as usize],
        );
        i_0 += 1;
        i_0;
    }
    let mut i_1: libc::c_int = 0 as libc::c_int;
    while i_1 < GGML_OP_COUNT as libc::c_int {
        if !(perf_total_per_op_us[i_1 as usize] == 0 as libc::c_int as libc::c_long) {
            printf(
                b"perf_total_per_op_us[%16s] = %7.3f ms\n\0" as *const u8 as *const libc::c_char,
                GGML_OP_LABEL[i_1 as usize],
                perf_total_per_op_us[i_1 as usize] as libc::c_double / 1000.0f64,
            );
        }
        i_1 += 1;
        i_1;
    }
    printf(b"========================================\n\0" as *const u8 as *const libc::c_char);
}
unsafe extern "C" fn ggml_graph_find(
    mut cgraph: *const ggml_cgraph,
    mut node: *const ggml_tensor,
) -> bool {
    if cgraph.is_null() {
        return 1 as libc::c_int != 0;
    }
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < (*cgraph).n_nodes {
        if (*cgraph).nodes[i as usize] == node as *mut ggml_tensor {
            return 1 as libc::c_int != 0;
        }
        i += 1;
        i;
    }
    return 0 as libc::c_int != 0;
}
unsafe extern "C" fn ggml_graph_get_parent(
    mut cgraph: *const ggml_cgraph,
    mut node: *const ggml_tensor,
) -> *mut ggml_tensor {
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < (*cgraph).n_nodes {
        let mut parent: *mut ggml_tensor = (*cgraph).nodes[i as usize];
        if (*parent).grad == node as *mut ggml_tensor {
            return parent;
        }
        i += 1;
        i;
    }
    return 0 as *mut ggml_tensor;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_graph_dump_dot(
    mut gb: *const ggml_cgraph,
    mut gf: *const ggml_cgraph,
    mut filename: *const libc::c_char,
) {
    let mut color: [libc::c_char; 16] = [0; 16];
    let mut fp: *mut FILE = fopen(filename, b"w\0" as *const u8 as *const libc::c_char);
    if fp.is_null() {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            14545 as libc::c_int,
            b"fp\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    fprintf(fp, b"digraph G {\n\0" as *const u8 as *const libc::c_char);
    fprintf(
        fp,
        b"  newrank = true;\n\0" as *const u8 as *const libc::c_char,
    );
    fprintf(
        fp,
        b"  rankdir = LR;\n\0" as *const u8 as *const libc::c_char,
    );
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < (*gb).n_nodes {
        let mut node: *mut ggml_tensor = (*gb).nodes[i as usize];
        if (ggml_graph_get_parent(gb, node)).is_null() {
            if (*node).is_param {
                snprintf(
                    color.as_mut_ptr(),
                    ::core::mem::size_of::<[libc::c_char; 16]>() as libc::c_ulong,
                    b"yellow\0" as *const u8 as *const libc::c_char,
                );
            } else if !((*node).grad).is_null() {
                if ggml_graph_find(gf, node) {
                    snprintf(
                        color.as_mut_ptr(),
                        ::core::mem::size_of::<[libc::c_char; 16]>() as libc::c_ulong,
                        b"green\0" as *const u8 as *const libc::c_char,
                    );
                } else {
                    snprintf(
                        color.as_mut_ptr(),
                        ::core::mem::size_of::<[libc::c_char; 16]>() as libc::c_ulong,
                        b"lightblue\0" as *const u8 as *const libc::c_char,
                    );
                }
            } else {
                snprintf(
                    color.as_mut_ptr(),
                    ::core::mem::size_of::<[libc::c_char; 16]>() as libc::c_ulong,
                    b"white\0" as *const u8 as *const libc::c_char,
                );
            }
            fprintf(
                fp,
                b"  \"%p\" [ style = filled; fillcolor = %s; shape = record; label=\"\0"
                    as *const u8 as *const libc::c_char,
                node as *mut libc::c_void,
                color.as_mut_ptr(),
            );
            if strlen(((*node).name).as_mut_ptr()) > 0 as libc::c_int as libc::c_ulong {
                fprintf(
                    fp,
                    b"%s |\0" as *const u8 as *const libc::c_char,
                    ((*node).name).as_mut_ptr(),
                );
            }
            if (*node).n_dims == 2 as libc::c_int {
                fprintf(
                    fp,
                    b"%d [%ld, %ld] | <x>%s\0" as *const u8 as *const libc::c_char,
                    i,
                    (*node).ne[0 as libc::c_int as usize],
                    (*node).ne[1 as libc::c_int as usize],
                    GGML_OP_SYMBOL[(*node).op as usize],
                );
            } else {
                fprintf(
                    fp,
                    b"%d [%ld, %ld, %ld] | <x>%s\0" as *const u8 as *const libc::c_char,
                    i,
                    (*node).ne[0 as libc::c_int as usize],
                    (*node).ne[1 as libc::c_int as usize],
                    (*node).ne[2 as libc::c_int as usize],
                    GGML_OP_SYMBOL[(*node).op as usize],
                );
            }
            if !((*node).grad).is_null() {
                fprintf(
                    fp,
                    b" | <g>%s\"; ]\n\0" as *const u8 as *const libc::c_char,
                    GGML_OP_SYMBOL[(*(*node).grad).op as usize],
                );
            } else {
                fprintf(fp, b"\"; ]\n\0" as *const u8 as *const libc::c_char);
            }
        }
        i += 1;
        i;
    }
    let mut i_0: libc::c_int = 0 as libc::c_int;
    while i_0 < (*gb).n_leafs {
        let mut node_0: *mut ggml_tensor = (*gb).leafs[i_0 as usize];
        snprintf(
            color.as_mut_ptr(),
            ::core::mem::size_of::<[libc::c_char; 16]>() as libc::c_ulong,
            b"pink\0" as *const u8 as *const libc::c_char,
        );
        fprintf(
            fp,
            b"  \"%p\" [ style = filled; fillcolor = %s; shape = record; label=\"<x>\0" as *const u8
                as *const libc::c_char,
            node_0 as *mut libc::c_void,
            color.as_mut_ptr(),
        );
        if strlen(((*node_0).name).as_mut_ptr()) > 0 as libc::c_int as libc::c_ulong {
            fprintf(
                fp,
                b"%s | \0" as *const u8 as *const libc::c_char,
                ((*node_0).name).as_mut_ptr(),
            );
        }
        if ggml_nelements(node_0) == 1 as libc::c_int as libc::c_long {
            if (*node_0).type_0 as libc::c_uint == GGML_TYPE_I8 as libc::c_int as libc::c_uint
                || (*node_0).type_0 as libc::c_uint == GGML_TYPE_I16 as libc::c_int as libc::c_uint
                || (*node_0).type_0 as libc::c_uint == GGML_TYPE_I32 as libc::c_int as libc::c_uint
            {
                fprintf(
                    fp,
                    b"%d\0" as *const u8 as *const libc::c_char,
                    ggml_get_i32_1d(node_0, 0 as libc::c_int),
                );
            } else {
                fprintf(
                    fp,
                    b"%.1e\0" as *const u8 as *const libc::c_char,
                    ggml_get_f32_1d(node_0, 0 as libc::c_int) as libc::c_double,
                );
            }
        } else {
            fprintf(
                fp,
                b"CONST %d [%ld, %ld]\0" as *const u8 as *const libc::c_char,
                i_0,
                (*node_0).ne[0 as libc::c_int as usize],
                (*node_0).ne[1 as libc::c_int as usize],
            );
        }
        fprintf(fp, b"\"; ]\n\0" as *const u8 as *const libc::c_char);
        i_0 += 1;
        i_0;
    }
    let mut i_1: libc::c_int = 0 as libc::c_int;
    while i_1 < (*gb).n_nodes {
        let mut node_1: *mut ggml_tensor = (*gb).nodes[i_1 as usize];
        let mut parent: *mut ggml_tensor = ggml_graph_get_parent(gb, node_1);
        if !((*node_1).src0).is_null() {
            let mut parent0: *mut ggml_tensor = ggml_graph_get_parent(gb, (*node_1).src0);
            fprintf(
                fp,
                b"  \"%p\":%s -> \"%p\":%s [ arrowhead = %s; style = %s; label = \"x\"; ]\n\0"
                    as *const u8 as *const libc::c_char,
                if !parent0.is_null() {
                    parent0 as *mut libc::c_void
                } else {
                    (*node_1).src0 as *mut libc::c_void
                },
                if !parent0.is_null() {
                    b"g\0" as *const u8 as *const libc::c_char
                } else {
                    b"x\0" as *const u8 as *const libc::c_char
                },
                if !parent.is_null() {
                    parent as *mut libc::c_void
                } else {
                    node_1 as *mut libc::c_void
                },
                if !parent.is_null() {
                    b"g\0" as *const u8 as *const libc::c_char
                } else {
                    b"x\0" as *const u8 as *const libc::c_char
                },
                if !parent.is_null() {
                    b"empty\0" as *const u8 as *const libc::c_char
                } else {
                    b"vee\0" as *const u8 as *const libc::c_char
                },
                if !parent.is_null() {
                    b"dashed\0" as *const u8 as *const libc::c_char
                } else {
                    b"solid\0" as *const u8 as *const libc::c_char
                },
            );
        }
        if !((*node_1).src1).is_null() {
            let mut parent1: *mut ggml_tensor = ggml_graph_get_parent(gb, (*node_1).src1);
            fprintf(
                fp,
                b"  \"%p\":%s -> \"%p\":%s [ arrowhead = %s; style = %s; label = \"y\"; ]\n\0"
                    as *const u8 as *const libc::c_char,
                if !parent1.is_null() {
                    parent1 as *mut libc::c_void
                } else {
                    (*node_1).src1 as *mut libc::c_void
                },
                if !parent1.is_null() {
                    b"g\0" as *const u8 as *const libc::c_char
                } else {
                    b"x\0" as *const u8 as *const libc::c_char
                },
                if !parent.is_null() {
                    parent as *mut libc::c_void
                } else {
                    node_1 as *mut libc::c_void
                },
                if !parent.is_null() {
                    b"g\0" as *const u8 as *const libc::c_char
                } else {
                    b"x\0" as *const u8 as *const libc::c_char
                },
                if !parent.is_null() {
                    b"empty\0" as *const u8 as *const libc::c_char
                } else {
                    b"vee\0" as *const u8 as *const libc::c_char
                },
                if !parent.is_null() {
                    b"dashed\0" as *const u8 as *const libc::c_char
                } else {
                    b"solid\0" as *const u8 as *const libc::c_char
                },
            );
        }
        i_1 += 1;
        i_1;
    }
    let mut i_2: libc::c_int = 0 as libc::c_int;
    while i_2 < (*gb).n_leafs {
        let mut node_2: *mut ggml_tensor = (*gb).leafs[i_2 as usize];
        if !((*node_2).src0).is_null() {
            fprintf(
                fp,
                b"  \"%p\":%s -> \"%p\":%s [ label = \"x\"; ]\n\0" as *const u8
                    as *const libc::c_char,
                (*node_2).src0 as *mut libc::c_void,
                b"x\0" as *const u8 as *const libc::c_char,
                node_2 as *mut libc::c_void,
                b"x\0" as *const u8 as *const libc::c_char,
            );
        }
        if !((*node_2).src1).is_null() {
            fprintf(
                fp,
                b"  \"%p\":%s -> \"%p\":%s [ label = \"y\"; ]\n\0" as *const u8
                    as *const libc::c_char,
                (*node_2).src1 as *mut libc::c_void,
                b"x\0" as *const u8 as *const libc::c_char,
                node_2 as *mut libc::c_void,
                b"x\0" as *const u8 as *const libc::c_char,
            );
        }
        i_2 += 1;
        i_2;
    }
    fprintf(fp, b"}\n\0" as *const u8 as *const libc::c_char);
    fclose(fp);
    printf(
        b"%s: dot -Tpng %s -o %s.png && open %s.png\n\0" as *const u8 as *const libc::c_char,
        (*::core::mem::transmute::<&[u8; 20], &[libc::c_char; 20]>(b"ggml_graph_dump_dot\0"))
            .as_ptr(),
        filename,
        filename,
        filename,
    );
}
unsafe extern "C" fn ggml_opt_set_params(
    mut np: libc::c_int,
    mut ps: *const *mut ggml_tensor,
    mut x: *const libc::c_float,
) {
    let mut i: libc::c_int = 0 as libc::c_int;
    let mut p: libc::c_int = 0 as libc::c_int;
    while p < np {
        let ne: int64_t = ggml_nelements(*ps.offset(p as isize));
        let mut j: int64_t = 0 as libc::c_int as int64_t;
        while j < ne {
            let fresh12 = i;
            i = i + 1;
            ggml_set_f32_1d(
                *ps.offset(p as isize),
                j as libc::c_int,
                *x.offset(fresh12 as isize),
            );
            j += 1;
            j;
        }
        p += 1;
        p;
    }
}
unsafe extern "C" fn ggml_opt_get_params(
    mut np: libc::c_int,
    mut ps: *const *mut ggml_tensor,
    mut x: *mut libc::c_float,
) {
    let mut i: libc::c_int = 0 as libc::c_int;
    let mut p: libc::c_int = 0 as libc::c_int;
    while p < np {
        let ne: int64_t = ggml_nelements(*ps.offset(p as isize));
        let mut j: int64_t = 0 as libc::c_int as int64_t;
        while j < ne {
            let fresh13 = i;
            i = i + 1;
            *x.offset(fresh13 as isize) = ggml_get_f32_1d(*ps.offset(p as isize), j as libc::c_int);
            j += 1;
            j;
        }
        p += 1;
        p;
    }
}
unsafe extern "C" fn ggml_opt_get_grad(
    mut np: libc::c_int,
    mut ps: *const *mut ggml_tensor,
    mut g: *mut libc::c_float,
) {
    let mut i: libc::c_int = 0 as libc::c_int;
    let mut p: libc::c_int = 0 as libc::c_int;
    while p < np {
        let ne: int64_t = ggml_nelements(*ps.offset(p as isize));
        let mut j: int64_t = 0 as libc::c_int as int64_t;
        while j < ne {
            let fresh14 = i;
            i = i + 1;
            *g.offset(fresh14 as isize) =
                ggml_get_f32_1d((**ps.offset(p as isize)).grad, j as libc::c_int);
            j += 1;
            j;
        }
        p += 1;
        p;
    }
}
unsafe extern "C" fn ggml_opt_adam(
    mut ctx: *mut ggml_context,
    mut params: ggml_opt_params,
    mut f: *mut ggml_tensor,
    mut gf: *mut ggml_cgraph,
    mut gb: *mut ggml_cgraph,
) -> ggml_opt_result {
    if !ggml_is_scalar(f) {
        fprintf(
            stderr,
            b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
            b"ggml.c\0" as *const u8 as *const libc::c_char,
            14720 as libc::c_int,
            b"ggml_is_scalar(f)\0" as *const u8 as *const libc::c_char,
        );
        abort();
    }
    (*gf).n_threads = params.n_threads;
    (*gb).n_threads = params.n_threads;
    let mut ps: [*mut ggml_tensor; 256] = [0 as *mut ggml_tensor; 256];
    let mut np: libc::c_int = 0 as libc::c_int;
    let mut nx: libc::c_int = 0 as libc::c_int;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < (*gf).n_nodes {
        if (*(*gf).nodes[i as usize]).is_param {
            if !(np < 256 as libc::c_int) {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    14734 as libc::c_int,
                    b"np < GGML_MAX_PARAMS\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            let fresh15 = np;
            np = np + 1;
            ps[fresh15 as usize] = (*gf).nodes[i as usize];
            nx = (nx as libc::c_long + ggml_nelements((*gf).nodes[i as usize])) as libc::c_int;
        }
        i += 1;
        i;
    }
    let alpha: libc::c_float = params.adam.alpha;
    let beta1: libc::c_float = params.adam.beta1;
    let beta2: libc::c_float = params.adam.beta2;
    let eps: libc::c_float = params.adam.eps;
    let mut x: *mut libc::c_float =
        (*ggml_new_tensor_1d(ctx, GGML_TYPE_F32, nx as int64_t)).data as *mut libc::c_float;
    let mut g1: *mut libc::c_float =
        (*ggml_new_tensor_1d(ctx, GGML_TYPE_F32, nx as int64_t)).data as *mut libc::c_float;
    let mut g2: *mut libc::c_float =
        (*ggml_new_tensor_1d(ctx, GGML_TYPE_F32, nx as int64_t)).data as *mut libc::c_float;
    let mut m: *mut libc::c_float =
        (*ggml_new_tensor_1d(ctx, GGML_TYPE_F32, nx as int64_t)).data as *mut libc::c_float;
    let mut v: *mut libc::c_float =
        (*ggml_new_tensor_1d(ctx, GGML_TYPE_F32, nx as int64_t)).data as *mut libc::c_float;
    let mut mh: *mut libc::c_float =
        (*ggml_new_tensor_1d(ctx, GGML_TYPE_F32, nx as int64_t)).data as *mut libc::c_float;
    let mut vh: *mut libc::c_float =
        (*ggml_new_tensor_1d(ctx, GGML_TYPE_F32, nx as int64_t)).data as *mut libc::c_float;
    let mut pf: *mut libc::c_float = (if params.past > 0 as libc::c_int {
        (*ggml_new_tensor_1d(ctx, GGML_TYPE_F32, params.past as int64_t)).data
    } else {
        0 as *mut libc::c_void
    }) as *mut libc::c_float;
    ggml_vec_set_f32(nx, m, 0.0f32);
    ggml_vec_set_f32(nx, v, 0.0f32);
    ggml_opt_get_params(np, ps.as_mut_ptr() as *const *mut ggml_tensor, x);
    ggml_graph_reset(gf);
    ggml_set_f32((*f).grad, 1.0f32);
    ggml_graph_compute(ctx, gb);
    let mut fx_prev: libc::c_float = ggml_get_f32_1d(f, 0 as libc::c_int);
    if !pf.is_null() {
        *pf.offset(0 as libc::c_int as isize) = fx_prev;
    }
    let mut n_no_improvement: libc::c_int = 0 as libc::c_int;
    let mut fx_best: libc::c_float = fx_prev;
    let mut t: libc::c_int = 0 as libc::c_int;
    while t < params.adam.n_iter {
        let mut i_0: libc::c_int = 0 as libc::c_int;
        while i_0 < np {
            i_0 += 1;
            i_0;
        }
        let t_start_wall: int64_t = ggml_time_us();
        let t_start_cpu: int64_t = ggml_cycles();
        ggml_opt_get_grad(np, ps.as_mut_ptr() as *const *mut ggml_tensor, g1);
        ggml_vec_scale_f32(nx, m, beta1);
        ggml_vec_mad_f32(nx, m, g1, 1.0f32 - beta1);
        ggml_vec_sqr_f32(nx, g2, g1);
        ggml_vec_scale_f32(nx, v, beta2);
        ggml_vec_mad_f32(nx, v, g2, 1.0f32 - beta2);
        ggml_vec_cpy_f32(nx, mh, m);
        ggml_vec_cpy_f32(nx, vh, v);
        ggml_vec_scale_f32(
            nx,
            mh,
            alpha / (1.0f32 - powf(beta1, (t + 1 as libc::c_int) as libc::c_float)),
        );
        ggml_vec_scale_f32(
            nx,
            vh,
            1.0f32 / (1.0f32 - powf(beta2, (t + 1 as libc::c_int) as libc::c_float)),
        );
        ggml_vec_sqrt_f32(nx, vh, vh);
        ggml_vec_acc1_f32(nx, vh, eps);
        ggml_vec_div_f32(nx, mh, mh, vh);
        ggml_vec_sub_f32(nx, x, x, mh);
        ggml_opt_set_params(np, ps.as_mut_ptr() as *const *mut ggml_tensor, x);
        ggml_graph_reset(gf);
        ggml_set_f32((*f).grad, 1.0f32);
        ggml_graph_compute(ctx, gb);
        let fx: libc::c_float = ggml_get_f32_1d(f, 0 as libc::c_int);
        if fabsf(fx - fx_prev) / fx < params.adam.eps_f {
            return GGML_OPT_OK;
        }
        if !pf.is_null() {
            if params.past <= t {
                let rate: libc::c_float = (*pf.offset((t % params.past) as isize) - fx) / fx;
                if fabsf(rate) < params.delta {
                    return GGML_OPT_OK;
                }
            }
            *pf.offset((t % params.past) as isize) = fx;
        }
        if params.max_no_improvement > 0 as libc::c_int {
            if fx_best > fx {
                fx_best = fx;
                n_no_improvement = 0 as libc::c_int;
            } else {
                n_no_improvement += 1;
                n_no_improvement;
                if n_no_improvement >= params.max_no_improvement {
                    return GGML_OPT_OK;
                }
            }
        }
        fx_prev = fx;
        let t_end_cpu: int64_t = ggml_cycles();
        let t_end_wall: int64_t = ggml_time_us();
        t += 1;
        t;
    }
    return GGML_OPT_DID_NOT_CONVERGE;
}
unsafe extern "C" fn linesearch_backtracking(
    mut ctx: *mut ggml_context,
    mut params: *const ggml_opt_params,
    mut nx: libc::c_int,
    mut x: *mut libc::c_float,
    mut fx: *mut libc::c_float,
    mut g: *mut libc::c_float,
    mut d: *mut libc::c_float,
    mut step: *mut libc::c_float,
    mut xp: *const libc::c_float,
    mut f: *mut ggml_tensor,
    mut gf: *mut ggml_cgraph,
    mut gb: *mut ggml_cgraph,
    np: libc::c_int,
    mut ps: *mut *mut ggml_tensor,
) -> ggml_opt_result {
    let mut count: libc::c_int = 0 as libc::c_int;
    let mut width: libc::c_float = 0.0f32;
    let mut dg: libc::c_float = 0.0f32;
    let mut finit: libc::c_float = 0.0f32;
    let mut dginit: libc::c_float = 0.0f32;
    let mut dgtest: libc::c_float = 0.0f32;
    let dec: libc::c_float = 0.5f32;
    let inc: libc::c_float = 2.1f32;
    if *step <= 0.0f32 {
        return GGML_LINESEARCH_INVALID_PARAMETERS;
    }
    ggml_vec_dot_f32(nx, &mut dginit, g, d);
    if (0 as libc::c_int as libc::c_float) < dginit {
        return GGML_LINESEARCH_FAIL;
    }
    finit = *fx;
    dgtest = (*params).lbfgs.ftol * dginit;
    loop {
        ggml_vec_cpy_f32(nx, x, xp);
        ggml_vec_mad_f32(nx, x, d, *step);
        ggml_opt_set_params(np, ps as *const *mut ggml_tensor, x);
        ggml_graph_reset(gf);
        ggml_set_f32((*f).grad, 1.0f32);
        ggml_graph_compute(ctx, gb);
        ggml_opt_get_grad(np, ps as *const *mut ggml_tensor, g);
        *fx = ggml_get_f32_1d(f, 0 as libc::c_int);
        count += 1;
        count;
        if *fx > finit + *step * dgtest {
            width = dec;
        } else {
            if (*params).lbfgs.linesearch as libc::c_uint
                == GGML_LINESEARCH_BACKTRACKING_ARMIJO as libc::c_int as libc::c_uint
            {
                return count as ggml_opt_result;
            }
            ggml_vec_dot_f32(nx, &mut dg, g, d);
            if dg < (*params).lbfgs.wolfe * dginit {
                width = inc;
            } else {
                if (*params).lbfgs.linesearch as libc::c_uint
                    == GGML_LINESEARCH_BACKTRACKING_WOLFE as libc::c_int as libc::c_uint
                {
                    return count as ggml_opt_result;
                }
                if dg > -(*params).lbfgs.wolfe * dginit {
                    width = dec;
                } else {
                    return count as ggml_opt_result;
                }
                return count as ggml_opt_result;
            }
        }
        if *step < (*params).lbfgs.min_step {
            return GGML_LINESEARCH_MINIMUM_STEP;
        }
        if *step > (*params).lbfgs.max_step {
            return GGML_LINESEARCH_MAXIMUM_STEP;
        }
        if (*params).lbfgs.max_linesearch <= count {
            return GGML_LINESEARCH_MAXIMUM_ITERATIONS;
        }
        *step *= width;
    }
}
unsafe extern "C" fn ggml_opt_lbfgs(
    mut ctx: *mut ggml_context,
    mut params: ggml_opt_params,
    mut f: *mut ggml_tensor,
    mut gf: *mut ggml_cgraph,
    mut gb: *mut ggml_cgraph,
) -> ggml_opt_result {
    if params.lbfgs.linesearch as libc::c_uint
        == GGML_LINESEARCH_BACKTRACKING_WOLFE as libc::c_int as libc::c_uint
        || params.lbfgs.linesearch as libc::c_uint
            == GGML_LINESEARCH_BACKTRACKING_STRONG_WOLFE as libc::c_int as libc::c_uint
    {
        if params.lbfgs.wolfe <= params.lbfgs.ftol || 1.0f32 <= params.lbfgs.wolfe {
            return GGML_OPT_INVALID_WOLFE;
        }
    }
    (*gf).n_threads = params.n_threads;
    (*gb).n_threads = params.n_threads;
    let m: libc::c_int = params.lbfgs.m;
    let mut ps: [*mut ggml_tensor; 256] = [0 as *mut ggml_tensor; 256];
    let mut np: libc::c_int = 0 as libc::c_int;
    let mut nx: libc::c_int = 0 as libc::c_int;
    let mut i: libc::c_int = 0 as libc::c_int;
    while i < (*gf).n_nodes {
        if (*(*gf).nodes[i as usize]).is_param {
            if !(np < 256 as libc::c_int) {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    15034 as libc::c_int,
                    b"np < GGML_MAX_PARAMS\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            let fresh16 = np;
            np = np + 1;
            ps[fresh16 as usize] = (*gf).nodes[i as usize];
            nx = (nx as libc::c_long + ggml_nelements((*gf).nodes[i as usize])) as libc::c_int;
        }
        i += 1;
        i;
    }
    let mut x: *mut libc::c_float =
        (*ggml_new_tensor_1d(ctx, GGML_TYPE_F32, nx as int64_t)).data as *mut libc::c_float;
    let mut xp: *mut libc::c_float =
        (*ggml_new_tensor_1d(ctx, GGML_TYPE_F32, nx as int64_t)).data as *mut libc::c_float;
    let mut g: *mut libc::c_float =
        (*ggml_new_tensor_1d(ctx, GGML_TYPE_F32, nx as int64_t)).data as *mut libc::c_float;
    let mut gp: *mut libc::c_float =
        (*ggml_new_tensor_1d(ctx, GGML_TYPE_F32, nx as int64_t)).data as *mut libc::c_float;
    let mut d: *mut libc::c_float =
        (*ggml_new_tensor_1d(ctx, GGML_TYPE_F32, nx as int64_t)).data as *mut libc::c_float;
    let mut pf: *mut libc::c_float = (if params.past > 0 as libc::c_int {
        (*ggml_new_tensor_1d(ctx, GGML_TYPE_F32, params.past as int64_t)).data
    } else {
        0 as *mut libc::c_void
    }) as *mut libc::c_float;
    let mut fx: libc::c_float = 0.0f32;
    let mut xnorm: libc::c_float = 0.0f32;
    let mut gnorm: libc::c_float = 0.0f32;
    let mut step: libc::c_float = 0.0f32;
    ggml_opt_get_params(np, ps.as_mut_ptr() as *const *mut ggml_tensor, x);
    let mut fresh17 = ::std::vec::from_elem(
        0,
        (::core::mem::size_of::<ggml_lbfgs_iteration_data>() as libc::c_ulong)
            .wrapping_mul(m as libc::c_ulong) as usize,
    );
    let mut lm: *mut ggml_lbfgs_iteration_data =
        fresh17.as_mut_ptr() as *mut ggml_lbfgs_iteration_data;
    let mut i_0: libc::c_int = 0 as libc::c_int;
    while i_0 < m {
        (*lm.offset(i_0 as isize)).alpha = 0.0f32;
        (*lm.offset(i_0 as isize)).ys = 0.0f32;
        let ref mut fresh18 = (*lm.offset(i_0 as isize)).s;
        *fresh18 =
            (*ggml_new_tensor_1d(ctx, GGML_TYPE_F32, nx as int64_t)).data as *mut libc::c_float;
        let ref mut fresh19 = (*lm.offset(i_0 as isize)).y;
        *fresh19 =
            (*ggml_new_tensor_1d(ctx, GGML_TYPE_F32, nx as int64_t)).data as *mut libc::c_float;
        i_0 += 1;
        i_0;
    }
    ggml_opt_set_params(np, ps.as_mut_ptr() as *const *mut ggml_tensor, x);
    ggml_graph_reset(gf);
    ggml_set_f32((*f).grad, 1.0f32);
    ggml_graph_compute(ctx, gb);
    ggml_opt_get_grad(np, ps.as_mut_ptr() as *const *mut ggml_tensor, g);
    fx = ggml_get_f32_1d(f, 0 as libc::c_int);
    if !pf.is_null() {
        *pf.offset(0 as libc::c_int as isize) = fx;
    }
    let mut fx_best: libc::c_float = fx;
    ggml_vec_neg_f32(nx, d, g);
    ggml_vec_norm_f32(nx, &mut xnorm, x);
    ggml_vec_norm_f32(nx, &mut gnorm, g);
    if xnorm < 1.0f32 {
        xnorm = 1.0f32;
    }
    if gnorm / xnorm <= params.lbfgs.eps {
        return GGML_OPT_OK;
    }
    ggml_vec_norm_inv_f32(nx, &mut step, d);
    let mut j: libc::c_int = 0 as libc::c_int;
    let mut k: libc::c_int = 1 as libc::c_int;
    let mut ls: libc::c_int = 0 as libc::c_int;
    let mut end: libc::c_int = 0 as libc::c_int;
    let mut bound: libc::c_int = 0 as libc::c_int;
    let mut n_no_improvement: libc::c_int = 0 as libc::c_int;
    let mut ys: libc::c_float = 0.0f32;
    let mut yy: libc::c_float = 0.0f32;
    let mut beta: libc::c_float = 0.0f32;
    loop {
        ggml_vec_cpy_f32(nx, xp, x);
        ggml_vec_cpy_f32(nx, gp, g);
        ls = linesearch_backtracking(
            ctx,
            &mut params,
            nx,
            x,
            &mut fx,
            g,
            d,
            &mut step,
            xp,
            f,
            gf,
            gb,
            np,
            ps.as_mut_ptr(),
        ) as libc::c_int;
        if ls < 0 as libc::c_int {
            ggml_vec_cpy_f32(nx, x, xp);
            ggml_vec_cpy_f32(nx, g, gp);
            return ls as ggml_opt_result;
        }
        ggml_vec_norm_f32(nx, &mut xnorm, x);
        ggml_vec_norm_f32(nx, &mut gnorm, g);
        if xnorm < 1.0f32 {
            xnorm = 1.0f32;
        }
        if gnorm / xnorm <= params.lbfgs.eps {
            return GGML_OPT_OK;
        }
        if !pf.is_null() {
            if params.past <= k {
                let rate: libc::c_float = (*pf.offset((k % params.past) as isize) - fx) / fx;
                if fabsf(rate) < params.delta {
                    return GGML_OPT_OK;
                }
            }
            *pf.offset((k % params.past) as isize) = fx;
        }
        if params.max_no_improvement > 0 as libc::c_int {
            if fx < fx_best {
                fx_best = fx;
                n_no_improvement = 0 as libc::c_int;
            } else {
                n_no_improvement += 1;
                n_no_improvement;
                if n_no_improvement >= params.max_no_improvement {
                    return GGML_OPT_OK;
                }
            }
        }
        if params.lbfgs.n_iter != 0 as libc::c_int && params.lbfgs.n_iter < k + 1 as libc::c_int {
            return GGML_OPT_DID_NOT_CONVERGE;
        }
        ggml_vec_sub_f32(nx, (*lm.offset(end as isize)).s, x, xp);
        ggml_vec_sub_f32(nx, (*lm.offset(end as isize)).y, g, gp);
        ggml_vec_dot_f32(
            nx,
            &mut ys,
            (*lm.offset(end as isize)).y,
            (*lm.offset(end as isize)).s,
        );
        ggml_vec_dot_f32(
            nx,
            &mut yy,
            (*lm.offset(end as isize)).y,
            (*lm.offset(end as isize)).y,
        );
        (*lm.offset(end as isize)).ys = ys;
        bound = if m <= k { m } else { k };
        k += 1;
        k;
        end = (end + 1 as libc::c_int) % m;
        ggml_vec_neg_f32(nx, d, g);
        j = end;
        let mut i_1: libc::c_int = 0 as libc::c_int;
        while i_1 < bound {
            j = (j + m - 1 as libc::c_int) % m;
            ggml_vec_dot_f32(
                nx,
                &mut (*lm.offset(j as isize)).alpha,
                (*lm.offset(j as isize)).s,
                d,
            );
            (*lm.offset(j as isize)).alpha /= (*lm.offset(j as isize)).ys;
            ggml_vec_mad_f32(
                nx,
                d,
                (*lm.offset(j as isize)).y,
                -(*lm.offset(j as isize)).alpha,
            );
            i_1 += 1;
            i_1;
        }
        ggml_vec_scale_f32(nx, d, ys / yy);
        let mut i_2: libc::c_int = 0 as libc::c_int;
        while i_2 < bound {
            ggml_vec_dot_f32(nx, &mut beta, (*lm.offset(j as isize)).y, d);
            beta /= (*lm.offset(j as isize)).ys;
            ggml_vec_mad_f32(
                nx,
                d,
                (*lm.offset(j as isize)).s,
                (*lm.offset(j as isize)).alpha - beta,
            );
            j = (j + 1 as libc::c_int) % m;
            i_2 += 1;
            i_2;
        }
        step = 1.0f64 as libc::c_float;
    }
}
#[no_mangle]
pub unsafe extern "C" fn ggml_opt_default_params(mut type_0: ggml_opt_type) -> ggml_opt_params {
    let mut result: ggml_opt_params = ggml_opt_params {
        type_0: GGML_OPT_ADAM,
        n_threads: 0,
        past: 0,
        delta: 0.,
        max_no_improvement: 0,
        print_forward_graph: false,
        print_backward_graph: false,
        adam: C2RustUnnamed_0 {
            n_iter: 0,
            alpha: 0.,
            beta1: 0.,
            beta2: 0.,
            eps: 0.,
            eps_f: 0.,
            eps_g: 0.,
        },
        lbfgs: C2RustUnnamed {
            m: 0,
            n_iter: 0,
            max_linesearch: 0,
            eps: 0.,
            ftol: 0.,
            wolfe: 0.,
            min_step: 0.,
            max_step: 0.,
            linesearch: GGML_LINESEARCH_BACKTRACKING_ARMIJO,
        },
    };
    match type_0 as libc::c_uint {
        0 => {
            result = {
                let mut init = ggml_opt_params {
                    type_0: GGML_OPT_ADAM,
                    n_threads: 1 as libc::c_int,
                    past: 0 as libc::c_int,
                    delta: 1e-5f32,
                    max_no_improvement: 100 as libc::c_int,
                    print_forward_graph: 1 as libc::c_int != 0,
                    print_backward_graph: 1 as libc::c_int != 0,
                    adam: {
                        let mut init = C2RustUnnamed_0 {
                            n_iter: 10000 as libc::c_int,
                            alpha: 0.001f32,
                            beta1: 0.9f32,
                            beta2: 0.999f32,
                            eps: 1e-8f32,
                            eps_f: 1e-5f32,
                            eps_g: 1e-3f32,
                        };
                        init
                    },
                    lbfgs: C2RustUnnamed {
                        m: 0,
                        n_iter: 0,
                        max_linesearch: 0,
                        eps: 0.,
                        ftol: 0.,
                        wolfe: 0.,
                        min_step: 0.,
                        max_step: 0.,
                        linesearch: GGML_LINESEARCH_BACKTRACKING_ARMIJO,
                    },
                };
                init
            };
        }
        1 => {
            result = {
                let mut init = ggml_opt_params {
                    type_0: GGML_OPT_LBFGS,
                    n_threads: 1 as libc::c_int,
                    past: 0 as libc::c_int,
                    delta: 1e-5f32,
                    max_no_improvement: 0 as libc::c_int,
                    print_forward_graph: 1 as libc::c_int != 0,
                    print_backward_graph: 1 as libc::c_int != 0,
                    adam: C2RustUnnamed_0 {
                        n_iter: 0,
                        alpha: 0.,
                        beta1: 0.,
                        beta2: 0.,
                        eps: 0.,
                        eps_f: 0.,
                        eps_g: 0.,
                    },
                    lbfgs: {
                        let mut init = C2RustUnnamed {
                            m: 6 as libc::c_int,
                            n_iter: 100 as libc::c_int,
                            max_linesearch: 20 as libc::c_int,
                            eps: 1e-5f32,
                            ftol: 1e-4f32,
                            wolfe: 0.9f32,
                            min_step: 1e-20f32,
                            max_step: 1e+20f32,
                            linesearch: GGML_LINESEARCH_DEFAULT,
                        };
                        init
                    },
                };
                init
            };
        }
        _ => {}
    }
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_opt(
    mut ctx: *mut ggml_context,
    mut params: ggml_opt_params,
    mut f: *mut ggml_tensor,
) -> ggml_opt_result {
    let mut free_ctx: bool = 0 as libc::c_int != 0;
    if ctx.is_null() {
        let mut params_ctx: ggml_init_params = {
            let mut init = ggml_init_params {
                mem_size: (16 as libc::c_int * 1024 as libc::c_int * 1024 as libc::c_int) as size_t,
                mem_buffer: 0 as *mut libc::c_void,
                no_alloc: 0 as libc::c_int != 0,
            };
            init
        };
        ctx = ggml_init(params_ctx);
        if ctx.is_null() {
            return GGML_OPT_NO_CONTEXT;
        }
        free_ctx = 1 as libc::c_int != 0;
    }
    let mut result: ggml_opt_result = GGML_OPT_OK;
    let mut gf: ggml_cgraph = ggml_build_forward(f);
    let mut gb: ggml_cgraph = ggml_build_backward(ctx, &mut gf, 1 as libc::c_int != 0);
    match params.type_0 as libc::c_uint {
        0 => {
            result = ggml_opt_adam(ctx, params, f, &mut gf, &mut gb);
        }
        1 => {
            result = ggml_opt_lbfgs(ctx, params, f, &mut gf, &mut gb);
        }
        _ => {}
    }
    if params.print_forward_graph {
        ggml_graph_print(&mut gf);
        ggml_graph_dump_dot(
            &mut gf,
            0 as *const ggml_cgraph,
            b"opt-forward.dot\0" as *const u8 as *const libc::c_char,
        );
    }
    if params.print_backward_graph {
        ggml_graph_print(&mut gb);
        ggml_graph_dump_dot(
            &mut gb,
            &mut gf,
            b"opt-backward.dot\0" as *const u8 as *const libc::c_char,
        );
    }
    if free_ctx {
        ggml_free(ctx);
    }
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_quantize_q4_0(
    mut src: *const libc::c_float,
    mut dst: *mut libc::c_void,
    mut n: libc::c_int,
    mut k: libc::c_int,
    mut hist: *mut int64_t,
) -> size_t {
    let nb: libc::c_int = k / 32 as libc::c_int;
    let mut b: libc::c_int = 0 as libc::c_int;
    while b < n {
        let mut y: *mut block_q4_0 =
            (dst as *mut block_q4_0).offset((b / 32 as libc::c_int) as isize);
        quantize_row_q4_0_reference(src.offset(b as isize), y, k);
        let mut i: libc::c_int = 0 as libc::c_int;
        while i < nb {
            let mut j: libc::c_int = 0 as libc::c_int;
            while j < 32 as libc::c_int {
                let vi0: uint8_t = ((*y.offset(i as isize)).qs[(j / 2 as libc::c_int) as usize]
                    as libc::c_int
                    & 0xf as libc::c_int) as uint8_t;
                let vi1: uint8_t = ((*y.offset(i as isize)).qs[(j / 2 as libc::c_int) as usize]
                    as libc::c_int
                    >> 4 as libc::c_int) as uint8_t;
                let ref mut fresh20 = *hist.offset(vi0 as isize);
                *fresh20 += 1;
                *fresh20;
                let ref mut fresh21 = *hist.offset(vi1 as isize);
                *fresh21 += 1;
                *fresh21;
                j += 2 as libc::c_int;
            }
            i += 1;
            i;
        }
        b += k;
    }
    return ((n / 32 as libc::c_int) as libc::c_ulong)
        .wrapping_mul(::core::mem::size_of::<block_q4_0>() as libc::c_ulong);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_quantize_q4_1(
    mut src: *const libc::c_float,
    mut dst: *mut libc::c_void,
    mut n: libc::c_int,
    mut k: libc::c_int,
    mut hist: *mut int64_t,
) -> size_t {
    let nb: libc::c_int = k / 32 as libc::c_int;
    let mut b: libc::c_int = 0 as libc::c_int;
    while b < n {
        let mut y: *mut block_q4_1 =
            (dst as *mut block_q4_1).offset((b / 32 as libc::c_int) as isize);
        quantize_row_q4_1_reference(src.offset(b as isize), y, k);
        let mut i: libc::c_int = 0 as libc::c_int;
        while i < nb {
            let mut j: libc::c_int = 0 as libc::c_int;
            while j < 32 as libc::c_int {
                let vi0: uint8_t = ((*y.offset(i as isize)).qs[(j / 2 as libc::c_int) as usize]
                    as libc::c_int
                    & 0xf as libc::c_int) as uint8_t;
                let vi1: uint8_t = ((*y.offset(i as isize)).qs[(j / 2 as libc::c_int) as usize]
                    as libc::c_int
                    >> 4 as libc::c_int) as uint8_t;
                let ref mut fresh22 = *hist.offset(vi0 as isize);
                *fresh22 += 1;
                *fresh22;
                let ref mut fresh23 = *hist.offset(vi1 as isize);
                *fresh23 += 1;
                *fresh23;
                j += 2 as libc::c_int;
            }
            i += 1;
            i;
        }
        b += k;
    }
    return ((n / 32 as libc::c_int) as libc::c_ulong)
        .wrapping_mul(::core::mem::size_of::<block_q4_1>() as libc::c_ulong);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_quantize_q5_0(
    mut src: *const libc::c_float,
    mut dst: *mut libc::c_void,
    mut n: libc::c_int,
    mut k: libc::c_int,
    mut hist: *mut int64_t,
) -> size_t {
    let nb: libc::c_int = k / 32 as libc::c_int;
    let mut b: libc::c_int = 0 as libc::c_int;
    while b < n {
        let mut y: *mut block_q5_0 =
            (dst as *mut block_q5_0).offset((b / 32 as libc::c_int) as isize);
        quantize_row_q5_0_reference(src.offset(b as isize), y, k);
        let mut i: libc::c_int = 0 as libc::c_int;
        while i < nb {
            let mut qh: uint32_t = 0;
            memcpy(
                &mut qh as *mut uint32_t as *mut libc::c_void,
                &mut (*y.offset(i as isize)).qh as *mut [uint8_t; 4] as *const libc::c_void,
                ::core::mem::size_of::<uint32_t>() as libc::c_ulong,
            );
            let mut j: libc::c_int = 0 as libc::c_int;
            while j < 32 as libc::c_int {
                let vh0: uint8_t = (((qh & (1 as libc::c_uint) << j + 0 as libc::c_int)
                    >> j + 0 as libc::c_int)
                    << 4 as libc::c_int) as uint8_t;
                let vh1: uint8_t = ((qh & (1 as libc::c_uint) << j + 16 as libc::c_int)
                    >> j + 12 as libc::c_int) as uint8_t;
                let vi0: uint8_t = (((*y.offset(i as isize)).qs[(j / 2 as libc::c_int) as usize]
                    as libc::c_int
                    & 0xf as libc::c_int
                    | vh0 as libc::c_int)
                    / 2 as libc::c_int) as uint8_t;
                let vi1: uint8_t = (((*y.offset(i as isize)).qs[(j / 2 as libc::c_int) as usize]
                    as libc::c_int
                    >> 4 as libc::c_int
                    | vh1 as libc::c_int)
                    / 2 as libc::c_int) as uint8_t;
                let ref mut fresh24 = *hist.offset(vi0 as isize);
                *fresh24 += 1;
                *fresh24;
                let ref mut fresh25 = *hist.offset(vi1 as isize);
                *fresh25 += 1;
                *fresh25;
                j += 2 as libc::c_int;
            }
            i += 1;
            i;
        }
        b += k;
    }
    return ((n / 32 as libc::c_int) as libc::c_ulong)
        .wrapping_mul(::core::mem::size_of::<block_q5_0>() as libc::c_ulong);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_quantize_q5_1(
    mut src: *const libc::c_float,
    mut dst: *mut libc::c_void,
    mut n: libc::c_int,
    mut k: libc::c_int,
    mut hist: *mut int64_t,
) -> size_t {
    let nb: libc::c_int = k / 32 as libc::c_int;
    let mut b: libc::c_int = 0 as libc::c_int;
    while b < n {
        let mut y: *mut block_q5_1 =
            (dst as *mut block_q5_1).offset((b / 32 as libc::c_int) as isize);
        quantize_row_q5_1_reference(src.offset(b as isize), y, k);
        let mut i: libc::c_int = 0 as libc::c_int;
        while i < nb {
            let mut qh: uint32_t = 0;
            memcpy(
                &mut qh as *mut uint32_t as *mut libc::c_void,
                &mut (*y.offset(i as isize)).qh as *mut [uint8_t; 4] as *const libc::c_void,
                ::core::mem::size_of::<uint32_t>() as libc::c_ulong,
            );
            let mut j: libc::c_int = 0 as libc::c_int;
            while j < 32 as libc::c_int {
                let vh0: uint8_t = (((qh & (1 as libc::c_uint) << j + 0 as libc::c_int)
                    >> j + 0 as libc::c_int)
                    << 4 as libc::c_int) as uint8_t;
                let vh1: uint8_t = ((qh & (1 as libc::c_uint) << j + 16 as libc::c_int)
                    >> j + 12 as libc::c_int) as uint8_t;
                let vi0: uint8_t = (((*y.offset(i as isize)).qs[(j / 2 as libc::c_int) as usize]
                    as libc::c_int
                    & 0xf as libc::c_int
                    | vh0 as libc::c_int)
                    / 2 as libc::c_int) as uint8_t;
                let vi1: uint8_t = (((*y.offset(i as isize)).qs[(j / 2 as libc::c_int) as usize]
                    as libc::c_int
                    >> 4 as libc::c_int
                    | vh1 as libc::c_int)
                    / 2 as libc::c_int) as uint8_t;
                let ref mut fresh26 = *hist.offset(vi0 as isize);
                *fresh26 += 1;
                *fresh26;
                let ref mut fresh27 = *hist.offset(vi1 as isize);
                *fresh27 += 1;
                *fresh27;
                j += 2 as libc::c_int;
            }
            i += 1;
            i;
        }
        b += k;
    }
    return ((n / 32 as libc::c_int) as libc::c_ulong)
        .wrapping_mul(::core::mem::size_of::<block_q5_1>() as libc::c_ulong);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_quantize_q8_0(
    mut src: *const libc::c_float,
    mut dst: *mut libc::c_void,
    mut n: libc::c_int,
    mut k: libc::c_int,
    mut hist: *mut int64_t,
) -> size_t {
    let nb: libc::c_int = k / 32 as libc::c_int;
    let mut b: libc::c_int = 0 as libc::c_int;
    while b < n {
        let mut y: *mut block_q8_0 =
            (dst as *mut block_q8_0).offset((b / 32 as libc::c_int) as isize);
        quantize_row_q8_0_reference(src.offset(b as isize), y, k);
        let mut i: libc::c_int = 0 as libc::c_int;
        while i < nb {
            let mut j: libc::c_int = 0 as libc::c_int;
            while j < 32 as libc::c_int {
                let vi: int8_t = (*y.offset(i as isize)).qs[j as usize];
                let ref mut fresh28 = *hist
                    .offset((vi as libc::c_int / 16 as libc::c_int + 8 as libc::c_int) as isize);
                *fresh28 += 1;
                *fresh28;
                j += 1;
                j;
            }
            i += 1;
            i;
        }
        b += k;
    }
    return ((n / 32 as libc::c_int) as libc::c_ulong)
        .wrapping_mul(::core::mem::size_of::<block_q8_0>() as libc::c_ulong);
}
#[no_mangle]
pub unsafe extern "C" fn ggml_quantize_chunk(
    mut type_0: ggml_type,
    mut src: *const libc::c_float,
    mut dst: *mut libc::c_void,
    mut start: libc::c_int,
    mut n: libc::c_int,
    mut hist: *mut int64_t,
) -> size_t {
    let mut result: size_t = 0 as libc::c_int as size_t;
    match type_0 as libc::c_uint {
        2 => {
            if !(start % 32 as libc::c_int == 0 as libc::c_int) {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    15479 as libc::c_int,
                    b"start % QK4_0 == 0\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            let mut block: *mut block_q4_0 =
                (dst as *mut block_q4_0).offset((start / 32 as libc::c_int) as isize);
            result = ggml_quantize_q4_0(
                src.offset(start as isize),
                block as *mut libc::c_void,
                n,
                n,
                hist,
            );
        }
        3 => {
            if !(start % 32 as libc::c_int == 0 as libc::c_int) {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    15485 as libc::c_int,
                    b"start % QK4_1 == 0\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            let mut block_0: *mut block_q4_1 =
                (dst as *mut block_q4_1).offset((start / 32 as libc::c_int) as isize);
            result = ggml_quantize_q4_1(
                src.offset(start as isize),
                block_0 as *mut libc::c_void,
                n,
                n,
                hist,
            );
        }
        6 => {
            if !(start % 32 as libc::c_int == 0 as libc::c_int) {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    15491 as libc::c_int,
                    b"start % QK5_0 == 0\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            let mut block_1: *mut block_q5_0 =
                (dst as *mut block_q5_0).offset((start / 32 as libc::c_int) as isize);
            result = ggml_quantize_q5_0(
                src.offset(start as isize),
                block_1 as *mut libc::c_void,
                n,
                n,
                hist,
            );
        }
        7 => {
            if !(start % 32 as libc::c_int == 0 as libc::c_int) {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    15497 as libc::c_int,
                    b"start % QK5_1 == 0\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            let mut block_2: *mut block_q5_1 =
                (dst as *mut block_q5_1).offset((start / 32 as libc::c_int) as isize);
            result = ggml_quantize_q5_1(
                src.offset(start as isize),
                block_2 as *mut libc::c_void,
                n,
                n,
                hist,
            );
        }
        8 => {
            if !(start % 32 as libc::c_int == 0 as libc::c_int) {
                fprintf(
                    stderr,
                    b"GGML_ASSERT: %s:%d: %s\n\0" as *const u8 as *const libc::c_char,
                    b"ggml.c\0" as *const u8 as *const libc::c_char,
                    15503 as libc::c_int,
                    b"start % QK8_0 == 0\0" as *const u8 as *const libc::c_char,
                );
                abort();
            }
            let mut block_3: *mut block_q8_0 =
                (dst as *mut block_q8_0).offset((start / 32 as libc::c_int) as isize);
            result = ggml_quantize_q8_0(
                src.offset(start as isize),
                block_3 as *mut libc::c_void,
                n,
                n,
                hist,
            );
        }
        _ => {}
    }
    return result;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cpu_has_avx() -> libc::c_int {
    return 1 as libc::c_int;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cpu_has_avx2() -> libc::c_int {
    return 1 as libc::c_int;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cpu_has_avx512() -> libc::c_int {
    return 1 as libc::c_int;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cpu_has_avx512_vbmi() -> libc::c_int {
    return 1 as libc::c_int;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cpu_has_avx512_vnni() -> libc::c_int {
    return 1 as libc::c_int;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cpu_has_fma() -> libc::c_int {
    return 1 as libc::c_int;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cpu_has_neon() -> libc::c_int {
    return 0 as libc::c_int;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cpu_has_arm_fma() -> libc::c_int {
    return 0 as libc::c_int;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cpu_has_f16c() -> libc::c_int {
    return 1 as libc::c_int;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cpu_has_fp16_va() -> libc::c_int {
    return 0 as libc::c_int;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cpu_has_wasm_simd() -> libc::c_int {
    return 0 as libc::c_int;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cpu_has_blas() -> libc::c_int {
    return 0 as libc::c_int;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cpu_has_cublas() -> libc::c_int {
    return 0 as libc::c_int;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cpu_has_clblast() -> libc::c_int {
    return 0 as libc::c_int;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cpu_has_gpublas() -> libc::c_int {
    return (ggml_cpu_has_cublas() != 0 || ggml_cpu_has_clblast() != 0) as libc::c_int;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cpu_has_sse3() -> libc::c_int {
    return 1 as libc::c_int;
}
#[no_mangle]
pub unsafe extern "C" fn ggml_cpu_has_vsx() -> libc::c_int {
    return 0 as libc::c_int;
}
unsafe extern "C" fn run_static_initializers() {
    CACHE_LINE_SIZE_F32 = (64 as libc::c_int as libc::c_ulong)
        .wrapping_div(::core::mem::size_of::<libc::c_float>() as libc::c_ulong);
}
#[used]
#[cfg_attr(target_os = "linux", link_section = ".init_array")]
#[cfg_attr(target_os = "windows", link_section = ".CRT$XIB")]
#[cfg_attr(target_os = "macos", link_section = "__DATA,__mod_init_func")]
static INIT_ARRAY: [unsafe extern "C" fn(); 1] = [run_static_initializers];
